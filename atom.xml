<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BaoWJ&#39;s Blog</title>
  
  
  <link href="https://1.15.86.100/atom.xml" rel="self"/>
  
  <link href="https://1.15.86.100/"/>
  <updated>2021-07-14T09:22:42.513Z</updated>
  <id>https://1.15.86.100/</id>
  
  <author>
    <name>Bao Wenjie</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SG函数</title>
    <link href="https://1.15.86.100/2021/07/14/SG%E5%87%BD%E6%95%B0/"/>
    <id>https://1.15.86.100/2021/07/14/SG%E5%87%BD%E6%95%B0/</id>
    <published>2021-07-14T09:15:40.000Z</published>
    <updated>2021-07-14T09:22:42.513Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SG函数"><a href="#SG函数" class="headerlink" title="SG函数"></a>SG函数</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>任何一个公平组合游戏都可以通过把每个<strong>局面（状态）</strong>看成一个<strong>顶点</strong>，对每个局面和它的子局面连一条有向边来抽象成一个<strong>有向图</strong>，游戏过程就是状态沿着顶点移动的过程，而<strong>SG函数</strong>就是定义在该<strong>有向图</strong>上的函数。</p>]]></content>
    
    
    <summary type="html">对SG函数的简单介绍和代码实现。</summary>
    
    
    
    <category term="组合数学" scheme="https://1.15.86.100/categories/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="Algorithm" scheme="https://1.15.86.100/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>正则表达式</title>
    <link href="https://1.15.86.100/2021/06/05/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    <id>https://1.15.86.100/2021/06/05/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</id>
    <published>2021-06-05T08:34:22.000Z</published>
    <updated>2021-06-05T08:36:38.093Z</updated>
    
    <content type="html"><![CDATA[<h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><h2 id="字符"><a href="#字符" class="headerlink" title="字符"></a>字符</h2><div class="table-container"><table><thead><tr><th>写法</th><th>描述</th></tr></thead><tbody><tr><td>[0-9]</td><td>匹配任何数字</td></tr><tr><td></td></tr></tbody></table></div>]]></content>
    
    
    <summary type="html">常用正则表达式。</summary>
    
    
    
    <category term="Python" scheme="https://1.15.86.100/categories/Python/"/>
    
    
  </entry>
  
  <entry>
    <title>卡特兰数</title>
    <link href="https://1.15.86.100/2021/05/26/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/"/>
    <id>https://1.15.86.100/2021/05/26/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/</id>
    <published>2021-05-26T11:58:21.000Z</published>
    <updated>2021-05-26T12:28:45.165Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卡特兰数"><a href="#卡特兰数" class="headerlink" title="卡特兰数"></a>卡特兰数</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>将<script type="math/tex">1,2,\cdots,N</script>，<strong>N</strong>个数依次进行入栈出栈操作，求输出的<strong>不同排列顺序的序列</strong>的个数。</p><p>假设<script type="math/tex">f(x)</script>表示<strong>x</strong>个数的序列个数，则对于<strong>N</strong>个数的数列，设<strong>i</strong>是最后出栈的数字，则比<strong>i</strong>小的数有<strong>i-1</strong>个，比<strong>i</strong>大的数有<strong>N-i</strong>个，所以<strong>递归公式</strong>为：</p><script type="math/tex; mode=display">f(N)=\sum_{i=1}^{N}f(i-1)\cdot f(N-i) \tag{*}</script><h2 id="递推公式"><a href="#递推公式" class="headerlink" title="递推公式"></a>递推公式</h2><script type="math/tex; mode=display">f(N)=\frac{4N-2}{n+1}f(N-1)</script><h2 id="通项公式"><a href="#通项公式" class="headerlink" title="通项公式"></a>通项公式</h2><script type="math/tex; mode=display">f(N)=\frac{C_{2N}^N}{N+1}</script>]]></content>
    
    
    <summary type="html">对卡特兰数的介绍。</summary>
    
    
    
    <category term="组合数学" scheme="https://1.15.86.100/categories/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>RSA</title>
    <link href="https://1.15.86.100/2021/05/25/RSA/"/>
    <id>https://1.15.86.100/2021/05/25/RSA/</id>
    <published>2021-05-25T03:40:20.000Z</published>
    <updated>2021-08-01T08:53:11.623Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h1><p><strong>RSA</strong>是一种<strong>非对称加密</strong>方法。1977年由<em>Ron Rivest</em>、<em>Adi Shamir</em>和<em>Leonard Adleman</em>一起提出的。当时他们三人都在。RSA就是他们三人姓氏开头字母拼在一起组成的。</p><h3 id="欧拉函数-phi​"><a href="#欧拉函数-phi​" class="headerlink" title="欧拉函数\phi​"></a>欧拉函数<script type="math/tex">\phi</script>​</h3><p>欧拉函数<script type="math/tex">\phi(n)</script>表示从<script type="math/tex">1</script>到<script type="math/tex">n</script>中和<script type="math/tex">n</script>​互质的自然数的个数。</p><p>根据<strong>质数性质</strong>，有以下结论：</p><ul><li><script type="math/tex">\phi(1)=1</script>，因为<script type="math/tex">1</script>和<strong>任何数</strong>互质；</li><li><script type="math/tex">\phi(n)=n-1</script>，<script type="math/tex">n</script>为质数时，<script type="math/tex">n</script>和<script type="math/tex">1\cdots n-1</script>互质；</li></ul><h3 id="欧拉定理"><a href="#欧拉定理" class="headerlink" title="欧拉定理"></a>欧拉定理</h3><p>如果两个正整数 <script type="math/tex">a</script> 和 <script type="math/tex">n</script> <strong>互质</strong>，则 <script type="math/tex">n</script> 的<strong>欧拉函数</strong> <script type="math/tex">\phi(n)</script> 可以让下面的等式成立：</p><script type="math/tex; mode=display">a^{\phi(n)}=1(\mod{n})</script><h2 id="RSA算法"><a href="#RSA算法" class="headerlink" title="RSA算法"></a>RSA算法</h2><h3 id="密钥生成"><a href="#密钥生成" class="headerlink" title="密钥生成"></a>密钥生成</h3><ul><li>随机选择两个<strong>不等</strong>的<strong>质数</strong> <script type="math/tex">p</script> 和 <script type="math/tex">q</script>；</li><li>计算 <script type="math/tex">p,q</script> 的<strong>乘积</strong> <script type="math/tex">n</script>​；【<strong>n</strong>做为<strong>公钥</strong>】</li><li>根据公式计算 <script type="math/tex">\phi(n)=LCM((p-1)(q-1))</script>​​，<strong>LCM</strong>：最小公倍数；</li><li>选择一个人整数<strong>e</strong>，满足：<script type="math/tex">1<e<\phi(n)\ and \ gcd(e,\phi(n))=1</script>​，即在<script type="math/tex">1</script>和<script type="math/tex">\phi(n)</script>之间和<script type="math/tex">\phi(n)</script>​互质的数；【<strong>e</strong>做为<strong>公钥</strong>】</li><li>令<script type="math/tex">d=e^{-1}(mod\ \phi(n))</script>；【<strong>d</strong>做为<strong>私钥</strong>】；</li></ul><h3 id="加密"><a href="#加密" class="headerlink" title="加密"></a>加密</h3><p>待加密数据：<script type="math/tex">m</script>；</p><p>计算<strong>密文c</strong>：<script type="math/tex">c(\mod n)=m^e</script></p><h3 id="解密"><a href="#解密" class="headerlink" title="解密"></a>解密</h3><p>密文：<script type="math/tex">c</script>；</p><p>计算<strong>原文m</strong>：<script type="math/tex">m(\mod n)=c^d</script></p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><ol><li>选择两个<strong>质数</strong>：<script type="math/tex">p=61,q=53</script>；</li><li>计算<strong>n</strong>：<script type="math/tex">n=p\cdot q=3233</script>；</li><li>计算欧拉函数：<script type="math/tex">\phi(n)=LCM((p-1)\cdot (q-1))=780</script>​；</li><li>随机取<strong>公钥</strong>（质数）：<script type="math/tex">e=17</script>；</li><li>计算<strong>私钥</strong>：<script type="math/tex">d=413,1=(17\times 413)(\mod780)</script>；</li><li>加密函数：<script type="math/tex">c(m)=m^{17}(\mod 3233)</script></li><li>解密函数：<script type="math/tex">m(c)=c^{413}(\mod 3233)</script></li></ol><h2 id="算法证明"><a href="#算法证明" class="headerlink" title="算法证明"></a>算法证明</h2><h3 id="基于欧拉定理"><a href="#基于欧拉定理" class="headerlink" title="基于欧拉定理"></a>基于欧拉定理</h3><p>对于和 <script type="math/tex">n</script> 互质的<strong>原文</strong>：<strong>m</strong>，需要证明：<script type="math/tex">m^{ed}=m(\mod n)</script></p><p>证明：</p><p>因为 <script type="math/tex">ed=1(\mod \phi(n))</script>​</p><p>有<script type="math/tex">ed=1+h\phi(n)</script></p><p>所以，<script type="math/tex">m^{ed}=m^{1+h\phi(n)}=m(m^{\phi(n)})^h</script>​​</p><p>根据<strong>欧拉定理</strong>：<script type="math/tex">m^{\phi(n)}=1(\mod n)</script></p><p>上式变为：<script type="math/tex">m(1)^h=m</script></p><p>故，原式得证。</p>]]></content>
    
    
    <summary type="html">对RSA加密算法的简单介绍。</summary>
    
    
    
    <category term="密码学" scheme="https://1.15.86.100/categories/%E5%AF%86%E7%A0%81%E5%AD%A6/"/>
    
    
    <category term="加密算法" scheme="https://1.15.86.100/tags/%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Dilworth定理</title>
    <link href="https://1.15.86.100/2021/05/24/Dilworth%E5%AE%9A%E7%90%86/"/>
    <id>https://1.15.86.100/2021/05/24/Dilworth%E5%AE%9A%E7%90%86/</id>
    <published>2021-05-24T12:04:37.000Z</published>
    <updated>2021-05-24T16:17:06.332Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dilworth定理"><a href="#Dilworth定理" class="headerlink" title="Dilworth定理"></a>Dilworth定理</h1><h2 id="概念定义"><a href="#概念定义" class="headerlink" title="概念定义"></a>概念定义</h2><ul><li><strong>自反性</strong>：对于集合<strong>A</strong>上的<strong>二元关系R</strong>，<script type="math/tex">xRx,\forall x\in A</script>；</li><li><strong>反对称性</strong>：对于集合<strong>A</strong>上的<strong>二元关系R</strong>，对于 <script type="math/tex">xRy\ \ and\ \ yRx,\forall x,y\in A</script>，有<script type="math/tex">x=y</script>；</li><li><strong>传递性</strong>：对于集合<strong>A</strong>上的<strong>二元关系R</strong>，对于 <script type="math/tex">xRy\ \ and\ \ yRz,\forall x,y,z\in A</script>，有<script type="math/tex">xRz</script>；</li><li><strong>偏序关系</strong>：满足<strong>自反性</strong>、<strong>反对称性</strong>、<strong>传递性</strong>的二元关系；</li><li><strong>偏序集</strong>：<script type="math/tex">(S,\precsim)</script>，集合<strong>S</strong>中任意两个元素之间都存在偏序关系；</li><li><strong>链</strong>：对于一个有序序列<script type="math/tex">(x_1,x_2,\cdots,x_n)</script>，如果<script type="math/tex">x_i\precsim x_j \forall i \le j</script>，则称该序列为<strong>链</strong>；</li><li><strong>反链</strong>：对于一个有序序列<script type="math/tex">(x_1,x_2,\cdots,x_n)</script>，如果<script type="math/tex">\forall i \le j,x_1,x_j</script>都不满足偏序关系<script type="math/tex">\precsim</script>，则称该序列为<strong>反链</strong>；</li></ul><h2 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h2><p>偏序集的<strong>最少</strong>的链的划分数等于其<strong>最长</strong>反链长度。</p><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><p><strong>数学归纳法</strong>：</p><p>设<strong>m</strong>表示<strong>偏序集</strong><script type="math/tex">(S,\precsim)</script>中元素个数。</p><ul><li><p>对于<strong>m=0，1</strong>，命题成立；</p></li><li><p>假设对于<script type="math/tex">m<n(n\in\mathbb{N}^+)</script>时命题成立，下面讨论<strong>m=n</strong>情况：</p><p>  设 <script type="math/tex">a</script> 为 <script type="math/tex">S</script> 中的一个<strong>极大元</strong>，构建偏序集 <script type="math/tex">S':=S-\{a\}</script>，由<strong>假设</strong>，<script type="math/tex">(S’,\precsim)</script> 满足该定理；</p><p>  假设 <script type="math/tex">S'</script> <strong>最小的链划分数</strong>以及<strong>最长反链长度</strong>为 <script type="math/tex">k</script>，划分为 <script type="math/tex">C_1,C_2\cdots,C_k</script> 这些不相交的链，长度为 <script type="math/tex">k</script> 的<strong>反链</strong>为 <script type="math/tex">a_1,a_2,\cdots,a_k</script>。</p><p>  记 <script type="math/tex">b_i</script> 为 <script type="math/tex">C_i</script> 中所有属于长为 <script type="math/tex">k</script> 的<strong>反链</strong>的元素中的<strong>最大元</strong>，构造<script type="math/tex">B:=\{b_1,b_2,\cdots,b_k\}</script>。在 <script type="math/tex">B</script> 中，易知 <script type="math/tex">B</script> 是<strong>反链</strong>，且 <script type="math/tex">C_i</script> 中的不可能存在多余一个元素都在<strong>反链</strong>上，也不会少于1个。</p><p>  下面考虑偏序集 <script type="math/tex">S</script>：</p><ul><li>因为 <script type="math/tex">a</script> 为 <script type="math/tex">S</script> 的<strong>极大元</strong>，那么 <script type="math/tex">\{a,b_1,b_2\cdots,b_k\}</script> 是一条长为 <script type="math/tex">k+1</script> 的<strong>反链</strong>。</li></ul></li></ul>]]></content>
    
    
    <summary type="html">对Dilworth定理的介绍和证明。</summary>
    
    
    
    <category term="组合数学" scheme="https://1.15.86.100/categories/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>以太网帧</title>
    <link href="https://1.15.86.100/2021/05/21/%E4%BB%A5%E5%A4%AA%E7%BD%91%E5%B8%A7/"/>
    <id>https://1.15.86.100/2021/05/21/%E4%BB%A5%E5%A4%AA%E7%BD%91%E5%B8%A7/</id>
    <published>2021-05-21T14:36:11.000Z</published>
    <updated>2021-05-30T08:37:11.730Z</updated>
    
    <content type="html"><![CDATA[<h1 id="以太帧"><a href="#以太帧" class="headerlink" title="以太帧"></a>以太帧</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在以太网<strong>链路层</strong>上的<strong>数据包</strong>称作<strong>以太帧</strong>。</p><p><strong>以太帧</strong>起始部分由<strong>前导码</strong>和<strong>帧开始符</strong>【802.3协议】组成。后面紧跟着一个以太网报头，以<strong>MAC地址</strong>说明目的地址和源地址。帧的中部是该帧负载的包含<strong>其他协议报头</strong>的数据包（例如IP协议）。以太帧由一个32位<strong>冗余校验码</strong>结尾。它用于检验数据传输是否出现损坏。</p><p><strong>前导码</strong>用于引起接受节点的注意，实质是告诉接收方准备接受新帧。</p><p>相关的有<strong>Ethernet II</strong>、<strong>802.2 LLC</strong>等协议。</p><h2 id="Ethernet-II"><a href="#Ethernet-II" class="headerlink" title="Ethernet II"></a>Ethernet II</h2><p><img src="/2021/05/21/%E4%BB%A5%E5%A4%AA%E7%BD%91%E5%B8%A7/700px-Ethernet_Type_II_Frame_format.svg.png" alt="img"></p><ul><li><strong>目的硬件地址（Destination MAC Address）</strong>：占 48 bit；</li><li><strong>源硬件地址（Source MAC Address）</strong>：占 48 bit；</li><li><strong>以太类型（Type）</strong>：占 16 bit；</li><li><strong>数据（Data）</strong>：上一层传递的数据；标识封装于<strong>以太网帧</strong>中的上层协议，常见值为16进制：<ul><li>0x800：IPv4；</li><li>0x86DD：IPv6；</li><li>0x806：ARP；</li></ul></li><li><strong>循环校验和（CRC Checksum）</strong>：占 32 bit，进行数据校验；</li></ul>]]></content>
    
    
    <summary type="html">对链路层的以太帧介绍。</summary>
    
    
    
    <category term="Computer Network" scheme="https://1.15.86.100/categories/Computer-Network/"/>
    
    
    <category term="Ethernet II" scheme="https://1.15.86.100/tags/Ethernet-II/"/>
    
  </entry>
  
  <entry>
    <title>TCP/IP</title>
    <link href="https://1.15.86.100/2021/05/21/TCP-IP/"/>
    <id>https://1.15.86.100/2021/05/21/TCP-IP/</id>
    <published>2021-05-21T07:45:45.000Z</published>
    <updated>2021-06-01T13:16:47.432Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TCP-IP"><a href="#TCP-IP" class="headerlink" title="TCP/IP"></a>TCP/IP</h1><h2 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h2><h2 id="TCP协议"><a href="#TCP协议" class="headerlink" title="TCP协议"></a>TCP协议</h2><p><img src="/2021/05/21/TCP-IP/6-19110Q62344I5.gif" alt="img"></p><h3 id="字段含义"><a href="#字段含义" class="headerlink" title="字段含义"></a>字段含义</h3><ul><li><strong>源端口（Source Port）</strong>：源计算机上的应用程序的端口号，占 16 位；</li><li><strong>目的端口（Destination Port）</strong>：目标计算机的应用程序端口号，占 16 位；</li><li><strong>序列号字段（Sequence Number）</strong>：占 32 位。它表示本报文段所发送数据的第一个字节的编号。在 TCP 连接中，所传送的字节流的每一个字节都会按顺序编号；</li><li><strong>确认号字段（Acknowledgement Number）</strong>：占 32 位。它表示接收方期望收到发送方下一个报文段的第一个字节数据的编号。其值是接收计算机即将接收到的下一个序列号，也就是下一个接收到的字节的序列号加1；</li><li><strong>数据偏移字段（Header Length）</strong>：占 4 位，数据段中的<strong>数据</strong>部分起始处，距离 TCP 数据段起始处的字节偏移量；</li><li><strong>保留字段（Resv）</strong>：占 4 位，为 TCP 将来的发展预留空间，目前必须全部为 0；</li><li><strong>标志字段</strong>：占 8 位，<ul><li><strong>CWR（Congestion Window Reduce）</strong>：拥塞窗口减少标志，用来表明它接收到了设置 ECE 标志的 TCP 包。并且，发送方收到消息之后，通过减小发送窗口的大小来降低发送速率；</li><li><strong>ECE（ECN Echo）</strong>：用来在 TCP 三次握手时表明一个 TCP 端是具备 ECN 功能的。在数据传输过程中，它也用来表明接收到的 TCP 包的 IP 头部的 ECN 被设置为 11，即网络线路拥堵；</li><li><strong>URG（Urgent）</strong>：表示本报文段中发送的数据是否包含紧急数据。URG=1 时表示有紧急数据。当 URG=1 时，后面的紧急指针字段才有效；</li><li><strong>ACK</strong>：表示前面的<strong>确认号</strong>字段是否有效。ACK=1 时表示有效。只有当 ACK=1 时，前面的确认号字段才有效。TCP 规定，连接建立后，ACK 必须为 1。</li><li><strong>PSH（Push）</strong>：告诉对方收到该报文段后是否立即把数据推送给上层。如果值为 1，表示应当立即把数据提交给上层，而不是缓存起来；</li><li><strong>RST</strong>：表示是否重置连接。如果 RST=1，说明 TCP 连接出现了严重错误（如主机崩溃），必须释放连接，然后再重新建立连接；</li><li><strong>SYN</strong>：在建立连接时使用，用来同步序号。当 <strong>SYN=1，ACK=0</strong> 时，表示这是一个请求建立连接的报文段；当 <strong>SYN=1，ACK=1</strong> 时，表示对方同意建立连接。SYN=1 时，说明这是一个请求建立连接或同意建立连接的报文。只有在前两次握手中 SYN 才为 1；</li><li><strong>FIN</strong>：标记数据是否发送完毕。如果 FIN=1，表示数据已经发送完成，可以释放连接；</li></ul></li><li><strong>窗口大小字段（Window Size）</strong>：占 16 位，它表示从<strong>确认号</strong>开始还可以接收多少字节的数据量，也表示当前接收端的接收窗口还有多少剩余空间，该字段可以用于 TCP 的流量控制；</li><li><strong>TCP 校验和字段（TCP Checksum）</strong>：占 16 位，它用于确认传输的数据是否有损坏。发送端基于数据内容校验生成一个数值，接收端根据接收的数据校验生成一个值。两个值必须相同，才能证明数据是有效的。如果两个值不同，则丢掉这个数据包。Checksum 是根据伪头 + TCP 头 + TCP 数据三部分进行计算的；</li><li><strong>紧急指针字段（Urgent Pointer）</strong>：占16位，仅当前面的 URG 控制位为 1 时才有意义。它指出本数据段中为紧急数据的字节数，占 16 位。当所有紧急数据处理完后，TCP 就会告诉应用程序恢复到正常操作。即使当前窗口大小为 0，也是可以发送紧急数据的，因为紧急数据无须缓存；</li><li><strong>可选字段（Options）</strong>：长度不定，但长度必须是 32bits 的整数倍。</li></ul><h2 id="IP协议"><a href="#IP协议" class="headerlink" title="IP协议"></a>IP协议</h2><h3 id="ipv4"><a href="#ipv4" class="headerlink" title="ipv4"></a>ipv4</h3><p><img src="/2021/05/21/TCP-IP/p1.png" alt="ipv4" style="zoom: 60%;"></p><ul><li><strong>版本（Version）</strong>：包含一个 4 位二进制值 0100，用于标识这是 IPv4 数据包。</li></ul><ul><li><p><strong>区分服务（DS）</strong>：以前称为服务类型 (ToS) 字段，DS 字段是一个用于确定每个数据包优先级的 8 位字段。DiffServ 字段的六个最高有效位是区分服务代码点 (DSCP)，而后两位是显式拥塞通知 (ECN) 位。</p></li><li><p><strong>生存时间 （TTL）</strong>：包含用于限制数据包寿命的一个 8 位二进制值。数据包发送方设置初始 TTL 值，数据包每经过一次路由器处理，数值就减少一。如果 TTL 字段的值减为零，则路由器将丢弃该数据包并向源 IP 地址发送 Internet 控制消息协议 (ICMP) 超时消息。</p></li><li><p><strong>协议（Protocol）</strong>：字段用于确定下一级协议。此 8 位二进制值表示数据包包含的数据负载类型，使网络层将数据传送到相应的上层协议。常用的值包括 <strong>ICMP (1)</strong>、<strong>TCP (6)</strong> 和 <strong>UDP (17)</strong>。</p></li><li><p><strong>源 IPv4 地址（Source Address）</strong>：包含表示数据包源 IPv4 地址的 32 位二进制值。源 IPv4 地址始终为单播地址。</p></li><li><p><strong>目的 IPv4 地址（Destination Address）</strong>：包含表示数据包目的 IPv4 地址的 32 位二进制值。目的 IPv4 地址为单播、组播或广播地址。</p></li></ul><ul><li><strong>互联网报头长度 (IHL)</strong>、<strong>总长度</strong>和<strong>报头校验和</strong>字段用于识别和验证数据包。</li></ul><h3 id="ipv6"><a href="#ipv6" class="headerlink" title="ipv6"></a>ipv6</h3><p><img src="/2021/05/21/TCP-IP/p2.png" alt="ipv6" style="zoom:80%;"></p><ul><li><p><strong>版本（Version）</strong>：此字段包含一个 4 位二进制值 0110，用于标识这是 IPv6 数据包；</p></li><li><p><strong>流量类别（Traffic Class）</strong>：此 8 位字段相当于<strong>IPv4 区分服务 (DS)</strong>字段；</p></li><li><p><strong>流量号（Flow Label）</strong>：此 20 位字段建议带有相同流量标签的所有数据包收到路由器的相同处理；</p></li><li><p><strong>有效负载长度（Payload Length）</strong>：此 16 位字段表示 IPv6 数据包的数据部分或负载的长度；</p></li><li><p><strong>下一首部（Next Header）</strong>：此 8 位字段相当于<strong>IPv4 协议</strong>字段。它表示数据包传送的数据负载类型，使网络层将数据传送到相应的上层协议；</p></li><li><p><strong>跳数限制（Hop Limit）</strong>：此 8 位字段取代 IPv4 的 TTL 字段。每个转发数据包的路由器均会使此数值减一。当跳数达到 0 时，会丢弃此数据包，并且会向发送主机转发 ICMPv6 超时消息来指明数据包未到达目的地，因为超过跳数限制。</p></li><li><p><strong>源 IPv6 地址 </strong>：此 128 位字段用于确定发送主机的 IPv6 地址；</p></li><li><p><strong>目的 IPv6 地址</strong>：此 128 位字段用于确定接收主机的 IPv6 地址；</p></li></ul><h3 id="ARP协议"><a href="#ARP协议" class="headerlink" title="ARP协议"></a>ARP协议</h3><p><strong>地址解析协议（Address Resolution Protocol）</strong>，其基本功能为通过目标设备的<strong>IP地址</strong>，查询目标设备的<strong>MAC地址</strong>，以保证通信的顺利进行。</p><h4 id="协议包结构："><a href="#协议包结构：" class="headerlink" title="协议包结构："></a>协议包结构：</h4><p><img src="/2021/05/21/TCP-IP/blog\blog\source\_posts\TCP-IP\p3.png" style="zoom:67%;"></p><ul><li><strong>硬件类型（Hardware type,HTYPE）</strong>：【16bits】指定网络连接协议类型：<ul><li>1：Ethernet；</li></ul></li><li><strong>协议类型（Protocol type,PTYPE）</strong>：【16bits】指定了<strong>ARP请求</strong>的互联网协议：<ul><li>0x0800：ipv4；</li></ul></li><li><strong>硬件地址长度（Hardware length,HLEN）</strong>：【8bits】硬件地址长度，以太网是6字节；</li><li><strong>协议地址长度（Protocol length,PLEN）</strong>：【8bits】网络地址长度，ipv4是4字节；</li><li><strong>操作（Operation）</strong>：【16bits】指定报文的目的：<ul><li>1：请求；</li><li>2：回复；</li></ul></li><li><strong>发送方地址（Sender hardware address,SHA）</strong>:【48bits】发送方<strong>MAC</strong>地址；</li><li><strong>发送方协议地址（Sender protocol address,SPA）</strong>：【32bits】发送方网络地址；</li><li><strong>目地方硬件地址（Target hardware address,THA）</strong>：【48bits】目的方<strong>MAC</strong>地址；</li><li><strong>目的方协议地址（Target protocol address,TPA）</strong>：【32bits】目的方网络地址；</li></ul><h2 id="网络字节顺序"><a href="#网络字节顺序" class="headerlink" title="网络字节顺序"></a>网络字节顺序</h2><p><strong>TCP/IP</strong>定义了标准的用于<strong>协议头</strong>中的二进制整数表示：<strong>网络字节顺序（Network Byte Order）</strong>。</p><h3 id="转换函数"><a href="#转换函数" class="headerlink" title="转换函数"></a>转换函数</h3><h4 id="htons"><a href="#htons" class="headerlink" title="htons"></a>htons</h4><ul><li>本地字节顺序 <script type="math/tex">\rightarrow</script> 网络字节顺序 【16bits】</li></ul><h4 id="ntohs"><a href="#ntohs" class="headerlink" title="ntohs"></a>ntohs</h4><ul><li>网络字节顺序 <script type="math/tex">\rightarrow</script> 本地字节顺序 【16bits】</li></ul><h4 id="htonl"><a href="#htonl" class="headerlink" title="htonl"></a>htonl</h4><ul><li>本地字节顺序 <script type="math/tex">\rightarrow</script> 网络字节顺序 【32bits】</li></ul><h4 id="ntohl"><a href="#ntohl" class="headerlink" title="ntohl"></a>ntohl</h4><ul><li>网络字节顺序 <script type="math/tex">\rightarrow</script> 本地字节顺序 【32bits】</li></ul>]]></content>
    
    
    <summary type="html">对TCP/IP的报文结构分析。</summary>
    
    
    
    <category term="Computer Network" scheme="https://1.15.86.100/categories/Computer-Network/"/>
    
    
    <category term="TCP" scheme="https://1.15.86.100/tags/TCP/"/>
    
    <category term="IP" scheme="https://1.15.86.100/tags/IP/"/>
    
  </entry>
  
  <entry>
    <title>WireShark</title>
    <link href="https://1.15.86.100/2021/05/21/WireShark/"/>
    <id>https://1.15.86.100/2021/05/21/WireShark/</id>
    <published>2021-05-21T06:46:05.000Z</published>
    <updated>2021-05-21T15:12:23.541Z</updated>
    
    <content type="html"><![CDATA[<h1 id="WireShark"><a href="#WireShark" class="headerlink" title="WireShark"></a>WireShark</h1><h2 id="WireShark抓包内容分析"><a href="#WireShark抓包内容分析" class="headerlink" title="WireShark抓包内容分析"></a>WireShark抓包内容分析</h2><h3 id="链路层封装"><a href="#链路层封装" class="headerlink" title="链路层封装"></a>链路层封装</h3><p>共占<strong>8*14=112</strong> bit。</p><p><img src="/2021/05/21/WireShark/p1.png" alt="p1" style="zoom: 80%;"></p><ul><li><strong>目的硬件地址（Destination Hardware Address）</strong>：占 48 bit；</li><li><strong>源硬件地址（Source Hardware Address）</strong>：占 48 bit；</li><li><strong>协议类别（Type）</strong>：占 16 bit；</li></ul><h3 id="网络层封装"><a href="#网络层封装" class="headerlink" title="网络层封装"></a>网络层封装</h3><p>共占<strong>8*20=160</strong> bit。</p><p><img src="/2021/05/21/WireShark/p2.png" alt="network" style="zoom:80%;"></p><ul><li><strong>版本号（Version）</strong>：占 4 bit，<strong>ipv4</strong>版本号；</li><li><strong>报文头长度（Header length）</strong>：占 4 bit，表示报文头长度【单位：字节（byte）】；</li><li><strong>区分服务域（Differentiated Services Field）</strong>：占 8 bit，<ul><li><strong>区分服务代码点（Differentiated Services Codepoints）</strong>：占 6 bit，DS标记值；</li><li><strong>显示拥塞通知（Explicit Congestion Notification）</strong>：占 2 bit；</li></ul></li><li><strong>总长度（Total Length）</strong>：占 16 bit；</li><li><strong>标识符（Identification）</strong>：占 16 bit；</li><li><strong>标志位（Flags）</strong>：占 3 bit；</li><li><strong>片位移（Fragment Offset）</strong>：占 13 bit；</li><li><strong>TTL（Time to Live）</strong>：占 8 bit；</li><li><strong>内层封装协议（Protocol）</strong>：占 8 bit；</li><li><strong>头部校验和（Header Checksum）</strong>：占 16 bit；</li><li><strong>源地址（Source Address）</strong>：占 32 bit；</li><li><strong>目标地址（Destination Address）</strong>：占 32 bit，即<strong>ip地址</strong>；</li></ul><h3 id="传输层封装（TCP）"><a href="#传输层封装（TCP）" class="headerlink" title="传输层封装（TCP）"></a>传输层封装（TCP）</h3><p>共占 <strong>8*32=256</strong> bit。</p><p><img src="/2021/05/21/WireShark/p3.png" alt="TCP"></p><ul><li><p><strong>源端口号（Source Port）</strong>：占 16 bit；</p></li><li><p><strong>目的端口号（Destination Port）</strong>：占 16 bit；</p></li><li><p><strong>序列号字段（Sequence Number）</strong>：占 32 bt；</p></li><li><p><strong>确认号字段（Acknowledgement Number）</strong>：占 32 bit；</p></li><li><p><strong>数据偏移字段（Header Length）</strong>：占 4 bit；</p></li><li><strong>标志字段（Flags）</strong>：占 12 bit；</li><li><strong>窗口大小字段（Window）</strong>：占 16 bit；</li><li><strong>校验和字段（Checksum）</strong>：占16 bit；</li><li><strong>紧急指针字段（Urgent Pointer）</strong>：占16 bit；</li><li><strong>可选字段（Option）</strong>：长度不定；</li></ul>]]></content>
    
    
    <summary type="html">关于抓包软件WireShark的使用。</summary>
    
    
    
    <category term="Computer Network" scheme="https://1.15.86.100/categories/Computer-Network/"/>
    
    
    <category term="Computer Network" scheme="https://1.15.86.100/tags/Computer-Network/"/>
    
  </entry>
  
  <entry>
    <title>Socket</title>
    <link href="https://1.15.86.100/2021/05/20/Socket/"/>
    <id>https://1.15.86.100/2021/05/20/Socket/</id>
    <published>2021-05-20T15:25:26.000Z</published>
    <updated>2021-05-30T15:09:05.222Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a>Socket</h1><h2 id="Socket-API"><a href="#Socket-API" class="headerlink" title="Socket API"></a>Socket API</h2><h4 id="WSAStartup"><a href="#WSAStartup" class="headerlink" title="WSAStartup"></a>WSAStartup</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">WSAStartup</span><span class="params">(WORD wVersionRequested, LPWSADATA IpWSAData)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li><p>使用<strong>Socket</strong>应用程序之前，必须调用<strong>WSAStartup</strong>；</p><ul><li>仅<strong>WinSock</strong>环境下；</li></ul></li><li><p>参数：</p><ul><li><p><strong>WORD wVersionRequested</strong>，指明程序请求使用的<strong>WinSock</strong>版本，<strong>高位字节</strong>指明<strong>副版本</strong>，<strong>低位字节</strong>指明<strong>主版本</strong>；</p><p>  例如：<strong>0x102</strong>表示<strong>2.1</strong>版；</p></li><li><p><strong>LPWSADATA IpWSAData</strong>，指向<strong>WSADATA</strong>的指针，保存返回的实际的<strong>WinSock</strong>的版本信息；</p></li></ul></li></ul><h4 id="WSACleanup"><a href="#WSACleanup" class="headerlink" title="WSACleanup"></a>WSACleanup</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">WSACleanup</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li>应用程序在完成<strong>Socket</strong>使用后调用；<ul><li>仅<strong>WinSock</strong>环境下；</li></ul></li><li>解除与<strong>Socket</strong>库的绑定；</li><li>释放<strong>Socket</strong>系统资源；</li></ul><h4 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">socket</span><span class="params">(<span class="keyword">int</span> protofamily, <span class="keyword">int</span> type, <span class="keyword">int</span> protocol)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li><p>创建<strong>套接字</strong>；</p></li><li><p>返回<strong>套接字</strong>描述符；</p></li><li><p>参数：</p><ul><li><p><strong>int protofamily</strong>：指明<strong>协议族</strong>，</p><p>  <strong>PF_INET</strong>——<strong>TCP/IP</strong></p></li><li><p><strong>int type</strong>：指明套接字类型，</p><p>  <strong>SOCK_STREAM</strong>：应用层到传输层（TCP）；</p><p>  <strong>SOCK_DGRAM</strong>：应用层到传输层（UDP）；</p><p>  <strong>SOCK_RAM</strong>：应用层到网络层；</p></li><li><p><strong>int protocol</strong>：指明协议号，0为默认；</p></li></ul></li></ul><h4 id="closesocket"><a href="#closesocket" class="headerlink" title="closesocket"></a>closesocket</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">closesocket</span><span class="params">(SOCKET sd)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li>关闭一个<strong>套接字</strong>；</li><li>如果多个进程共享一个套接字，调用<strong>closesocket</strong>将套接字<strong>引用计数</strong>减1，减至0才关闭该套接字；</li><li>一个进程中的多个线程对一个套接字的使用没有<strong>计数</strong>功能；</li><li>参数：<ul><li><strong>SOCKET sd</strong>：套接字描述符；</li></ul></li><li>返回：0——成功，<strong>SOCKET_ERROR</strong>——失败；</li></ul><h4 id="bind"><a href="#bind" class="headerlink" title="bind"></a>bind</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bind</span><span class="params">(<span class="keyword">int</span> sockfd, <span class="keyword">const</span> struct sockaddr *addr, <span class="keyword">socklen_t</span> addrlen)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li><p>绑定<strong>套接字</strong>本地<strong>端点地址</strong>；</p></li><li><p>客户端一般不用调用，服务器端需要；</p></li><li><p>参数：</p><ul><li><strong>int sockfd</strong>：套接字描述符；</li><li><strong>const struct sockaddr *addr</strong>：本地端口地址（IP地址+端口号），地址通配符<strong>INADDR_ANY</strong>可以绑定所有端口；</li><li><strong>socklen_t addrlen</strong>：地址大小；</li></ul></li></ul><h4 id="listen"><a href="#listen" class="headerlink" title="listen"></a>listen</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">listen</span><span class="params">(<span class="keyword">int</span> sockfd, <span class="keyword">int</span> queuesize)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li>将<strong>服务器端</strong>的<strong>流套接字</strong>置于<strong>监听状态</strong>；<ul><li>仅用于<strong>服务器端</strong>；</li><li>仅用于<strong>面向连接的流套接字</strong>；</li></ul></li><li>参数：<ul><li><strong>int sockfd</strong>：套接字描述符；</li><li><strong>int queuesize</strong>：请求队列的大小；</li></ul></li></ul><h4 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">connect</span>(<span class="keyword">int</span> sockfd, <span class="keyword">const</span> struct sockaddr *saddr, <span class="keyword">int</span> saddrlen)</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>客户</strong>程序调用<strong>connect</strong>使<strong>客户端套接字sockfd</strong>，与特点计算机的特定端口<strong>saddr</strong>的套接字进行连接；<ul><li>仅用于<strong>客户端</strong>；</li><li>可用于<strong>TCP、UDP</strong>；</li></ul></li></ul><h4 id="accept"><a href="#accept" class="headerlink" title="accept"></a>accept</h4><h4 id="send"><a href="#send" class="headerlink" title="send"></a>send</h4><h4 id="sendto"><a href="#sendto" class="headerlink" title="sendto"></a>sendto</h4><h4 id="recv"><a href="#recv" class="headerlink" title="recv"></a>recv</h4><h4 id="recvfrom"><a href="#recvfrom" class="headerlink" title="recvfrom"></a>recvfrom</h4><h4 id="setsockopt"><a href="#setsockopt" class="headerlink" title="setsockopt"></a>setsockopt</h4><h4 id="getsockopt"><a href="#getsockopt" class="headerlink" title="getsockopt"></a>getsockopt</h4>]]></content>
    
    
    <summary type="html">关于计算机网络套接字Socket。</summary>
    
    
    
    <category term="Computer Network" scheme="https://1.15.86.100/categories/Computer-Network/"/>
    
    
  </entry>
  
  <entry>
    <title>A Frustratingly Easy Approach for Joint Entity and Relation Extraction</title>
    <link href="https://1.15.86.100/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/"/>
    <id>https://1.15.86.100/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/</id>
    <published>2021-05-16T15:33:21.000Z</published>
    <updated>2021-05-17T12:46:06.913Z</updated>
    
    <content type="html"><![CDATA[<h1 id="A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction1"><a href="#A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction1" class="headerlink" title="A Frustratingly Easy Approach for Joint Entity and Relation Extraction1"></a>A Frustratingly Easy Approach for Joint Entity and Relation Extraction<a href="#refer-anchor-1"><sup>1</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>传统的<strong>命名实体识别（NER）</strong>和<strong>关系抽取（ER）</strong>任务是相互独立的；过去几年，开始出现把这两种任务进行<strong>联合训练</strong>的方法。</p><p>现有的<strong>关系抽取</strong>方法可以分为两种：</p><ul><li><strong>结构化预测（structured prediction）</strong>：将两个任务合并成一个任务；</li><li><strong>多任务学习（multi-task learning）</strong>：为两个任务构建两个分离的模型，然后通过<strong>参数共享（parameter  sharing）</strong>的方式进行联合训练和优化；</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><h4 id="NER（Named-Entity-Recognition）"><a href="#NER（Named-Entity-Recognition）" class="headerlink" title="NER（Named Entity Recognition）"></a>NER（Named Entity Recognition）</h4><p>设<script type="math/tex">\Epsilon</script>表示<strong>预先定义的实体类型集合</strong>，该<strong>NER</strong>任务是：</p><p>对于每个<strong>词组（span）</strong><script type="math/tex">s_i\in S</script>，需要预测一个实体<script type="math/tex">y_e(s_i)\in \Epsilon \cup \{\epsilon\}</script>，其中：<script type="math/tex">\epsilon</script>表示该<strong>词组</strong>不是<strong>实体</strong>。</p><p>输出为：<script type="math/tex">Y_e=\{(s_i,e),s_i\in S,e\in \Epsilon\cup \{\epsilon\}\}</script></p><h4 id="ER（Relation-Extraction）"><a href="#ER（Relation-Extraction）" class="headerlink" title="ER（Relation Extraction）"></a>ER（Relation Extraction）</h4><p>设<script type="math/tex">\mathcal{R}</script>表示预先定义的<strong>关系类型</strong>的集合，该<strong>ER</strong>任务为：</p><p>对于每对<strong>词组（span）</strong><script type="math/tex">s_i\in S,s_j\in S</script>，需要预测一个关系种类<script type="math/tex">y_r(s_i,s_j)\in \mathcal{R}\cup\{\epsilon\}</script>，<script type="math/tex">\epsilon</script>表示不存在关系。</p><p>输出为：<script type="math/tex">Y_r=\{(s_i,s_j,r),s_i,s_j\in S,r\in \mathcal{R}\}</script></p><h3 id="NER模型"><a href="#NER模型" class="headerlink" title="NER模型"></a>NER模型</h3><ul><li><p>使用<strong>预训练模型</strong>（例如：BERT）获取<script type="math/tex">x_t的</script><strong>上下文表征（contextualized representation）</strong><script type="math/tex">\mathrm{x}_t</script>；</p></li><li><p>获得<strong>词组表征（span representation）</strong>：</p><p>  对于词组<script type="math/tex">s_i\in S</script>，其<strong>词组表征</strong>为：</p><script type="math/tex; mode=display">\bold{h}_e(s_i)=[\mathrm{x}_{START(i)};\mathrm{x}_{END(i)};\phi(s_i)]</script><p>  其中，<script type="math/tex">\phi(s_i)\in\mathbb{R}^{d_W}</script>表示<strong>可学习的</strong>对于<strong>词组长度（span width）</strong>的表征；<script type="math/tex">\mathrm{x}_{START(i)}</script>表示<strong>词组</strong>的首单词，<script type="math/tex">\mathrm{x}_{END(i)}</script>表示词组的尾单词。</p></li><li><p>使用两层带<strong>ReLU激活函数</strong>的<strong>前向神经网络</strong>，将<strong>词组表征</strong>投影到<strong>分类空间</strong><script type="math/tex">\Epsilon</script>：</p><script type="math/tex; mode=display">P_e(e|s_i)=\mathrm{softmax}(\mathrm{W_eFFNN}(\bold{h_e(s_i)}))</script></li></ul><h3 id="ER模型"><a href="#ER模型" class="headerlink" title="ER模型"></a>ER模型</h3><p>过去的模型直接使用<strong>NER模型</strong>的<script type="math/tex">\textbf{h}_e(s_i),\textbf{h}(s_j)</script>来进行<strong>关系</strong>预测。但是，该文章认为：</p><ul><li><strong>NER</strong>这些表征只获取了每个<strong>实体</strong>附近的<strong>上下文信息（contextual information）</strong>，可能无法获取两个<strong>词组（span）</strong>之间的联系（因为这需要获取整个句子信息）。</li><li>在不同种类的<strong>词组对（pair of span）</strong>的表征之间共享参数可能不太好；</li></ul><p>于是该文章提出下面模型：</p><ul><li><p>将<strong>NER</strong>获得的实体标签插入到句子中：</p><p>  对于一对词组，<script type="math/tex">s_i,s_j</script>，其对应的<strong>实体</strong>为<script type="math/tex">e_i,e_j</script>，这里定义<strong>文本标记（text marker）</strong><script type="math/tex">\langle S:e_i \rangle,\langle /S:e_i\rangle,\langle O:e_j\rangle,\langle /O:e_j\rangle</script>，然后将他们插入原句子得到：</p><script type="math/tex; mode=display">\hat{X}=\cdots \langle S:e_i\rangle,x_{START(i)},\cdots,x_{END(i)},\langle /S:e_i\rangle,</script><script type="math/tex; mode=display">\cdots\langle O:e_j\rangle,x_{START(j)},\cdots,x_{END(j)},\langle/O:e_j\rangle,\cdots</script></li><li><p>使用另一个<strong>预训练模型</strong>获得<script type="math/tex">\hat{X}</script>的表征：</p><script type="math/tex; mode=display">\textbf{h}_r(s_i,s_j)=[\hat{\mathrm{x}}_\hat{START(i)};\hat{\mathrm{x}}_\hat{START(j)}]</script><p>  其中，<script type="math/tex">\hat{START(i)},\hat{START(j)}</script>分别是<script type="math/tex">\hat{X}</script>中<script type="math/tex">\langle S:e_i\rangle,\langle O:e_j\rangle</script>的<strong>序号</strong>；</p></li><li><p>计算分类结果：</p><script type="math/tex; mode=display">P_r(r|s_i,s_j)=\mathrm{softmax}(\textbf{W}_r\textbf{h}_r(s_i,s_j))</script></li></ul><h3 id="跨句文本表示（Cross-sentence-context）"><a href="#跨句文本表示（Cross-sentence-context）" class="headerlink" title="跨句文本表示（Cross-sentence context）"></a>跨句文本表示（Cross-sentence context）</h3><p>某个句子的前后文可能会对<strong>NER</strong>和<strong>ER</strong>结果产生影响。</p><p>这里将<strong>NER</strong>和<strong>ER</strong>输入句子长度<strong>固定为W</strong>，对于每个长度为<strong>n</strong>的句子，从该句子的上下文中分别补充<script type="math/tex">(W-n)/2</script>个单词。</p><h3 id="训练和推断"><a href="#训练和推断" class="headerlink" title="训练和推断"></a>训练和推断</h3><h4 id="训练loss"><a href="#训练loss" class="headerlink" title="训练loss"></a>训练loss</h4><p>都使用<strong>交叉熵</strong>损失。</p><p><strong>NER</strong>：</p><script type="math/tex; mode=display">\mathcal{L}_e=-\sum_{s_i\in S}\log P_e(e_i^*|s_i)</script><p><strong>ER</strong>：</p><script type="math/tex; mode=display">\mathcal{L}_r=-\sum_{s_i,s_j\in S_G,s_i\ne s_j}\log P_r(r_{i,j}^*|s_i,s_j)</script><p>对于<strong>ER</strong>训练，只考虑存在关系的实体对（gold entities）</p><h3 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h3><p><strong>NER</strong>：</p><script type="math/tex; mode=display">y_e(s_i)=\mathrm{argmax}_{e\in \Epsilon}P_e(e|s_i)</script><p><strong>ER</strong>：</p><p>枚举所有的<strong>实体对</strong>，并计算：<script type="math/tex">P_r(r|s_i,s_j)</script></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/image-20210517201212314.png" alt="image-20210517201212314"></p><p><strong>效果很好</strong></p><h3 id="验证构建文本标记的重要性"><a href="#验证构建文本标记的重要性" class="headerlink" title="验证构建文本标记的重要性"></a>验证构建文本标记的重要性</h3><p><img src="/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/image-20210517201423138.png" alt="image-20210517201423138" style="zoom:67%;"></p><ul><li><strong>TEXT</strong>：直接使用，<script type="math/tex">[\textbf{h}_e(s_i), \textbf{h}_e(s_j),\textbf{h}_e(s_i) \odot \textbf{h}_e(s_j)]</script>；</li><li><strong>TEXTETYPE</strong>：在<strong>TEXT</strong>基础上加入<strong>长度表征</strong>，<script type="math/tex">[\textbf{h}_e(s_i), \textbf{h}_e(s_j),\textbf{h}_e(s_i) \odot \textbf{h}_e(s_j),\phi(e_i)]</script>；</li><li><strong>MARKER</strong>：文本标记改为：<script type="math/tex">\langle S \rangle,\langle /S\rangle,\langle O\rangle,\langle /O\rangle</script>；</li><li><strong>MAKERETYPE</strong>：在<strong>MAKER</strong>基础上加入<strong>长度表征</strong>；</li><li><strong>MAKKERELOSS</strong>：在<strong>MARKER</strong>基础上使用<strong>FFNN</strong>进行预测；</li><li><strong>TYPEDMARKERS</strong>：模型提出的方法；</li></ul><p><strong>发现</strong>：使用<strong>标记</strong>更好，加上实体信息更好，加上长度信息更好。</p><h3 id="NER和ER之间关系"><a href="#NER和ER之间关系" class="headerlink" title="NER和ER之间关系"></a>NER和ER之间关系</h3><h4 id="NER和ER共享encoder效果更差"><a href="#NER和ER共享encoder效果更差" class="headerlink" title="NER和ER共享encoder效果更差"></a>NER和ER共享encoder效果更差</h4><p><img src="/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/image-20210517202950372.png" alt="image-20210517202950372" style="zoom:50%;"></p><h4 id="联合训练效果基本没有提高"><a href="#联合训练效果基本没有提高" class="headerlink" title="联合训练效果基本没有提高"></a>联合训练效果基本没有提高</h4><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/2010.12812">A Frustratingly Easy Approach for Joint Entity and Relation Extraction</a><p></p>]]></content>
    
    
    <summary type="html">本文是对A Frustratingly Easy Approach for Joint Entity and Relation Extraction论文的阅读总结。</summary>
    
    
    
    
    <category term="NLP" scheme="https://1.15.86.100/tags/NLP/"/>
    
    <category term="NER" scheme="https://1.15.86.100/tags/NER/"/>
    
    <category term="RE" scheme="https://1.15.86.100/tags/RE/"/>
    
  </entry>
  
  <entry>
    <title>Naive Bayes</title>
    <link href="https://1.15.86.100/2021/05/15/Naive-Bayes/"/>
    <id>https://1.15.86.100/2021/05/15/Naive-Bayes/</id>
    <published>2021-05-15T09:10:22.000Z</published>
    <updated>2021-05-16T08:03:56.253Z</updated>
    
    <content type="html"><![CDATA[<h1 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h1><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>利用<strong>条件独立性假设</strong>和<strong>贝叶斯公式</strong>，将对<strong>测试集</strong>的<strong>概率预测</strong>变为可计算的，再利用<strong>训练集</strong>求得。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>根据给定数据<script type="math/tex">X^*\in\mathbb{R}^d</script>，预测出其标签值<script type="math/tex">y^*\in\{0,1,\cdots,n\}</script>。</p><p>易得，预测结果为<script type="math/tex">y'</script>的概率为：</p><script type="math/tex; mode=display">P_\theta(y=y'|X=X^*)</script><p>由<strong>贝叶斯定理</strong>：</p><script type="math/tex; mode=display">p(a|b)=\frac{p(b|a)\cdot p(a)}{p(b)}</script><p>得：</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{p(X^*|y')\cdot p(y')}{p(X^*)}</script><p>由<strong>条件独立性假设</strong>，得：</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{p(x_1=x_1^*|y')\cdot p(x_2=x_2^*|y')\cdots p(x_d=x_d^*|y')\cdot p(y')}{p(X^*)}</script><p>其中，<script type="math/tex">p(x_i=x_i^*|y'),p(x_i=x_i^*)</script>可以由<strong>训练集</strong>求得<strong>后验概率</strong>。</p><h3 id="条件独立性假设"><a href="#条件独立性假设" class="headerlink" title="条件独立性假设"></a>条件独立性假设</h3><p>朴素贝叶斯法对条件概率分布做了 <strong>条件独立性假设</strong>，该<strong>条件独立性假设</strong>不等于<strong>独立性假设</strong>。</p><p>它是说 【<strong>用于分类的特征</strong>】在【<strong>类确定</strong>】的条件下都 是【<strong>条件独立</strong>】的，即：在给定<script type="math/tex">y</script>的条件下<script type="math/tex">x_i,x_j</script>是<strong>条件独立</strong>的。</p><h3 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h3><p>假设原始数据服从<strong>高斯分布</strong>，有以下概率公式：</p><script type="math/tex; mode=display">p(x_i=x|y=y')=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}</script><p>其中，<script type="math/tex">\mu,\sigma^2</script>是<script type="math/tex">y=y'</script>的第<script type="math/tex">i</script>个属性的数据分布的<strong>均值</strong>和<strong>方差</strong>。</p><h3 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h3><p>若特征值不是代表着简单的<strong>离散类型</strong>（如男性、女性），而是一种<strong>计数形式</strong>的特殊类型（身高，薪水等级），可以使用<strong>多项式模型</strong>。</p><p>该模型一般用于<strong>文本分类</strong>，<strong>属性</strong>是<strong>单词</strong>，<strong>属性值</strong>是<strong>单词出现次数</strong>。</p><script type="math/tex; mode=display">p(x_i|y=y')=\frac{N_{yi}+\alpha}{N_y+\alpha n}</script><p>其中，<script type="math/tex">N_{yi}=\sum_{x\in T}x_i</script>是训练集<script type="math/tex">D</script>中，标签为<script type="math/tex">y'</script>且第<script type="math/tex">i</script>个属性值求和（即单词<script type="math/tex">i</script>出现次数），<script type="math/tex">N_y=\sum_{i=1}^dN_{yi}</script>（所有单词出现总次数）。</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{m!}{x_1!\cdot x_2!\cdots x_d!}\prod_{i=1}^d(p^{x_i=x}(x_i|y=y'))</script><h3 id="伯努利模型"><a href="#伯努利模型" class="headerlink" title="伯努利模型"></a>伯努利模型</h3>]]></content>
    
    
    <summary type="html">朴素贝叶斯介绍</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Naive Bayes" scheme="https://1.15.86.100/tags/Naive-Bayes/"/>
    
  </entry>
  
  <entry>
    <title>KNN</title>
    <link href="https://1.15.86.100/2021/05/15/KNN/"/>
    <id>https://1.15.86.100/2021/05/15/KNN/</id>
    <published>2021-05-15T08:19:02.000Z</published>
    <updated>2021-05-15T08:37:25.987Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K近邻（KNN）"><a href="#K近邻（KNN）" class="headerlink" title="K近邻（KNN）"></a>K近邻（KNN）</h1><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>给定<strong>训练集</strong><script type="math/tex">D</script>，当对<strong>测试样本</strong>进行<strong>分类</strong>时，找到<strong>训练集</strong>中与该<strong>样本</strong>最相似的<script type="math/tex">k</script>个<strong>样本（k近邻）</strong>，根据<script type="math/tex">k</script>个<strong>样本</strong>的标签确定测试样本的标签。</p><p><img src="/2021/05/15/KNN/image-20210515162556039.png" alt="image-20210515162556039"></p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ul><li>输入训练集<script type="math/tex">D</script>；</li><li>确定<script type="math/tex">k</script>的大小和距离计算方法<script type="math/tex">d(\cdot)</script>；</li><li>从<strong>训练集</strong>中得到前<script type="math/tex">k</script>个与测试样本最近的样本（<script type="math/tex">k</script>近邻）；</li><li>根据<script type="math/tex">k</script>个最相似的训练样本的类别，通过<strong>投票</strong>的方式来确定测试样本的类别；</li></ul><h3 id="常用距离度量方法"><a href="#常用距离度量方法" class="headerlink" title="常用距离度量方法"></a>常用距离度量方法</h3><h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sqrt{\sum_{i=1}^d(x_{1i}-x_{2i})^2}</script><h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sum_{i=1}^d|x_{1i}-x_{2i}|</script><h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sqrt{(x_1-x_2)^T\Sigma^{-1}(x_1-x_2)}</script><h4 id="余弦距离"><a href="#余弦距离" class="headerlink" title="余弦距离"></a>余弦距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\frac{x_1^Tx_2}{||x_1||_2||x_2||_2}</script><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>对<strong>异常数据</strong>不敏感，具有较好的<strong>抗噪性</strong>；</li><li><strong>KNN</strong>算法的计算<strong>效率低</strong>；</li><li>当<strong>训练集较小</strong>的时候，<strong>KNN</strong>算法易导致<strong>过拟合</strong>；</li></ul><h2 id="Kd树"><a href="#Kd树" class="headerlink" title="Kd树"></a>Kd树</h2>]]></content>
    
    
    <summary type="html">KNN介绍</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="KNN" scheme="https://1.15.86.100/tags/KNN/"/>
    
    <category term="Supervised Learning" scheme="https://1.15.86.100/tags/Supervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Decision Tree</title>
    <link href="https://1.15.86.100/2021/05/15/Decision-Tree/"/>
    <id>https://1.15.86.100/2021/05/15/Decision-Tree/</id>
    <published>2021-05-15T08:17:08.000Z</published>
    <updated>2021-05-15T08:18:28.651Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h3 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h3><p><strong>决策树</strong>是一种描述对<em>实例</em>分类的树状结构，由结点（node）和有向边（directed edge）组成。在进行分类时，对每个实例，从<strong>根节点</strong>（root）开始，根据<em>该实例</em>是否满足结点的调节，选择不同的分支进入下个结点，最终到达<strong>叶节点</strong>（leaf）得到分类结果。</p><h4 id="信息增益："><a href="#信息增益：" class="headerlink" title="信息增益："></a>信息增益：</h4><p><strong>熵（entropy）</strong>表示随机变量<strong>不确定性</strong>的度量，设$X$是一个取有限个值的离散随机变量，其概率分布</p>]]></content>
    
    
    <summary type="html">决策树介绍</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Decision Tree" scheme="https://1.15.86.100/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>K-means</title>
    <link href="https://1.15.86.100/2021/05/15/K-means/"/>
    <id>https://1.15.86.100/2021/05/15/K-means/</id>
    <published>2021-05-15T08:10:22.000Z</published>
    <updated>2021-05-16T15:29:16.690Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h1><h2 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>K-means</strong>聚类算法是一种简单的<strong>无监督</strong>聚类方法。</p><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p><strong>输入</strong>：类数<strong>K</strong>，训练数据<strong>D</strong>。</p><ul><li>选择（一般随机）<strong>K</strong>个点作为初始<strong>簇中心</strong>；</li><li>将每个点指派到<strong>最近的簇中心</strong>形成<strong>K个簇</strong>；</li><li>重新计算每个<strong>簇</strong>的簇中心；</li><li>重复上述步骤直到<strong>簇中心不发生变化</strong>；</li></ul><p><strong>算法特点</strong>：</p><ul><li>简单、直观；</li><li>聚类结果依赖于<strong>𝐾</strong>个初始簇中心的选择；</li><li>容易陷入局部最优，不易处理非簇状数据；</li><li>聚类结果容易受离群值影响；</li></ul><p><strong>时间复杂度</strong>：</p><script type="math/tex; mode=display">O(mnkt)</script><p>其中，<script type="math/tex">m</script>：数据维度；<script type="math/tex">n</script>：样本个数；<script type="math/tex">k</script>：类别数；<script type="math/tex">t</script>：迭代次数。</p><p><img src="/2021/05/15/K-means/sphx_glr_plot_kmeans_assumptions_001.png" alt="Incorrect Number of Blobs, Anisotropicly Distributed Blobs, Unequal Variance, Unevenly Sized Blobs"></p><h2 id="改进模型"><a href="#改进模型" class="headerlink" title="改进模型"></a>改进模型</h2><h3 id="K-medoids"><a href="#K-medoids" class="headerlink" title="K-medoids"></a>K-medoids</h3><p><strong>K-medoids</strong>改进方法：</p><ul><li>选取每个簇中到<strong>簇内其他点</strong>的<strong>距离和最小</strong>的样本作为簇的中心。</li></ul><h3 id="二分K-means"><a href="#二分K-means" class="headerlink" title="二分K-means"></a>二分K-means</h3><p><strong>Bi K-means</strong>改进方法：</p><ul><li>每次从所有簇中选择能最大限度降低<strong>损失函数</strong>的簇，然后将该簇<strong>划分为两个簇</strong>，重复该过程，直至簇集合中含有<strong>𝐾</strong>个簇；</li></ul><h3 id="K-median"><a href="#K-median" class="headerlink" title="K-median"></a>K-median</h3><p><strong>K-median</strong>改进方法：</p><ul><li>距离度量使用<strong>曼哈顿距离</strong>；</li><li>每次<strong>簇中心</strong>使用簇的中位数；</li></ul><h2 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h2><h3 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h3><ul><li><strong>紧凑度</strong>：衡量簇内样本之间是否足够紧凑；</li><li><strong>分离度</strong>：衡量不同簇之间距离是否足够远；</li></ul><h3 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h3><ul><li>将<strong>聚类</strong>结果与其他模型进行比较；</li></ul><h3 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h3><p><strong>轮廓系数</strong>是聚类效果好坏的一种评价方式。它结合<strong>紧凑度</strong>和<strong>分离度</strong>两种因素。可以用来在相同原始数据的基础上用来评价不同算法，或者算法不同运行方式对聚类结果所产生的影响。</p><h4 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h4><ul><li><p><strong>簇内不相似度</strong>：计算样本<script type="math/tex">x_i</script>到同簇其他样本的平均距离<script type="math/tex">a_i</script>，称为样本<script type="math/tex">x_i</script>的<strong>簇内不相似度</strong>。</p><script type="math/tex; mode=display">a_i=\frac{1}{N-1}\sum_{j \ne i}d(x_i,x_j)</script><p>  <script type="math/tex">a_i</script>越小，说明样本<script type="math/tex">x_i</script>越应该被聚类到该簇。</p></li><li><p><strong>簇不相似度</strong>：簇<strong>C</strong>中所有样本的<strong>簇内不相似度</strong>的<strong>均值</strong>。</p></li><li><p><strong>不相似度</strong>：样本<script type="math/tex">x_i</script>到其他某簇<script type="math/tex">C_j</script>的<strong>所有样本</strong>的<strong>平均距离</strong><script type="math/tex">b_{iC_j}</script>，称为样本<script type="math/tex">x_i</script>与簇<script type="math/tex">C_j</script>的<strong>不相似度</strong>。</p><script type="math/tex; mode=display">b_{iC_j}=\frac{1}{N_{C_j}}\sum_{x\in C_j}d(x_i,x)</script></li><li><p><strong>簇间不相似度</strong>：样本<script type="math/tex">x_i</script>与其他簇的<strong>不相似度</strong>的<strong>最小值</strong><script type="math/tex">b_i</script>。</p><script type="math/tex; mode=display">b_i=\min(b_1,\cdots,b_N)</script><p>  <script type="math/tex">b_i</script>越大，说明样本<script type="math/tex">x_i</script>越不属于其他簇。</p></li><li><p><strong>轮廓系数</strong>：</p><script type="math/tex; mode=display">s(x_i)=\frac{b_i-a_i}{\max(a_i,b_i)}</script></li></ul><h4 id="判断方法"><a href="#判断方法" class="headerlink" title="判断方法"></a>判断方法</h4><ul><li><script type="math/tex">s(x_i)</script>越接近<strong>1</strong>，则说明样本<script type="math/tex">x_i</script>聚类越合理；</li><li><script type="math/tex">s(x_i)</script>越接近<strong>-1</strong>，则说明样本<script type="math/tex">x_i</script>越应该分类到另外的簇；</li><li><script type="math/tex">s(x_i)</script>近似为<strong>0</strong>，则说明样本<script type="math/tex">x_i</script>在两个簇的<strong>边界</strong>上。</li></ul><h3 id="兰德指数"><a href="#兰德指数" class="headerlink" title="兰德指数"></a>兰德指数</h3><p><strong>兰德指数（Rand index）</strong>需要给定<strong>实际标签C</strong>，假设<script type="math/tex">y_i,y_j</script>是<script type="math/tex">x_i,x_j</script>聚类结果，<script type="math/tex">y_i^*,y_j^*</script>是实际类别。则，<script type="math/tex">a</script>表示<script type="math/tex">y_i^*=y_j^*\and y_i=y_j</script>的对数，<script type="math/tex">b</script>表示<script type="math/tex">y_i^*\ne y_j^*\and y_i\ne y_j</script>的对数：</p><script type="math/tex; mode=display">\mathrm{RI}=\frac{a + b}{C_{|C|}^2}</script><h4 id="判断方法-1"><a href="#判断方法-1" class="headerlink" title="判断方法"></a>判断方法</h4><ul><li><strong>兰德指数</strong>取值范围为<strong>[0, 1]</strong>，值越大意味着聚类结果与真实情况越吻合；</li></ul>]]></content>
    
    
    <summary type="html">对K-means聚类算法的介绍。</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Cluser" scheme="https://1.15.86.100/tags/Cluser/"/>
    
    <category term="K-means" scheme="https://1.15.86.100/tags/K-means/"/>
    
  </entry>
  
  <entry>
    <title>Discretization</title>
    <link href="https://1.15.86.100/2021/05/15/Discretization/"/>
    <id>https://1.15.86.100/2021/05/15/Discretization/</id>
    <published>2021-05-15T03:11:48.000Z</published>
    <updated>2021-05-17T15:49:41.212Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据离散化"><a href="#数据离散化" class="headerlink" title="数据离散化"></a>数据离散化</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>将<strong>连续性特征</strong>转换成为<strong>离散型特征</strong>的过程称为<strong>特征离散化（data discretization）</strong>。</p><h2 id="无监督离散化"><a href="#无监督离散化" class="headerlink" title="无监督离散化"></a>无监督离散化</h2><h3 id="等距离散化"><a href="#等距离散化" class="headerlink" title="等距离散化"></a>等距离散化</h3><p>该方法根据<strong>连续特征</strong>的取值，将其均匀地划分成<script type="math/tex">k</script>个区间，每个区间的<strong>区间宽度<script type="math/tex">w</script></strong>相等：</p><script type="math/tex; mode=display">w=\frac{x_{max}-x_{min}}{k}</script><p><strong>特点</strong>：</p><ul><li>对数据要求高；</li><li>对离群值敏感；</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cut()</span><br></pre></td></tr></tbody></table></figure><h3 id="等频离散化"><a href="#等频离散化" class="headerlink" title="等频离散化"></a>等频离散化</h3><p>根据<strong>连续性特征</strong>取值的总数是<script type="math/tex">N</script>，仍然将其划分为<script type="math/tex">k</script>个区段，每个区段包含的数据个数<script type="math/tex">n</script>为:</p><script type="math/tex; mode=display">n=\frac{N}{k}</script><p><strong>特点</strong>：</p><ul><li>保证了每个区间段有相同的样本数；</li><li>取值相近的样本会被划分到不同区间；</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="聚类离散化"><a href="#聚类离散化" class="headerlink" title="聚类离散化"></a>聚类离散化</h3><p>离散化<strong>连续性特征</strong>时，如果<strong>相似的样本</strong>能落到<strong>相同的区间段</strong>内，则这样的划分可以更好地代表原始数据的信息，因此可以考虑利用<strong>聚类</strong>对连续性特征进行离散化处理。</p><h2 id="有监督离散化"><a href="#有监督离散化" class="headerlink" title="有监督离散化"></a>有监督离散化</h2><h3 id="信息增益离散化"><a href="#信息增益离散化" class="headerlink" title="信息增益离散化"></a>信息增益离散化</h3><p>该方法源自于<strong>决策树</strong>模型，在建立决策树时，遍历每一个特征，选择熵最小也就是<strong>信息增益最大</strong>的特征作为正式分裂节点。</p><h4 id="算法步骤："><a href="#算法步骤：" class="headerlink" title="算法步骤："></a>算法步骤：</h4><ul><li>对连续型特征进行排序；</li><li>把特征的每一个取值作为候选<strong>切分点</strong>，计算出相应的<strong>熵</strong>，选择熵最小的取值作为正式的切分点，将原来的区间一分为二；</li><li>递归处理第二步中得到的两个新区间段，直到每个区间段内特征的类别一样为止；</li><li>合并相邻的，<strong>类的熵值为0且特征类别相同</strong>的区段，重新计算新区间段类的熵值；</li><li>重复第四步到满足终止条件（决策树的深度或叶子数）；</li></ul><h3 id="卡方离散化"><a href="#卡方离散化" class="headerlink" title="卡方离散化"></a>卡方离散化</h3><p>基于<strong>卡方</strong>的离散化方法是采用<strong>自底向上</strong>的合并策略，首先将特征的取值看作单独的区间，然后逐一递归进行区间合并。</p><h4 id="ChiMerge方法步骤："><a href="#ChiMerge方法步骤：" class="headerlink" title="ChiMerge方法步骤："></a>ChiMerge方法步骤：</h4><ul><li>将连续型特征的每一个取值看作是一个单独的区间段，并进行排序；</li><li>针对每对相邻的区间段，计算卡方统计量。卡方值最小或者低于设定阈值的相邻区间段合并：</li><li>对于新的区间段，递归进行第1,2步，只到满足终止条件；</li></ul><h3 id="类别属性依赖最大化（CAIM）离散化"><a href="#类别属性依赖最大化（CAIM）离散化" class="headerlink" title="类别属性依赖最大化（CAIM）离散化"></a>类别属性依赖最大化（CAIM）离散化</h3><p>它采取<strong>自顶向下</strong>的策略。通过选择切分点<script type="math/tex">p</script>，把特征的取值空间划分为<script type="math/tex">x\le p</script>和<script type="math/tex">x \gt p</script>两个子区间段, 用来衡量切分点选择优劣的度量方法是类别属性的<strong>相互依赖程度</strong>。</p><p>假设某个<strong>连续型特征</strong>有<script type="math/tex">n</script>个取值，<script type="math/tex">C</script>个类别. 假设我们把特征划分为<script type="math/tex">k</script>个<strong>子区间段</strong>，子区间段集合记为：</p><script type="math/tex; mode=display">D=\{[x_0,x_1],(x_1,x_2],\cdots, (x_{k-1},x_k]\}</script><p>其中，<script type="math/tex">x_0,x_k</script>分别为特征的<strong>最小值</strong>和<strong>最大值</strong>。</p><p>设，<script type="math/tex">n_{i\cdot}</script>表示<strong>属于类别i</strong>的样本个数，<script type="math/tex">n_{\cdot j}</script>表示落在<strong>区间段</strong><script type="math/tex">(x_{j-1},x_j]</script>的样本个数，<script type="math/tex">n_{ij}</script>表示在区间内<script type="math/tex">(x_{j-1},x_j)</script>的且属于类别<script type="math/tex">\textbf{i}</script>的样本个数. 我们可以得到一个由<strong>类别特征</strong>和<strong>离散化特征取值</strong>所构成的二维表：</p><div class="table-container"><table><thead><tr><th>类别</th><th style="text-align:center"><script type="math/tex">[x_0,x_1]</script></th><th style="text-align:center"><script type="math/tex">(x_1,x_2]</script></th><th style="text-align:center">……</th><th style="text-align:center"><script type="math/tex">(x_{k-1},x_k]</script></th><th style="text-align:center">类别样本数</th></tr></thead><tbody><tr><td><strong>1</strong></td><td style="text-align:center"><script type="math/tex">n_{11}</script></td><td style="text-align:center"><script type="math/tex">n_{12}</script></td><td style="text-align:center">……</td><td style="text-align:center"><script type="math/tex">n_{1k}</script></td><td style="text-align:center"><script type="math/tex">n_{1\cdot}</script></td></tr><tr><td><strong>2</strong></td><td style="text-align:center"><script type="math/tex">n_{21}</script></td><td style="text-align:center"><script type="math/tex">n_{22}</script></td><td style="text-align:center">……</td><td style="text-align:center"><script type="math/tex">n_{2k}</script></td><td style="text-align:center"><script type="math/tex">n_{2\cdot}</script></td></tr><tr><td><script type="math/tex">\vdots</script></td><td style="text-align:center"><script type="math/tex">\vdots</script></td><td style="text-align:center"><script type="math/tex">\vdots</script></td><td style="text-align:center"><script type="math/tex">\ddots</script></td><td style="text-align:center"><script type="math/tex">\vdots</script></td><td style="text-align:center"><script type="math/tex">\vdots</script></td></tr><tr><td><strong>C</strong></td><td style="text-align:center"><script type="math/tex">n_{C1}</script></td><td style="text-align:center"><script type="math/tex">n_{C2}</script></td><td style="text-align:center">……</td><td style="text-align:center"><script type="math/tex">n_{Ck}</script></td><td style="text-align:center"><script type="math/tex">n_{C\cdot}</script></td></tr><tr><td><strong>区间样本数</strong></td><td style="text-align:center"><script type="math/tex">n_{\cdot 1}</script></td><td style="text-align:center"><script type="math/tex">n_{\cdot 2}</script></td><td style="text-align:center">……</td><td style="text-align:center"><script type="math/tex">n_{\cdot k}</script></td><td style="text-align:center"><script type="math/tex">n_{\cdot}</script></td></tr></tbody></table></div><p>则：</p><script type="math/tex; mode=display">\mathrm{CAIM}=\frac{1}{N}\sum_{j=1}^k\frac{M_j^2}{n_{\cdot j}},M_j=\max(\{n_{1j},n_{2j},\cdots,n_{Cj}\})</script><p><strong>CAIM</strong>的值<strong>越大</strong>，说明类和离散区间的<strong>相互依赖程度越大</strong>，也就说明了<strong>离散化效果越好</strong>。</p><h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><ul><li>对进行离散化的<strong>特征</strong>进行<strong>升序排列</strong>，确定取值区间的<strong>最小值</strong><script type="math/tex">x_0</script>和<strong>最大值</strong><script type="math/tex">x_k</script>，初始化划分策略<script type="math/tex">D=\{[d_0,d_k]\}</script>；</li><li>把区间内的每个值当作<strong>候选切分点</strong>，计算把<strong>区间二分</strong>后的<strong>CAIM</strong>值，并选取<strong>CAIM值最高</strong>的点作为<strong>切分点</strong>，并更新<script type="math/tex">D</script>；</li><li>对于<script type="math/tex">D</script>中的每个<strong>区间段</strong>，重复第二步的计算，直到满足终止条件；</li></ul>]]></content>
    
    
    <summary type="html">数据离散化是特征工程或者数据预处理中很关键的一个环节，这里会对数据离散化进行简单介绍。</summary>
    
    
    
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
    <category term="Discretization" scheme="https://1.15.86.100/tags/Discretization/"/>
    
  </entry>
  
  <entry>
    <title>Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
    <link href="https://1.15.86.100/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/"/>
    <id>https://1.15.86.100/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/</id>
    <published>2021-05-09T08:32:26.000Z</published>
    <updated>2021-05-09T12:47:16.349Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models1"><a href="#Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models1" class="headerlink" title="Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models1"></a>Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models<a href="#refer-anchor-1"><sup>1</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>目前最好的方式是将<strong>对话系统</strong>看成一个<strong>部分可观测的马尔可夫决策过程（Partially Observable Markov Decision Process，POMDP）</strong>。</p><p>但是大部分部署的<strong>对话系统</strong>都是基于<strong>人工指定的关于状态和行为的特征</strong>。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>将一个<strong>对话（dialogue）</strong>看成由两个<strong>对话者（interlocutor）</strong>发出的<script type="math/tex">M</script>个<strong>话语（utterance）</strong><script type="math/tex">D={U_1,\cdots,U_M}</script>构成的<strong>序列（sequence）</strong>。其中每个<strong>话语</strong><script type="math/tex">U_m=\{w_{m,1},\cdots,w_{m,n}\}</script>，其中<script type="math/tex">w_{m,n}</script>表示第<script type="math/tex">m</script>句话的第<script type="math/tex">n</script>个位置的<strong>token（包括单词和说话动作speech act）</strong>。</p><p>因此，一段对话<script type="math/tex">D</script>可以被分解为：</p><script type="math/tex; mode=display">P_\theta(U_1,\cdots,U_M)=\prod_{m=1}^MP_\theta(U_m|U_{<m})</script><script type="math/tex; mode=display">=\prod_{m=1}^M\prod_{n=1}^NP_\theta(w_{m,n}|w_{m,<n},U_{<m})</script><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><script type="math/tex; mode=display">h_n=f(h_{n-1}, w_n)=\tanh(Hh_{n-1}+I_{w_n})</script><p>其中，<script type="math/tex">h_n\in\mathbb{R}^{d_h}</script>是<strong>隐状态</strong>，<script type="math/tex">I</script>是词向量，是对从1到n的语句片段的表征。</p><p>对下一个单词的预测：</p><script type="math/tex; mode=display">P_\theta(w_{n+1}=v|w\le n)=\frac{\exp(g(h_n,v))}{\sum_{v'}\exp(g(h_n,v'))}</script><script type="math/tex; mode=display">g(h_n,v)=O^T_{w_n}h_n</script><p>其中，<script type="math/tex">O</script>是词向量。</p><h3 id="Hierarchical-Recurrent-Encoder-Decoder"><a href="#Hierarchical-Recurrent-Encoder-Decoder" class="headerlink" title="Hierarchical Recurrent Encoder-Decoder"></a>Hierarchical Recurrent Encoder-Decoder</h3><p>每个对话看成由<strong>语句</strong>组成，而每个<strong>语句</strong>又由<strong>字词</strong>组成。所以可以利用<strong>RNN</strong>在两个层次建模。<img src="/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/image-20210509200101676.png" alt="image-20210509200101676"></p><ul><li><strong>encoder RNN</strong>：将每个<strong>语句</strong>编码成一个<strong>语句向量（utterance vector）</strong>【最后的hidden state】；</li><li><strong>context RNN</strong>：记录经过的每个语句；</li><li><strong>decoder RNN</strong>：进行下一个语句的预测。</li></ul><h3 id="Bi-HRED"><a href="#Bi-HRED" class="headerlink" title="Bi-HRED"></a>Bi-HRED</h3><p>将上面的<strong>encoder RNN</strong>变成双向的。最终的<strong>语句向量</strong>，有两种方法取的：</p><ul><li>直接将双向RNN最后的<strong>hidden state</strong>拼接起来；</li><li>采用<script type="math/tex">L_2</script><strong>池化</strong>：<script type="math/tex">\sqrt{\frac{\sum_{n=1}^{N_m}h_n^2}{N_m}}</script>，<script type="math/tex">h_n</script>是位置n的<strong>hidden state</strong>，然后再拼接；</li></ul><h3 id="Bootstrapping-from-Word-Embeddings-and-Subtitles-Q-A"><a href="#Bootstrapping-from-Word-Embeddings-and-Subtitles-Q-A" class="headerlink" title="Bootstrapping from Word Embeddings and Subtitles Q-A"></a>Bootstrapping from Word Embeddings and Subtitles Q-A</h3><ul><li>使用在包括1000亿个单词的<strong>Google News</strong>数据集上训练的<strong>Word2Vec</strong>词向量，引入常识知识；</li><li>在<strong>非对话语料库</strong>上进行<strong>预训练</strong>；</li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><p>定义<strong>单词困惑度（word perplexity）</strong>：</p><script type="math/tex; mode=display">\exp{(-\frac{1}{N_w}\sum_{n=1}^N\log P_\theta(U_1^n,U_2^n,U_3^n))}</script><p>其中，<script type="math/tex">\theta</script>是参数，<script type="math/tex">\{U_1^n,U_2^n,U_3^n\}_{n=1}^N</script>是三元组的数据集，<script type="math/tex">N_w</script>是整个数据集的单词个数。</p><p><strong>困惑度越低模型越好</strong>。</p><h3 id="MAP-Output"><a href="#MAP-Output" class="headerlink" title="MAP Output"></a>MAP Output</h3><p>利用<strong>集束搜索（beam search）</strong>得到预测结果：</p><p><img src="/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/image-20210509203744611.png" alt="image-20210509203744611"></p><p>发现：结果比较合理，但大部分结果都是<strong>客套话（generic）</strong>，比如<em>I don’t know,I’m sorry</em>。</p><p>原因：</p><ul><li><strong>数据不够（data scarcity）</strong>：该模型比较大，可能需要更多数据；</li><li><strong>大部分token是标点符号代词（pronouns）</strong>：训练时，每个token都是地位相同的，导致模型训练不够好；</li><li><strong>三元组太短了</strong>：模型需要更多的信息。</li></ul><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1507.04808v2">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</a><p></p>]]></content>
    
    
    <summary type="html">本文是对Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models论文的阅读总结。</summary>
    
    
    
    
    <category term="Dialogue" scheme="https://1.15.86.100/tags/Dialogue/"/>
    
    <category term="RNN" scheme="https://1.15.86.100/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>Sklearn Naive Bayes</title>
    <link href="https://1.15.86.100/2021/04/28/Sklearn-Naive-Bayes/"/>
    <id>https://1.15.86.100/2021/04/28/Sklearn-Naive-Bayes/</id>
    <published>2021-04-28T08:15:21.000Z</published>
    <updated>2021-04-28T08:16:57.295Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h1>]]></content>
    
    
    <summary type="html">本文是对朴素贝叶斯(Naive Bayes)的简单介绍[结合sklearn文档]。</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Sklearn" scheme="https://1.15.86.100/tags/Sklearn/"/>
    
    <category term="Naive Bayes" scheme="https://1.15.86.100/tags/Naive-Bayes/"/>
    
  </entry>
  
  <entry>
    <title>Consistency by Agreement in Zero-shot Neural Machine Translation</title>
    <link href="https://1.15.86.100/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/"/>
    <id>https://1.15.86.100/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/</id>
    <published>2021-04-24T07:30:30.000Z</published>
    <updated>2021-04-26T15:52:01.644Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation"><a href="#Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation" class="headerlink" title="Consistency by Agreement in Zero-shot Neural Machine Translation[*]"></a>Consistency by Agreement in Zero-shot Neural Machine Translation<a href="#refer-*"><sup>[*]</sup></a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>一般对于多语言间（例如<strong>k</strong>中语言）间的互相翻译，需要训练<strong>k*k</strong>个<strong>NMT</strong>。</p><p>后来有人提出<strong>zero-shot translation</strong>，即给定三种语言<strong>German（De）、English（En）、French（Fr）</strong>，然后在<strong>平行文本（De，En）、（En，Fr）</strong>，同时使得该模型能够学得<strong>（De，Fr）</strong>翻译的方法。</p><ul><li><strong>零射学习（zero-shot learning）</strong>：对学习目标不提供任何样本。</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="定义【1】（Expected-Zero-shot-Consistency）"><a href="#定义【1】（Expected-Zero-shot-Consistency）" class="headerlink" title="定义【1】（Expected Zero-shot Consistency）"></a>定义【1】（Expected Zero-shot Consistency）</h3><p>令<script type="math/tex">\varepsilon_s\ \varepsilon_0</script>分别表示<strong>监督（supervised）</strong>和<strong>零射（zero-shot）</strong>任务，<script type="math/tex">\mathcal{l(\cdot)}</script>表示非负的<strong>损失函数</strong>，<script type="math/tex">\mathcal{M}</script>是使用<strong>最大期望监督损失（maximum expected supervised loss）</strong>的模型，并满足下列约束：</p><script type="math/tex; mode=display">\mathrm{max}_{(i,j)\in \varepsilon_s}\mathbb{E}_{x_i,x_j}[l(\mathcal{M})]<\epsilon</script><p>当对于<script type="math/tex">k(\epsilon)>0</script>满足：</p><script type="math/tex; mode=display">\max_{(i,j)\in \varepsilon_0}\mathbb{E}_{x_i,x_j}[l(M)]<k(\epsilon)</script><p>且当<script type="math/tex">\epsilon\to0</script>时，<script type="math/tex">k(\epsilon)\to 0</script></p><p>直观上说明，就是如果<strong>监督任务上的低loss会使零射任务上的loss降低</strong>则称<strong>zero-shot consistent</strong>。</p><h3 id="新方法"><a href="#新方法" class="headerlink" title="新方法"></a>新方法</h3><p>考虑对于四种语言（En，Es，Fr，Ru）的任务：</p><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426201028430.png" alt="image-20210426201028430" style="zoom:67%;"></p><p>考虑对<strong>En-Fr</strong>的联合似然：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}^{ind}(\theta)=\log[\mathbb{P}_\theta(\mathrm{x}_{Fr}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{x}_{En}|\mathrm{x}_{Fr})]</script><script type="math/tex; mode=display">=\log[\sum_{z'_{Es},z'_{Ru}}\mathbb{P}_\theta(\mathrm{x}_{Fr},\mathrm{z}'_{Es},\mathrm{z}'_{Ru}|\mathrm{x}_{En})]\times</script><script type="math/tex; mode=display">\sum_{z''_{Es},z''_{Ru}}\mathbb{P}_\theta(\mathrm{x}_{En},\mathrm{z}''_{Es},\mathrm{z}''_{Ru}|\mathrm{x}_{Fr})</script><p>其中<strong>Es，Ru</strong>是<strong>隐译文（latent translations）</strong>，同时这里假设<script type="math/tex">En\to Fr</script>和<script type="math/tex">Fr\to En</script>是<strong>相互独立</strong>的。</p><p>为了简便起见，令<script type="math/tex">\mathrm{z}:=(\mathrm{z}_{Es},\mathrm{z}_{Ru})</script></p><p>于是得到下面简化的公式：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}^{agree}(\theta)=\log\sum_\mathrm{z}[\mathbb{P}_\theta(\mathrm{x}_{Fr},\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{x}_{En},\mathrm{z}|\mathrm{x}_{Fr})] \tag{2}</script><p>接着利用下列公式：</p><script type="math/tex; mode=display">\mathbb{P}(x,z|y)=\mathbb{P}(x|z,y)\mathbb{P}(z|y)</script><script type="math/tex; mode=display">\mathbb{P}(x_{Fr}|z,x_{En})\approx \mathbb{P}(x_{Fr}|x_{En})</script><p>将<font color="red">(2)</font>式变形为：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}(\theta)\approx \tag{3}</script><script type="math/tex; mode=display">\log \mathbb{P}_\theta(\mathrm{x}_{Fr}|\mathrm{x}_{En})+\log\mathbb{P}_\theta(\mathrm{x}_{En}|\mathrm{x}_{Fr})+\  (composite\ likelihood\ terms)</script><script type="math/tex; mode=display">\log\sum_\mathrm{z}\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{Fr})\ (agreement\ term)</script><h4 id="Lower-bound"><a href="#Lower-bound" class="headerlink" title="Lower bound"></a>Lower bound</h4><p>对上式的<strong>(agreement term)</strong>利用<strong>Jensen不等式</strong>得到下面不等式：</p><script type="math/tex; mode=display">\log\sum_\mathrm{z}\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{Fr})</script><script type="math/tex; mode=display">\ge \mathbb{E}_{\mathrm{z}_{Es}|\mathrm{x}_{En}}[\log \mathbb{P}_\theta(\mathrm{z}_{Es}|\mathrm{x}_{Fr})]+</script><script type="math/tex; mode=display">\mathbb{E}_{\mathrm{z}_{Ru}|\mathrm{x}_{En}}[\log\mathbb{P}_\theta(\mathrm{z}_{Ru}|\mathrm{x}_{Fr})]</script><h3 id="定理【2】（Agreement-Zero-shot-Consistency）"><a href="#定理【2】（Agreement-Zero-shot-Consistency）" class="headerlink" title="定理【2】（Agreement Zero-shot Consistency）"></a>定理【2】（Agreement Zero-shot Consistency）</h3><script type="math/tex; mode=display">L_1,L_2,L_3$$是三种语言，其中$$L_1 \leftrightarrow L_2,L_2\leftrightarrow L_3$$是监督学习，$$L_1\leftrightarrow L_3$$是零射学习的内容。如果$$\mathbb{E}_{\mathrm{x_1,x_2,x_3}}[\mathcal{L}_{12}^{agree}(\theta)+\mathcal{L}_{23}^{agree}{(\theta)}]$$存在边界$$\epsilon>0$$，则：$$\mathbb{E}_{\mathrm{x_1,x_3}}[-\log\mathbb{P}_\theta(\mathrm{x_3}|\mathrm{x_1})]\le k(\epsilon)\ where \ k(\epsilon)\to 0\ as \ \epsilon\to 0</script><h3 id="模型总体结构"><a href="#模型总体结构" class="headerlink" title="模型总体结构"></a>模型总体结构</h3><p>给定平行句子<script type="math/tex">\mathrm{x_{En},x_{Fr}}</script>，以及辅助语言<script type="math/tex">\mathrm{x_{Es}}</script>。</p><h4 id="agreement-term"><a href="#agreement-term" class="headerlink" title="agreement term"></a>agreement term</h4><p><font color="red">(3)</font>式的<strong>agreement term</strong>按下列步骤计算：</p><p>首先将<strong>Es标签</strong>连接（concatenate）到<script type="math/tex">\mathrm{x_{En},x_{Fr}}</script>句子上，然后<strong>编码（encode）</strong>这些句子，使之翻译成<strong>Es</strong>语言</p><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426233520953.png" alt="image-20210426233520953" style="zoom:67%;"></p><p>然后，对编码的句子进行<strong>解码（decode）</strong>，得到辅助语言的翻译内容<script type="math/tex">\mathrm{z_{Es}(x_{En}),z_{Es}(x_{Fr})}</script>。</p><p>接着，用<script type="math/tex">\mathrm{(x_{Fr},z_{Es}(x_{En})),(x_{En},z_{Es}(x_{Fr}))}</script>分别作为两个<strong>平行语料</strong>训练<script type="math/tex">\mathrm{En\to Es,Fr\to Es}</script>两个翻译器，可以计算出：</p><script type="math/tex; mode=display">\log\mathbb{P}_\theta(\mathrm{z_{Es}(x_{Fr})|x_{En}})</script><script type="math/tex; mode=display">\log\mathbb{P}_\theta(\mathrm{z_{Es}(x_{En})|x_{Fr}})</script><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426234435972.png" alt="image-20210426234435972" style="zoom:67%;"></p><h4 id="算法流程如下："><a href="#算法流程如下：" class="headerlink" title="算法流程如下："></a>算法流程如下：</h4><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426234553607.png" alt="image-20210426234553607" style="zoom:67%;"></p><p>在每一轮迭代中，我们希望：</p><ol><li>提高梯度下降对<strong>zero-shot</strong>的影响；</li><li>降低梯度下降对<strong>监督学习</strong>的影响；</li></ol><p>为此分别使用：</p><ol><li><strong>连续贪心解码（greedy continuous decoding）</strong></li><li><strong>停止梯度下降（stop-gradient）</strong></li></ol><p><strong>REF</strong></p><p></p><div id="#refer-*"></div>[*]<a href="https://www.aclweb.org/anthology/N19-1121/">Consistency by Agreement in Zero-shot Neural Machine Translation</a><p></p>]]></content>
    
    
    <summary type="html">本文是对Consistency by Agreement in Zero-shot Neural Machine Translation论文的阅读总结。</summary>
    
    
    
    
    <category term="Paper" scheme="https://1.15.86.100/tags/Paper/"/>
    
    <category term="Neural Machine Translation" scheme="https://1.15.86.100/tags/Neural-Machine-Translation/"/>
    
    <category term="Zero-shot" scheme="https://1.15.86.100/tags/Zero-shot/"/>
    
  </entry>
  
  <entry>
    <title>Triangular Architecture for Rare Language Translation</title>
    <link href="https://1.15.86.100/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/"/>
    <id>https://1.15.86.100/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/</id>
    <published>2021-04-23T07:58:19.000Z</published>
    <updated>2021-04-23T08:52:07.727Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Triangular-Architecture-for-Rare-Language-Translation"><a href="#Triangular-Architecture-for-Rare-Language-Translation" class="headerlink" title="Triangular Architecture for Rare Language Translation[*]"></a>Triangular Architecture for Rare Language Translation<a href="#refer-*"><sup>[*]</sup></a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>作者提出了一种引入<strong>（Y，Z），（X，Y）</strong>平行文本，来增强<strong>（X，Z）</strong>语言机器翻译的<strong>三角形结构</strong>模型<strong>TA-NMT</strong>，。</p><p><img src="/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/image-20210423161225412.png" alt="image-20210423161225412" style="zoom:67%;"></p><p><strong>实线</strong>代表丰富的文本，<strong>虚线</strong>代表少量的文本。</p><p>该论文主要思路是将<script type="math/tex">X\to Y</script>分解成训练两个少量文本的模型<script type="math/tex">X\to Z</script>和<script type="math/tex">Y\to Z</script>。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul><li><p><strong>NMT</strong>首先将输入句子<strong>encode</strong>成<strong>vector</strong>，然后<strong>decoder</strong>在其基础上生成目标句子；</p></li><li><p>为了解决<strong>数据少（data sparsity）</strong>的问题，提出了以下几种方法：</p><ul><li><strong>单语言文本（monolingual data）</strong>；</li><li><strong>反向翻译（back translation）</strong>；</li><li><strong>联合训练（joint training）</strong>；</li><li><strong>对偶学习（dual learning）</strong>；</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="X-to-Y的loss"><a href="#X-to-Y的loss" class="headerlink" title="X \to Y的loss"></a><script type="math/tex">X \to Y</script>的loss</h3><script type="math/tex; mode=display">L(\Theta;D)=\sum_{(x,y)\in D}\mathrm{log}p(y|x)</script><script type="math/tex; mode=display">=\sum_{(x,y)\in D}\mathrm{log}\sum_z p(z|x)p(y|z)</script><script type="math/tex; mode=display">=\sum_{(x,y)\in D}\mathrm{log}\sum_zQ(z)\frac{p(z|x)p(y|z)}{Q(z)}\tag{1}</script><script type="math/tex; mode=display">\ge \sum_{(x,y)\in D}\sum_zQ(z)\mathrm{log}\frac{p(z|x)p(y|z)}{Q(z)}\ (Jensen's\ Inequation)</script><script type="math/tex; mode=display">=\mathcal{L}(Q)</script><p><strong>其中</strong>：</p><ul><li><script type="math/tex">Q(z)</script>是<script type="math/tex">z</script>的<strong>后验分布</strong>，满足<script type="math/tex">\sum_zQ(z)=1</script>；</li><li><script type="math/tex">p(y|x,z) \approx p(y|z)</script>；</li><li><script type="math/tex">D</script>：整个训练集；</li></ul><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>用<strong>EM算法</strong>来最大化<strong>下界</strong><script type="math/tex">\mathcal{L}(Q)</script>。</p><p>这里令<script type="math/tex">Q(z)=p(z|x)</script>来近似：</p><h4 id="M步"><a href="#M步" class="headerlink" title="M步"></a>M步</h4><script type="math/tex; mode=display">\Theta_{y|z}=\mathrm{argmax}_{\Theta_{y|x}}\mathcal{L}(Q)</script><script type="math/tex; mode=display">=\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}\sum_z p(z|x)\mathrm{log}p(y|z)</script><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}E_{z\sim p(z|x)\mathrm{log}p(y|z)} \tag{2}</script><h4 id="E步"><a href="#E步" class="headerlink" title="E步"></a>E步</h4><p>因为：<script type="math/tex">L(\Theta;D)-\mathcal{L}(Q)=\sum_zQ(z)\mathrm{log}\frac{Q(z)}{p(z|y)}</script></p><script type="math/tex; mode=display">=KL(Q(z)||p(z|y))=KL(p(z|x)||p(z|y))</script><p>所以在<strong>E步</strong>，需要最小化上式：</p><script type="math/tex; mode=display">\Theta_{z|x}=\mathrm{argmin}_{\Theta_{z|x}}KL(p(z|x)||p(z|y)) \tag{4}</script><h3 id="双向训练"><a href="#双向训练" class="headerlink" title="双向训练"></a>双向训练</h3><h4 id="X-to-Y方向"><a href="#X-to-Y方向" class="headerlink" title="X\to Y方向"></a><script type="math/tex">X\to Y</script>方向</h4><h5 id="E：最优化-Theta-z-x"><a href="#E：最优化-Theta-z-x" class="headerlink" title="E：最优化\Theta_{z|x}"></a>E：最优化<script type="math/tex">\Theta_{z|x}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{z|x}}KL(p(z|x)||p(z|y)) \tag{5}</script><h5 id="M：最优化-Theta-y-z"><a href="#M：最优化-Theta-y-z" class="headerlink" title="M：最优化\Theta_{y|z}"></a>M：最优化<script type="math/tex">\Theta_{y|z}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}E_{z\sim p(z|x)}\mathrm{log}p(y|z) \tag{6}</script><h4 id="Y-to-X方向"><a href="#Y-to-X方向" class="headerlink" title="Y\to X方向"></a><script type="math/tex">Y\to X</script>方向</h4><h5 id="E：最优化-Theta-z-y"><a href="#E：最优化-Theta-z-y" class="headerlink" title="E：最优化\Theta_{z|y}"></a>E：最优化<script type="math/tex">\Theta_{z|y}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{z|y}}KL(p(z|y)||p(z|x)) \tag{7}</script><h5 id="M：最优化-Theta-x-z"><a href="#M：最优化-Theta-x-z" class="headerlink" title="M：最优化\Theta_{x|z}"></a>M：最优化<script type="math/tex">\Theta_{x|z}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{x|z}}\sum_{(x,y)\in D}E_{z\sim p(z|y)}\mathrm{log}p(x|z) \tag{8}</script><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>训练过程的难点在于<strong>分解候选词的搜索空间（exponential search space of the translation candidates）</strong>，这里使用<strong>采样</strong>方法。</p><p>将<font color="red">(5)(7)</font>式改写成如下形式：</p><script type="math/tex; mode=display">\bigtriangledown_{\Theta_{z|x}}KL(p(z|x)||p(z|y))</script><script type="math/tex; mode=display">=E_{z\sim p(z|x)}\mathrm{log}\frac{p(z|x)}{p(z|y)}\bigtriangledown_{\Theta_{z|x}}\mathrm{log}p(z|x)\tag{9}</script><script type="math/tex; mode=display">\bigtriangledown_{\Theta_{z|y}}KL(p(z|y)||p(z|x))</script><script type="math/tex; mode=display">=E_{z\sim p(z|y)}\mathrm{log}\frac{p(z|y)}{p(z|x)}\bigtriangledown_{\Theta_{z|y}}\mathrm{log}p(z|y)\tag{10}</script><p>因为数据的<strong>target</strong>是由模型生成的，效果不好，使用不使用<strong>BLEU</strong>，而是使用<strong>IBM</strong>loss函数。</p><p><strong>REF</strong>:</p><p></p><div id="#refer-*"></div>[*]<a href="https://arxiv.org/abs/1805.04813">Triangular Architecture for Rare Language Translation</a><p></p>]]></content>
    
    
    <summary type="html">本文是对Triangular Architecture for Rare Language Translation论文的阅读总结。</summary>
    
    
    
    
    <category term="Paper" scheme="https://1.15.86.100/tags/Paper/"/>
    
    <category term="Monolingual Data" scheme="https://1.15.86.100/tags/Monolingual-Data/"/>
    
  </entry>
  
  <entry>
    <title>KL-Divergence</title>
    <link href="https://1.15.86.100/2021/04/20/KL-Divergence/"/>
    <id>https://1.15.86.100/2021/04/20/KL-Divergence/</id>
    <published>2021-04-20T12:44:22.000Z</published>
    <updated>2021-04-20T12:58:43.752Z</updated>
    
    <content type="html"><![CDATA[<h1 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL Divergence"></a>KL Divergence</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><h3 id="连续形式"><a href="#连续形式" class="headerlink" title="连续形式"></a>连续形式</h3><script type="math/tex; mode=display">\mathrm{KL}(P||Q)=\int_x P(x)\mathrm{log}\frac{P(x)}{Q(x)}</script><h3 id="离散形式"><a href="#离散形式" class="headerlink" title="离散形式"></a>离散形式</h3><script type="math/tex; mode=display">\mathrm{KL}(P||Q)=\sum_{i=1}^N P(x_i)\mathrm{log}\frac{P(x_i)}{Q(x_i)}</script><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><p>只有在<script type="math/tex">P(x)=Q(x)</script>时等于0，其他任何时候都大于0。</p>]]></content>
    
    
    <summary type="html">本文主要介绍KL散度概念</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Math" scheme="https://1.15.86.100/tags/Math/"/>
    
  </entry>
  
</feed>
