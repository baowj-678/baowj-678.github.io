<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BaoWJ&#39;s Blog</title>
  
  
  <link href="https://1.15.86.100/atom.xml" rel="self"/>
  
  <link href="https://1.15.86.100/"/>
  <updated>2021-04-18T09:00:32.020Z</updated>
  <id>https://1.15.86.100/</id>
  
  <author>
    <name>Bao Wenjie</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Nginx</title>
    <link href="https://1.15.86.100/2021/04/18/Nginx/"/>
    <id>https://1.15.86.100/2021/04/18/Nginx/</id>
    <published>2021-04-18T08:17:19.000Z</published>
    <updated>2021-04-18T09:00:32.020Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="配置文件位置"><a href="#配置文件位置" class="headerlink" title="配置文件位置"></a>配置文件位置</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/nginx/conf.d</span><br></pre></td></tr></tbody></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx start</span><br></pre></td></tr></tbody></table></figure><h3 id="停止"><a href="#停止" class="headerlink" title="停止"></a>停止</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx stop</span><br></pre></td></tr></tbody></table></figure><h3 id="重启"><a href="#重启" class="headerlink" title="重启"></a>重启</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx [restart|reload|force-reload|status|configtest|rotate|upgrade]</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <summary type="html">Nginx相关操作</summary>
    
    
    
    
    <category term="Nginx, Linux" scheme="https://1.15.86.100/tags/Nginx-Linux/"/>
    
  </entry>
  
  <entry>
    <title>ML Metrics</title>
    <link href="https://1.15.86.100/2021/04/17/ML-Metrics/"/>
    <id>https://1.15.86.100/2021/04/17/ML-Metrics/</id>
    <published>2021-04-17T02:12:13.000Z</published>
    <updated>2021-04-17T02:12:13.194Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Standardization</title>
    <link href="https://1.15.86.100/2021/04/03/Standardization/"/>
    <id>https://1.15.86.100/2021/04/03/Standardization/</id>
    <published>2021-04-03T08:16:38.000Z</published>
    <updated>2021-04-03T09:26:38.015Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>数据标准化</strong>就是把数据集中服从不同”分布“的各个属性空间，通过数学变换到同一的”分布空间“，一般映射到<strong>[0,1]</strong>范围。</p><h3 id="数据标准化的必要性"><a href="#数据标准化的必要性" class="headerlink" title="数据标准化的必要性"></a>数据标准化的必要性</h3><p>例如给定一条数据，其不同属性的值分别为</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">100, 1.0, 0.1, 0.01</span><br></pre></td></tr></tbody></table></figure><p>那么，假如每个属性都有一个0.01的微小扰动，则第四个属性的扰动对整个数据的影响远远大于第一个属性。</p><p>同时，对于机器学习模型，数据未标准化会对模型产生影响。</p><h2 id="数据标准化方法"><a href="#数据标准化方法" class="headerlink" title="数据标准化方法"></a>数据标准化方法</h2><p>数据集：</p><script type="math/tex; mode=display">X=\{x_1,x_2, \dots,x_n\}</script><h3 id="min-max方法"><a href="#min-max方法" class="headerlink" title="min-max方法"></a>min-max方法</h3><p>即通过<strong>线性变换</strong>将数据映射到<strong>[0,1]</strong>范围。</p><script type="math/tex; mode=display">f(x_i)=\frac{x_i-\mathrm{min}(X)}{\mathrm{max}(X)-\mathrm{min}(X)}\ for\ 1\le i \le n</script><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p2.png" alt=""></p><h3 id="z-score方法"><a href="#z-score方法" class="headerlink" title="z-score方法"></a>z-score方法</h3><p>通过<strong>按比例缩放</strong>的形式，将数据映射到<strong>[-x,x]</strong>范围，要求<script type="math/tex">\mu=0,\sigma=1</script>。</p><script type="math/tex; mode=display">f(x_i)=\frac{x_i- \mu}{\sigma},\mu=\mathrm{mean}(X),\sigma=\sqrt{\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2}</script><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p3.png" alt=""></p><h3 id="logistic方法"><a href="#logistic方法" class="headerlink" title="logistic方法"></a>logistic方法</h3><p>通过函数映射，将数据映射到<strong>[0,1]</strong>范围。</p><script type="math/tex; mode=display">f(x_i)=\frac{1}{1+e^{-x_i}}</script><p><img src="/2021/04/03/Standardization/p1.png" alt=""></p><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p4.png" alt=""></p><p><strong>Code</strong></p><p><a href="https://github.com/baowj-678/Machine-Learning/tree/master/Standardization">github:baowj-678</a></p>]]></content>
    
    
    <summary type="html">数据标准化是特征工程或者数据预处理中很关键的一个环节，这里会对数据标准化进行简单介绍。</summary>
    
    
    
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>Feature Selection: Embedding</title>
    <link href="https://1.15.86.100/2021/04/02/Feature-Selection-Embedding/"/>
    <id>https://1.15.86.100/2021/04/02/Feature-Selection-Embedding/</id>
    <published>2021-04-02T12:28:50.000Z</published>
    <updated>2021-04-03T13:18:00.392Z</updated>
    
    <content type="html"><![CDATA[<h1 id="嵌入式特征选择"><a href="#嵌入式特征选择" class="headerlink" title="嵌入式特征选择"></a>嵌入式特征选择</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>嵌入式特征选择（Embedding）</strong>是将特征选择和模型训练融为一体，也就是模型中包括了特征选择的过程，而训练时只需将原始数据全部输入到模型中即可。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>给定数据集</p><script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\}</script><script type="math/tex; mode=display">x_i\in \mathbb{R}^d</script><script type="math/tex; mode=display">y_i\in \mathbb{R}</script><p>如果使用<strong>线性回归模型</strong>，则优化目标为：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d</script><p>而上述模型可能会陷入<strong>过拟合（over fitting）</strong>，为了解决这个问题，会引入<strong>L1正则化</strong>和<strong>L2正则化</strong>；</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p><strong>L2正则化</strong>将优化目标修改为：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d+\lambda||w||_2^2,\lambda>0</script><p>该模型称为<strong>岭回归</strong>，能降低过拟合风险。</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>将<strong>L2正则化</strong>的<strong>2范数</strong>替换成<strong>1范数</strong>，就得到<strong>L1正则化</strong>：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d+\lambda||w||_1,\lambda>0</script><p>该模型称为<strong>LASSO</strong>。</p><p><strong>L1正则化</strong>除了能降低<strong>过拟合</strong>风险，而且该模型还会倾向于<strong>”稀疏解“</strong>，即该模型倾向于只使用原数据的某些<strong>子属性</strong>，这就间接等同于进行了<strong>特征选择</strong>。</p><p>下面介绍为何会倾向<strong>稀疏解</strong>。</p><p><img src="/2021/04/02/Feature-Selection-Embedding/p1.png" alt="image-20210403211035916"></p><p>假设该模型只有两个参数<script type="math/tex">w_1,w_2</script>，则其<strong>L1范数</strong>和<strong>L2范数</strong>等值线如图所示。<strong>平方项</strong>（即原始的最优化目标）等值线近似如图所示。可以发现，平方误差加上范数误差，对于<strong>L1范数</strong>来说，最小值易在<strong>两个坐标轴上取的</strong>，而<strong>L2范数</strong>偏向在<strong>第一象限取的</strong>。当最优值在坐标轴取的时，<script type="math/tex">w_1,w_2</script>其中一个值就为0，也就是该特征被剔除了，所以易得到<strong>稀疏解</strong>。</p><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p253</p>]]></content>
    
    
    <summary type="html">介绍嵌入式(Embedding)特征选择的方法和L1正则化相关问题。</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
    <category term="L1" scheme="https://1.15.86.100/tags/L1/"/>
    
  </entry>
  
  <entry>
    <title>Feature Selection: Wrapper</title>
    <link href="https://1.15.86.100/2021/04/02/Feature-Selection-Wrapper/"/>
    <id>https://1.15.86.100/2021/04/02/Feature-Selection-Wrapper/</id>
    <published>2021-04-02T12:28:38.000Z</published>
    <updated>2021-04-02T12:57:57.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="包裹式特征选择"><a href="#包裹式特征选择" class="headerlink" title="包裹式特征选择"></a>包裹式特征选择</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>包裹式特征选择</strong>（Wrapper）是特征选择的三大方法之一，其主要思路是：利用选择的特征的子数据集进行训练，并把训练结果直接作为评判特征选择的标准。</p><p>所以在<strong>特定模型</strong>的最终结果上，<strong>包裹式特征选择</strong>比<strong>选择式</strong>更好。</p><h2 id="LVW"><a href="#LVW" class="headerlink" title="LVW"></a>LVW</h2><p><strong>拉斯维加斯方法（LVW，Las Vegas Wrapper）</strong>是一种典型的<strong>包裹式特征选择</strong>方法。</p><p>该方法每次随机选取特征子集，然后送入模型，如果模型损失下降则更新特征子集，依次迭代进行。</p><hr><p><strong>输入</strong>：数据集<script type="math/tex">D</script>；</p><p>​            选择的特征集<script type="math/tex">A</script>；</p><p>​            某个学习器<script type="math/tex">\zeta</script>；</p><p>​            停止条件控制参数<script type="math/tex">T</script>【搜索次数】；</p><p><strong>过程</strong>：</p><ol><li><script type="math/tex">E=\infty</script>；</li><li><script type="math/tex">d=|A|</script>；</li><li><script type="math/tex">A^*=A</script>；</li><li><script type="math/tex">t=0</script>；</li><li><strong>while</strong> <script type="math/tex">\ t < T</script> <strong>do</strong></li><li>​    随机产生<strong>特征子集</strong><script type="math/tex">A'</script>；</li><li>​    <script type="math/tex">d'=|A'|</script>；</li><li>​    <script type="math/tex">E'=\mathrm{Cross Validation}(\zeta(D^{A'}))</script>【计算<strong>loss</strong>】；</li><li>​    <strong>if</strong> <script type="math/tex">(E'<E)\ or\ ((E'=E)\ and\ (d'<d))</script> 【<strong>loss</strong>降低，或者<strong>loss</strong>不变，特征数量减少】<strong>then</strong></li><li>​         <script type="math/tex">t=0</script>；【更新】</li><li>​        <script type="math/tex">E=E'</script>；</li><li>​        <script type="math/tex">d=d'</script>；</li><li>​        <script type="math/tex">A^*=A'</script>；</li><li>​    <strong>else</strong></li><li>​        <script type="math/tex">t=t+1</script></li><li>​    <strong>end if</strong></li><li><strong>end while</strong></li></ol><p><strong>输出</strong>：特征子集 <script type="math/tex">A^*</script></p><hr><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p251</p>]]></content>
    
    
    <summary type="html">介绍包裹式(Wrapper)特征选择的方法，以及典型的算法拉斯维加斯方法(LVW)</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
    <category term="LVW" scheme="https://1.15.86.100/tags/LVW/"/>
    
  </entry>
  
  <entry>
    <title>Server</title>
    <link href="https://1.15.86.100/2021/04/02/Server/"/>
    <id>https://1.15.86.100/2021/04/02/Server/</id>
    <published>2021-04-02T09:29:32.000Z</published>
    <updated>2021-04-02T09:31:07.237Z</updated>
    
    <content type="html"><![CDATA[<h1 id="服务器管理"><a href="#服务器管理" class="headerlink" title="服务器管理"></a>服务器管理</h1><h3 id="查看IP登录情况"><a href="#查看IP登录情况" class="headerlink" title="查看IP登录情况"></a>查看IP登录情况</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> -n 指定登录次数前多少名</span></span><br><span class="line">awk '{print $1}' blog-log |sort |uniq -c|sort -nr|head -n 10000</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <summary type="html">服务器管理云云</summary>
    
    
    
    
    <category term="Server" scheme="https://1.15.86.100/tags/Server/"/>
    
  </entry>
  
  <entry>
    <title>Deep Forest</title>
    <link href="https://1.15.86.100/2021/04/02/Deep-Forest/"/>
    <id>https://1.15.86.100/2021/04/02/Deep-Forest/</id>
    <published>2021-04-02T08:56:45.000Z</published>
    <updated>2021-04-02T09:12:19.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Deep-Forest-christmas-tree"><a href="#Deep-Forest-christmas-tree" class="headerlink" title="Deep Forest:christmas_tree:"></a>Deep Forest<span class="github-emoji"><span>🎄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f384.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h1><h3 id="简介-ticket"><a href="#简介-ticket" class="headerlink" title="简介:ticket:"></a>简介<span class="github-emoji"><span>🎫</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ab.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><p>用<strong>深度神经网络(DNN)</strong>的思路来组织<strong>随机森林(RF)</strong>，极大地提高了随机森林的准确率。</p><h3 id="安装-wrench"><a href="#安装-wrench" class="headerlink" title="安装:wrench:"></a>安装<span class="github-emoji"><span>🔧</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install deep-forest</span><br></pre></td></tr></tbody></table></figure><h3 id="函数-funeral-urn"><a href="#函数-funeral-urn" class="headerlink" title="函数:funeral_urn:"></a>函数<span class="github-emoji"><span>⚱</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/26b1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><p>用法详见<strong>测试代码</strong></p><ul><li><p><strong>deepforest.CascadeForestClassifier</strong>：对<strong>Deep Forest</strong>的实现；</p></li><li><p><strong>deepforest.DecisionTreeClassifier</strong>：<strong>Deep Forest</strong>的树的实现；</p><p>​    </p></li></ul><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>分类的类别需从<strong>0</strong>开始标记，即<strong>label={0,1,2…}</strong></p><h3 id="原论文阅读-page-with-curl"><a href="#原论文阅读-page-with-curl" class="headerlink" title="原论文阅读:page_with_curl:"></a>原<a href="https://arxiv.org/pdf/1702.08835.pdf">论文</a>阅读<span class="github-emoji"><span>📃</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><h4 id="background"><a href="#background" class="headerlink" title="background"></a>background</h4><ul><li><p>对<strong>深度神经网络(DNN)</strong>成功的原因分析：</p><ol><li>一层一层的堆叠（<em>layer-by-layer processing</em>）；</li><li>模型内部的数据表征方式的转变【例如LSTM中<strong>词向量</strong>的传递和维度变化】（<em>in-model feature transformation</em>）；</li><li>足够的模型复杂度（<em>sufficient model complexity</em>）；</li></ol></li><li><p><strong>DNN</strong>的缺陷：</p><ol><li>超参数太多（too many hyper-parameters），模型表现十分依赖参数选择和训练（parameter tuning）；</li><li>需要大量训练数据（a huge amount of training data）；</li><li><strong>黑箱系统</strong>（玄学），很难进行理论分析（theoretical analysis）；</li><li>模型结构的确定先于模型训练；</li></ol></li></ul><h4 id="inspiration"><a href="#inspiration" class="headerlink" title="inspiration"></a>inspiration</h4><p><strong>从DNN</strong>：</p><ul><li><p>从<strong>DNN</strong>中观察到，在<strong>DNN</strong>每层的传播中，<strong>数据特征（feature）</strong>越来越集中，越来越抽象。</p></li><li><p><strong>DNN</strong>的成功和<strong>模型复杂度</strong>关系不大，否则<em>为什么无限扩大模型参数量并不能提升模型效果？</em>；</p></li><li><strong>DNN</strong>的层次性和<strong>决策树</strong>的层次性不一样：<ul><li><strong>决策树</strong>始终利用的是输入的数据，并没有对<strong>数据表征（feature）</strong>做出任何改变（work on the original feature representation），没有出现（<em>in-model feature transformation</em>）；</li><li><strong>DNN</strong>每一层的输出都会对<strong>数据表征（feature）</strong>做出改变；</li></ul></li></ul><p><strong>从集成学习（Ensemble Learning）</strong></p><ul><li><p>要做好集成学习，每个<strong>学习单元（learner）</strong>要做到<strong>准确（accurate）</strong> <strong>多样（diverse）</strong>；</p></li><li><p>实践中常常会通过技巧提高模型的<strong>多样性</strong>：</p><ul><li><p><strong>数据采样（data sample manipulation）</strong>：</p><p>  从原始数据集中采样出不同的<strong>子训练集</strong>来训练不同的<strong>学习单元（learner）</strong>；</p><p>  例如：</p><p>  <strong>Bagging</strong>中的<strong>bootstrap sampling</strong>；</p><p>  <strong>AdaBoost</strong>中的<strong>sequential importance sampling</strong>；</p></li><li><p><strong>输入特征采样（input feature manipulation）</strong>：</p><p>  从原始的数据特征中采样出不同的子特征（feature）生成<strong>子空间（subspace）</strong>，训练不同的<strong>学习单元（learner）</strong>；</p></li><li><p><strong>学习参数区别（learning parameter manipulation）</strong>：</p><p>  不同的<strong>学习单元（learner）</strong>采用不同的参数训练；</p></li><li><p><strong>输出表征区别（output representation manipulation）</strong>：</p><p>  对不同的学习单元使用不同的<strong>表征（representation）</strong>；</p></li></ul></li></ul><h4 id="gcForest"><a href="#gcForest" class="headerlink" title="gcForest"></a>gcForest</h4><p><strong>层次森林结构（Cascade Forest Structure）</strong></p><p><img src="/2021/04/02/Deep-Forest/p1.png" alt="pic"></p><ul><li><p>每一层从其前面的层获取数据，再将数据传递到下一层；</p></li><li><p>每一层都是<strong>随机森林</strong>的集成；</p></li><li><p>每个森林中的树个数作为超参数；</p></li><li><p>图中：</p><ul><li><p>黑色森林是<strong>随机森林（random forest）</strong>；</p><p>  每个森林包括500棵<strong>随机树</strong>，树的每个节点从随机选择的$\sqrt{d}$（d是特征个数）个<strong>候选特征</strong>中按照<strong>gini</strong>系数选择一个特征来切分；</p></li><li><p>蓝色森林是<strong>完全随机树森林（completely-random tree forest）</strong>；</p><p>  每个森林包括500棵<strong>完全随机树</strong>，树的每个节点会从<strong>所有的特征</strong>中选择一个特征切分出来，树生长直到完全是叶子；</p></li><li><p>假设数据分为三类，每个<strong>随机森林</strong>将输出<strong>三维向量</strong>，然后将所有向量连接（concatenate）作为输出；</p></li></ul></li><li><p>每个<strong>随机森林</strong>的输出是所有树的平均，如下图：<img src="/2021/04/02/Deep-Forest/p2.png" alt="pic"></p></li><li><p>为了减小<strong>过拟合（overfitting）</strong>风险，每个<strong>随机森林</strong>的输出都使用<strong>K折交叉验证（k-fold cross validation）</strong>：</p><ul><li>每个条数据会被训练<em>k-1</em>次，生成<em>k-1</em>个向量，然后平均作为该树的输出；</li><li>交叉验证的结果作为判定条件，如果模型效果相对上一层有提高则继续扩展下一层，否则结束；</li></ul></li></ul><p><strong>卷积特征提取（Multi-Grained Scanning）</strong></p><p><img src="/2021/04/02/Deep-Forest/p3.png" alt="pic"></p><ul><li><p>用一个一维或者二维的窗口扫描原数据，将窗口数据提取出来作为<strong>新特征</strong>；</p></li><li><p>将<strong>新特征</strong>送入训练，再将结果连接起来，作为最终的输出结果；</p></li><li><p>有可能某些<strong>新特征</strong>与结果丝毫没有关系（例如：需要识别一张图片的<em>汽车</em>，但提取出来的小片段不包含任何相关内容），这时，可以把<strong>新特征</strong>看成一种<strong>output representation manipulation</strong>，可以提高模型的多样性；</p></li><li><p>当<strong>新特征</strong>太多时，可以对其进行<strong>采样</strong>；</p></li><li><p>模型中通常使用不同大小的窗口进行特征提取，如下图：</p><p>  <img src="/2021/04/02/Deep-Forest/p4.png" alt="pic"></p></li></ul><p><strong>Code</strong>：</p><p><a href="https://github.com/baowj-678/Machine-Learning/tree/master/Deep-Forest">github:baowj-678</a></p><p><strong>REF</strong>：</p><p><a href="https://arxiv.org/pdf/1702.08835.pdf">arXiv:1702.08835 [cs.LG]</a></p>]]></content>
    
    
    <summary type="html">用深度神经网络(DNN)的思路来组织随机森(RF)，可以极大地提高了随机森林的准确率。</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Decision Tree" scheme="https://1.15.86.100/tags/Decision-Tree/"/>
    
    <category term="Random Forest" scheme="https://1.15.86.100/tags/Random-Forest/"/>
    
  </entry>
  
  <entry>
    <title>Windows Terminal添加登录腾讯云服务器</title>
    <link href="https://1.15.86.100/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>https://1.15.86.100/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/</id>
    <published>2021-04-02T04:27:36.000Z</published>
    <updated>2021-04-02T05:10:15.999Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Windows-Terminal配置腾讯云服务器"><a href="#Windows-Terminal配置腾讯云服务器" class="headerlink" title="Windows Terminal配置腾讯云服务器"></a>Windows Terminal配置腾讯云服务器</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol><li>租一台<strong>腾讯云服务器</strong>；</li><li>本地安装好<strong>Windows Terminal</strong>；</li></ol><h2 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h2><h3 id="生成SSH私钥"><a href="#生成SSH私钥" class="headerlink" title="生成SSH私钥"></a>生成SSH私钥</h3><p>在腾讯云<strong>控制台</strong>，进入<strong>SSH密钥</strong>界面，点击<strong>创建密钥</strong>：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p1.png" alt="p1"></p><p>填写密钥的名称（描述密钥存放位置和用处，方便记忆）：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p2.png" alt="p2"></p><p>点击<strong>确定</strong>，系统会自动下载密钥文件。</p><h3 id="保存密钥"><a href="#保存密钥" class="headerlink" title="保存密钥"></a>保存密钥</h3><p>将上一步下载的密钥改名为<strong>id_rsa_tencent</strong>（自定义），并置于<em>~/.ssh/</em>目录下：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p3.png" alt="p3"></p><h3 id="配置Windows-Terminal"><a href="#配置Windows-Terminal" class="headerlink" title="配置Windows Terminal"></a>配置Windows Terminal</h3><p>打开<strong>Windows Terminal</strong>，点击设置：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p4.png" alt="p4"></p><p>在<strong>list</strong>下添加下列<strong>配置信息</strong>：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p5.png" alt="p5"></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;guid&quot;</span>: <span class="string">&quot;&#123;******************************&#125;&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;TencentCloud&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;commandline&quot;</span>: <span class="string">&quot;ssh -i 密钥文件位置 服务器用户名@服务器ip地址&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;hidden&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">&quot;icon&quot;</span>: <span class="string">&quot;图标位置&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中：</p><p><strong>guid</strong>可以在<a href="https://www.guidgen.com/"><em>guidgen.com</em></a>生成。</p><h2 id="配置结果"><a href="#配置结果" class="headerlink" title="配置结果"></a>配置结果</h2><p>点击<strong>腾讯云图标</strong>即可自动登录，并进入<strong>控制台</strong>：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p6.png" alt="p6"></p>]]></content>
    
    
    <summary type="html">在windows terminal里添加腾讯云服务器配置，使其可以自动利用ssh登录服务器。</summary>
    
    
    
    
    <category term="Server" scheme="https://1.15.86.100/tags/Server/"/>
    
  </entry>
  
  <entry>
    <title>Position Encoding Methods of Transformer</title>
    <link href="https://1.15.86.100/2021/03/31/Position-Encoding-Methods-of-Transformer/"/>
    <id>https://1.15.86.100/2021/03/31/Position-Encoding-Methods-of-Transformer/</id>
    <published>2021-03-31T09:05:07.000Z</published>
    <updated>2021-04-16T15:44:46.850Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h1><h2 id="Background1"><a href="#Background1" class="headerlink" title="Background1"></a>Background<a href="#refer-anchor-1"><sup>1</sup></a></h2><p>2017年<strong>谷歌</strong>提出的<strong>Transformer</strong>模型深刻地影响了<strong>NLP</strong>领域，<strong>Transformer</strong>模型是基于<strong>Attention</strong>机制的<strong>降噪自编码器（denoising autoencoder）</strong>模型。因为采用了这种架构，所有的输入文本都<strong>平行地进行</strong>计算。优点是提高了模型效率，但缺点就导致了需要加入<strong>位置编码</strong>，并且<strong>位置编码</strong>很大程度上影响着模型的效果。</p><h2 id="Vanilla-Position-Encoding1"><a href="#Vanilla-Position-Encoding1" class="headerlink" title="Vanilla Position-Encoding1"></a>Vanilla Position-Encoding<a href="#refer-anchor-1"><sup>1</sup></a></h2><h3 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h3><p>在这篇论文中，作者提出下面的编码方法：</p><script type="math/tex; mode=display">PE_{(pos,2i)} =sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>其中： </p><ul><li><p><script type="math/tex">pos</script>：句中字词（token）所在的位置；</p></li><li><p><script type="math/tex">i</script>：编码的<strong>位置向量</strong>的一个维度；</p></li></ul><p>这个编码中，<strong>每个位置的每个维度</strong>都服从一个<strong>正弦函数</strong>的“分布”，且可以编码的距离也较长。</p><p>该编码可以学习到<strong>相对位置</strong>，对于任意一个位置<script type="math/tex">PE_{pos+k}</script>都可以表示成<script type="math/tex">PE_{pos}</script>的<strong>线性函数</strong>。</p><h3 id="编码原理2"><a href="#编码原理2" class="headerlink" title="编码原理2"></a>编码原理<a href="#refer-anchor-2"><sup>2</sup></a></h3><p>假设模型为<script type="math/tex">f(\dots,x_m,\dots,x_n,\dots)</script>，其中参数都是输入的<strong>词向量</strong>，而<script type="math/tex">x_m,x_n</script>是在<script type="math/tex">n,m</script>位置上的词向量。这就是<strong>Transformer</strong>的基本模型（不包括位置编码）。</p><p>在加入位置编码后，变为：<script type="math/tex">\tilde{f}(\dots,x_m+p_m,\dots,x_n+p_n,\dots)</script>。</p><p>然后用<strong>泰勒公式</strong>将其展开：</p><script type="math/tex; mode=display">\tilde{f}\approx f+p^T_m\frac{\partial f}{\partial x_m}+p^T_n\frac{\partial f}{\partial x_n}+\frac{1}{2}p^T_m\frac{\partial^2 f}{\partial x_m^2}p_m+\frac{1}{2}p^T_n\frac{\partial^2 f}{\partial x_n^2}p_n+p_m^T\frac{\partial^2f}{\partial x_m\partial x_n}p_n</script><p>可以发现：</p><ul><li><strong>第一项</strong>和位置无关，<strong>第二、三、四、五项</strong>只依赖于单一位置，所以他们只是<strong>绝对位置信息</strong>；</li><li><strong>第六项</strong>简化为<script type="math/tex">p^T_m\mathcal{H}p_n</script>，目标是希望该项能表达<strong>相对位置信息</strong>；</li></ul><p>将<script type="math/tex">\mathcal{H}</script>简化为<script type="math/tex">I</script>（单位阵），则位置编码的相对信息为<script type="math/tex">p_m^Tp_n</script></p><p>当维度<script type="math/tex">d=2</script>时，<strong>Sinusoidal</strong>编码为<script type="math/tex">p_m=[\mathrm{cos}(\frac{m}{10000^{\theta}}),\mathrm{sin}(\frac{m}{10000^{\theta}})]</script>,<script type="math/tex">p_n=[\mathrm{cos}(\frac{n}{10000^{\theta}}),\mathrm{sin}(\frac{n}{10000^{\theta}})]</script>。而<script type="math/tex">p^T_mp_n</script>计算结果为：</p><script type="math/tex; mode=display">\mathrm{cos}(\frac{m-n}{10000^\theta})</script><p>所以可以表示成关于<script type="math/tex">m-n</script>的函数，进而可以表示<strong>相对位置信息</strong>。</p><p>当<script type="math/tex">d</script>为多维时，可以把上述方法叠加，编码就变成</p><script type="math/tex; mode=display">p_m=[\mathrm{cos}(\frac{m}{10000^{\theta_1}}),\mathrm{sin}(\frac{m}{10000^{\theta_1}}),\dots,\mathrm{cos}(\frac{m}{10000^{\theta_{d/2}}}),\mathrm{sin}(\frac{m}{10000^{\theta_{d/2}}})]</script><script type="math/tex; mode=display">p^T_mp_n=\sum_d(\mathrm{cos}(\frac{n-m}{10000^{\theta_i}}))</script><p>对于<script type="math/tex">n-m\in [1, 1000]</script>的数值计算结果如下图：</p><p><img src="/2021/03/31/Position-Encoding-Methods-of-Transformer/Figure_1.png" alt=""></p><h3 id="编码的缺点"><a href="#编码的缺点" class="headerlink" title="编码的缺点"></a>编码的缺点</h3><p>由于<strong>Transformer</strong>一般由多层<strong>Self-Attention</strong>叠加构成。而<strong>Vanilla Position Encoding</strong>只在第一层的输入对位置进行了编码，因此在网络的前向传播过程中，位置编码的信息会逐渐消减。</p><p><img src="/2021/03/31/Position-Encoding-Methods-of-Transformer/Figure_2.png" alt="image-20210416230442907" style="zoom:80%;"></p><p>上图是<strong>Gaussian Transformer</strong><a href="#refer-anchor-8"><sup>8</sup></a>论文中的图片，这里主要关注<strong>（a）</strong>，在<strong>（a）</strong>图中，每个柱形的高度代表每个单词对<strong>book</strong>单词的“关注程度”（即Transformer的Attention矩阵的值）。从图中可以发现，对于同一个单词<strong>new</strong>在三个不同的位置，但模型训练的结果显示它们对<strong>book</strong>的关注度相同。这显然是<strong>违反语言常识</strong>的，因为</p><p>第一个<strong>new</strong>是修饰<strong>book</strong>的，显然应该对<strong>book</strong>有更高的关注度；</p><p>而后面两个<strong>new</strong>分别是修饰<strong>friend</strong>以及是<strong>New York</strong>短语的一部分，显然对<strong>book</strong>没什么关注度；</p><p>三相同的单词，在不同的位置，意味着这三个单词在模型中的<strong>词向量</strong>是一样的，唯一区别的只有<strong>位置编码</strong>，但<strong>Attention</strong>结果又没有区别，说明<strong>位置编码没有发挥应有的作用！</strong></p><h2 id="Relative-Position-Encoding3"><a href="#Relative-Position-Encoding3" class="headerlink" title="Relative Position-Encoding3"></a>Relative Position-Encoding<a href="#refer-anchor-3"><sup>3</sup></a></h2><p><strong>Self-Attention</strong>主要将输入的<strong>n</strong>元素：<script type="math/tex">x=(x_1,\dots,x_n),x_i\in \mathbb{R}^{d_x}</script>转化成新的长度为<strong>n</strong>的表示序列：<script type="math/tex">z=(z_1,\dots,z_n),z_i\in\mathbb{R}^{d_z}</script></p><p>对于其中的每个输出元素<script type="math/tex">z_i</script>，<strong>Self-Attention</strong>计算过程如下：</p><script type="math/tex; mode=display">z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V),W^V\in\mathbb{R}^{d_x\times d_z} \tag{1}</script><p>式中：<script type="math/tex">\alpha</script>是权重</p><script type="math/tex; mode=display">\alpha_{ij}=\frac{\mathrm{exp}(e_{ij})}{\sum_{k=1}^n\mathrm{exp}(e_{ik})} \tag{2}</script><p>式中：<script type="math/tex">e_{ij}</script>就是<strong>Q、K</strong>向量作用的结果：</p><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(x_jW^K)}{\sqrt{d_z}},W^K,W^Q\in\mathbb{R}^{d_x\times d_z} \tag{3}</script><h3 id="编码原理"><a href="#编码原理" class="headerlink" title="编码原理"></a>编码原理</h3><p>这里引入<strong>输入元素的成对关系（pairwise relationships between input elements）</strong>【注意，该关系是和位置相关的，与字词语义无关】，其关系可以表示成<strong>有向完全图</strong>。比如<script type="math/tex">x_i,x_j</script>的关系可以被<script type="math/tex">a_{ij}^V,a_{ij}^K\in \mathbb{R}^{d_a},d_a=d_z</script>表示。</p><p>从而，上式<font color="red">(1)</font>可以修改为【因为位置会影响查询的结果】：</p><script type="math/tex; mode=display">z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V+a_{ij}^V) \tag{4}</script><p>上式<font color="red">(3)</font>也可以修改为【因为位置会影响查询的权重】：</p><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(x_jW^K+a_{ij}^K)}{\sqrt{d_z}} \tag{5}</script><p>其中<script type="math/tex">a_{ij}^V,a_{ij}^K</script>可以通过学习得到。</p><p>在语言模型中，为了表示字词的<strong>相对位置关系</strong>，<script type="math/tex">a_{ij}</script>应当只与<script type="math/tex">i-j</script>有关，所以该模型可以简化为：</p><script type="math/tex; mode=display">a_{ij}=a_{i-j}\tag{6}</script><p>同时在实践中，当句子较长时，可以采用<strong>修剪（clip）</strong>方法，如下：</p><script type="math/tex; mode=display">a_{ij}=a_{\mathrm{min(i-j,k)}},k=const</script><h2 id="DeBERTa-Position-Encoding4"><a href="#DeBERTa-Position-Encoding4" class="headerlink" title="DeBERTa Position-Encoding4"></a>DeBERTa Position-Encoding<a href="#refer-anchor-4"><sup>4</sup></a></h2><p>位置编码模型可以简化为：</p><script type="math/tex; mode=display">A_{i,j}=(W_i+P_i)(W_j+P_j)=W_iW_j+W_iP_j+P_iW_j+P_iP_j \tag{*}</script><p><strong>DeBERTa</strong>认为<strong>位置和语义</strong>之间的作用是十分重要的，而因为使用的是<strong>相对位置</strong>，所有位置和位置之间的作用的没用的。所以该模型舍去了<font color="red">(*)</font>的右边第四项。得到下列模型：</p><script type="math/tex; mode=display">Q_c=HW_{q,c},K_c=HW_{k,c},V_c=HW_{v,c}</script><script type="math/tex; mode=display">\tilde{A}_{i,j}=Q_i^cK_j^{cT}+Q_i^cK_{\delta(i,j)}^{rT}+K_j^cQ_{\delta(j,i)}^{rT}</script><script type="math/tex; mode=display">H_o=\mathrm{softmax}(\frac{\tilde{A}}{\sqrt{3d}})V_c</script><h2 id="XL-Position-Encoding5"><a href="#XL-Position-Encoding5" class="headerlink" title="XL Position-Encoding5"></a>XL Position-Encoding<a href="#refer-anchor-5"><sup>5</sup></a></h2><p>该方法在<strong>Transformer-XL</strong>模型中提出，主要考虑的也是<strong>相对位置编码</strong>。</p><p>该论文认为<strong>Vanilla Transformer</strong>是：</p><script type="math/tex; mode=display">A_{i,j} = (E_{x_i}+U_i)^TW_q^TW_k(E_{x_j}+U_j)</script><script type="math/tex; mode=display">=E_{x_i}^TW_q^TW_{k}E_{x_j}+E_{x_i}^TW_q^TW_{k}U_j+U_i^TW_{q}^TW_kE_{x_j}+U_i^TW_{q}^TW_kU_j</script><p>所以，该方法主要是将上式修改成如下形式：</p><script type="math/tex; mode=display">A_{i,j}=E_{x_i}^TW_q^TW_{k,E}E_{x_j}+E_{x_i}^TW_q^TW_{k,R}R_{i-j}+u^TW_{k,E}E_{x_j}+v^TW_{k,R}R_{i-j}</script><p>主要考虑是用相对位置编码<script type="math/tex">R_{i-j}</script>替换绝对位置编码<script type="math/tex">U_j</script>，用另一待训练参数替换绝对位置编码<script type="math/tex">U_i</script>。</p><p>该方法在预训练模型<strong>XLNet</strong>中也有应用。</p><h2 id="Position-aware-Attention-Position-Encoding6"><a href="#Position-aware-Attention-Position-Encoding6" class="headerlink" title="Position-aware Attention Position-Encoding6"></a>Position-aware Attention Position-Encoding<a href="#refer-anchor-6"><sup>6</sup></a></h2><p>该方法是<strong>RPE</strong><a href="#refer-anchor-3"><sup>3</sup></a>的改进，当然该论文不仅提出了该<strong>位置编码</strong>还提出了一种<strong>网络架构</strong>（这里不介绍）。</p><p>该方法将<font color="red">(5)</font>式修改为下面形式：</p><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(x_jW^K)+x_iR_{j-i}^Kx_j^T}{\sqrt{d_z}} \tag{6}</script><p>其中，加号前面部分是<strong>文本相关</strong>的，后面部分是<strong>位置相关</strong>的，并且<script type="math/tex">R_{j-i}</script>是相对位置相关的。</p><p>该模型<strong>亮点</strong>在于在<strong>相对位置</strong>编码中考虑了<strong>文本信息</strong>。</p><p>而后，为了优化参数数量以及计算复杂度，该模型提出将<script type="math/tex">R_{j-i}^K</script>进行<strong>SVD</strong>分解：</p><script type="math/tex; mode=display">R_{j-i}^K=U_c\Gamma^K_{j-i}V_c^T \tag{7}</script><p>其中<script type="math/tex">U_c\in \mathbb{R}^{d_z\times d_c},V_c\in \mathbb{R}^{d_z \times d_c},\Gamma^K_{j-i}\in \mathbb{R}^{d_c\times d_c}</script>是一个<strong>对角阵</strong>。</p><p>为了简化参数，令<script type="math/tex">U_c=W^Q,V_c=W^K</script></p><p>式<font color="red">(7)</font>可以写为：</p><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(x_jW^K)+(x_iW^Q)\Gamma^K_{j-i}(x_j^TW^K)}{\sqrt{d_z}}</script><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(I+\Gamma^K_{j-i})(x_jW^K)}{\sqrt{d_z}} \tag{8}</script><p>式<font color="red">(4)</font>相应改写为：</p><script type="math/tex; mode=display">z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V(I+\Gamma^V_{j-i})) \tag{9}</script><h2 id="T5-Position-Encoding"><a href="#T5-Position-Encoding" class="headerlink" title="T5 Position-Encoding"></a>T5 Position-Encoding</h2><h2 id="Complex-Position-Encoding7"><a href="#Complex-Position-Encoding7" class="headerlink" title="Complex Position-Encoding7"></a>Complex Position-Encoding<a href="#refer-anchor-7"><sup>7</sup></a></h2><p>该论文创新之处在于利用<strong>复数</strong>进行<strong>相对位置</strong>编码，其不仅修改了编码方法，还修改了<strong>神经网络</strong>架构，使之成为<strong>复数神经网络</strong>。</p><p>对于位置向量长度为<script type="math/tex">D</script>的模型，其编码方法如下：</p><script type="math/tex; mode=display">f(j,pos)=[r_{j,1}e^{i(w_{j,1}\times pos+\theta_{j,1})},r_{j,2}e^{i(w_{j,2}\times pos+\theta_{j,2})},\dots,r_{j,D}e^{i(w_{j,D}\times pos+\theta_{j,D})}]</script><p>其中：</p><ul><li><p><script type="math/tex">f(j,pos)</script>：表示<strong>词汇表</strong>中第<script type="math/tex">j</script>个单词在<script type="math/tex">pos</script>位置的编码结果，只和单词有关（可以是词向量），该向量会进行<strong>Attention</strong>；</p></li><li><p><script type="math/tex">r_j\in \mathbb{R}^D</script>：带训练的模型参数；</p></li><li><p><script type="math/tex">w_j\in \mathbb{R}^D</script>：频率相关的向量；</p></li><li><p><script type="math/tex">\theta_j\in\mathbb{R}^D</script>：初相位向量；</p></li></ul><h3 id="编码原理-1"><a href="#编码原理-1" class="headerlink" title="编码原理"></a>编码原理</h3><p>该编码满足下列性质【因为该编码方式是先假定满足性质，而后推导出的编码方程】</p><ul><li>位置的偏移变换和和位置本身无关的，即<script type="math/tex">g(pos+n)=Transform_n(g(pos))</script>；</li><li>有界性，即<script type="math/tex">|g(pos)|\le \delta,pos\in\mathbb{N}</script></li></ul><h3 id="与Vanilla-Tranformer联系"><a href="#与Vanilla-Tranformer联系" class="headerlink" title="与Vanilla Tranformer联系"></a>与Vanilla Tranformer联系</h3><p><strong>Vanilla Tranformer</strong>可以看成该方法的一种特例。</p><p>令<script type="math/tex">f(\cdot,pos,k)=e^{i\times10000 ^{2k/d_{model}}}=cos(10000^{2k/d_{model}}pos)+isin(10000^{2k/d_{model}}pos)</script></p><p>所以：</p><script type="math/tex; mode=display">PE_{pos,2k}=img(f(\cdot,pos,k))</script><script type="math/tex; mode=display">PE_{pos,2k+1}=real(f(\cdot,pos,k))</script><p>与<strong>Complex</strong>不同的是，<strong>Vanilla</strong>将位置编码<strong>加</strong>上<strong>语义编码</strong>，而<strong>Complex</strong>是<strong>乘</strong>上。</p><p><strong>REF</strong>：</p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a><p></p><p></p><div id="refer-anchor-2"></div> [2] <a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/1150593142">Transformer升级之路：Sinusoidal位置编码追根溯源</a><p></p><p></p><div id="refer-anchor-3"></div> [3] <a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a><p></p><p></p><div id="refer-anchor-4"></div> [4] <a href="https://arxiv.org/abs/2006.03654">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a><p></p><p></p><div id="refer-anchor-5"></div> [5] <a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a><p></p><p></p><div id="refer-anchor-6"></div> [6] <a href="https://ieeexplore.ieee.org/document/9099090">Improving Self-Attention Networks With Sequential Relations</a><p></p><p></p><div id="refer-anchor-7"></div> [7] <a href="https://arxiv.org/abs/1912.12333">ENCODING WORD ORDER IN COMPLEX  EMBEDDINGS</a><p></p><p></p><div id="refer-anchor-8"></div> [8] <a href="https://ojs.aaai.org//index.php/aaai/article/view/4614">Gaussian Transformer: A Lightweight Approach for Natural Language Inference</a><p></p>]]></content>
    
    
    <summary type="html">Transformer模型用Attention机制取代了递归的序列机制，因此其位置编码对模型效果有十分重要的影响；所以本文梳理了Transformer的经典位置编码方法。</summary>
    
    
    
    
    <category term="NLP" scheme="https://1.15.86.100/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://1.15.86.100/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Feature Selection: Relief</title>
    <link href="https://1.15.86.100/2021/03/30/Feature-Selection-Relief/"/>
    <id>https://1.15.86.100/2021/03/30/Feature-Selection-Relief/</id>
    <published>2021-03-30T11:22:38.000Z</published>
    <updated>2021-04-02T12:52:49.759Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Relief"><a href="#Relief" class="headerlink" title="Relief"></a>Relief</h1><p><strong>Relief（Relevant Features）</strong>是一种著名的<strong>过滤式特征选择</strong>方法，所谓<strong>过滤式</strong>就是先对<strong>数据集</strong>进行<strong>特征选择</strong>，然后再利用选择的数据集训练学习器。</p><p><strong>Relief</strong>设置了一个“相关统计量”，来度量一个特征的重要性，</p><h2 id="Relief算法过程（二分类）"><a href="#Relief算法过程（二分类）" class="headerlink" title="Relief算法过程（二分类）"></a>Relief算法过程（二分类）</h2><p>给定训练集：</p><script type="math/tex; mode=display">\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m\}</script><p>对于每个实例$x_i$：</p><p><strong>Relief</strong>，在<script type="math/tex">x_i</script>的<strong>同类样本</strong>（即<script type="math/tex">y_i</script>相同）中寻找<strong>最近邻 </strong><script type="math/tex">x_{i,nh}</script>，称为<strong>猜中近邻（near-hit）</strong>；</p><p>然后，在<script type="math/tex">x_i</script>的<strong>异类样本</strong>（即<script type="math/tex">y_i</script>不同）中寻找<strong>最近邻</strong><script type="math/tex">x_{i,nm}</script>​，称为<strong>猜错近邻（near-miss）</strong>；</p><p>则，对于属性$j$，其<strong>相关统计量</strong>为：</p><script type="math/tex; mode=display">\delta^j=\sum_idiff(x_i^j,x_{i,nm}^j)^2-diff(x_i^j,x_{i,nh}^j)^2</script><p>其中：</p><p>$x_a^j$：样本实例$x_a$在属性$j$上的取值；</p><p>$diff(a,b)$：$a$，$b$之间的差值；</p><ul><li>若是<strong>连续数据</strong>：<script type="math/tex">diff(a,b)=|a-b|</script></li><li>若是<strong>离散数据</strong>：<script type="math/tex">diff(a,b)=0,if\ a=b</script>；<script type="math/tex">diff(a,b)=1,if\ a\ne b</script></li></ul><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><ul><li>如果<script type="math/tex">sum_idiff(x_i^j,x_{i,nm}^j)^2-diff(x_i^j,x_{i,nh}^j)^2>0</script>，即<script type="math/tex">x_i</script>，与其<strong>猜中近邻</strong><script type="math/tex">x_{i,nh}</script>在属性<script type="math/tex">j</script>上的<strong>距离</strong>小于<strong>猜错近邻</strong><script type="math/tex">x_{i,nm}</script>。则说明属性$j$对于区分类别是有益的，所以其<strong>相关统计量</strong>增大；</li><li>相反情况，其<strong>相关统计量</strong>减小；</li><li>最终，<strong>相关统计量</strong>值越大的属性，说明其作用越显著；</li></ul><h3 id="Relief-F算法（多分类）"><a href="#Relief-F算法（多分类）" class="headerlink" title="Relief-F算法（多分类）"></a>Relief-F算法（多分类）</h3><p>给定数据集：</p><ul><li><p>数据集：<script type="math/tex">\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m\}</script></p></li><li><p>其类别集：<script type="math/tex">y\in \{1,2,\dots,|\mathcal{Y}|\}</script></p></li></ul><p>对于每个实例$x_i$：</p><p><strong>Relief-F</strong>，在<script type="math/tex">x_i</script>的<strong>同类样本</strong>（即<script type="math/tex">y_i</script>相同）中寻找<strong>最近邻</strong><script type="math/tex">x_{i,nh}</script>，称为<strong>猜中近邻</strong>；</p><p>然后，在<script type="math/tex">x_i</script>的每个不同类（共<script type="math/tex">|\mathcal{Y}|-1</script>个）寻找一个<strong>最近邻</strong><script type="math/tex">x_{i,l,nm},l\in\{1,2,\dots,|\mathcal{Y}|;l\ne k\}</script>，称为<strong>猜错近邻</strong>；</p><p>于是，改进的<strong>相关统计量</strong>为：</p><script type="math/tex; mode=display">\delta^j=\sum_i(\sum_{l\ne k}(p_l\times diff(x_i^j,x_{i,l,nm}^j)^2)-diff(x_i^j.x_{i,nh}^j)^2)</script><p>其中：</p><p>$p_l$：第$l$类样本在数据集$D$中所占的比例。</p><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p249</p>]]></content>
    
    
    <summary type="html">Relief是一种著名的过滤式特征选择的方法。</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>Hexo ERROR:hexo-renderer-swig</title>
    <link href="https://1.15.86.100/2021/03/28/Hexo-ERROR-hexo-renderer-swig/"/>
    <id>https://1.15.86.100/2021/03/28/Hexo-ERROR-hexo-renderer-swig/</id>
    <published>2021-03-28T14:14:10.000Z</published>
    <updated>2021-03-28T14:19:14.200Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hexo-ERROR-hexo-renderer-swig"><a href="#Hexo-ERROR-hexo-renderer-swig" class="headerlink" title="Hexo ERROR: hexo-renderer-swig"></a>Hexo ERROR: hexo-renderer-swig</h1><h3 id="报错内容"><a href="#报错内容" class="headerlink" title="报错内容"></a>报错内容</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends &#x27;_layout.swig&#x27; %&#125; &#123;% import &#x27;_macro/post.swig&#x27; as post_template %&#125; &#123;% import &#x27;_macro/sidebar.swig&#x27; as sidebar_template %&#125; &#123;% block title %&#125;&#123;&#123; config.title &#125;&#125;&#123;% if theme.index_with_subtitle and config.subtitle %&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;% endif %&#125;&#123;% endblock %&#125; &#123;% block page_class %&#125; &#123;% if is_home() %&#125;page-home&#123;% endif -%&#125; &#123;% endblock %&#125; &#123;% block content %&#125;</span><br><span class="line">&#123;% for post in page.posts %&#125; &#123;&#123; post_template.render(post, true) &#125;&#125; &#123;% endfor %&#125;</span><br><span class="line">&#123;% include &#x27;_partials/pagination.swig&#x27; %&#125; &#123;% endblock %&#125; &#123;% block sidebar %&#125; &#123;&#123; sidebar_template.render(false) &#125;&#125; &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p><img src="/2021/03/28/Hexo-ERROR-hexo-renderer-swig/image-20210328221727193.png" alt="image-20210328221727193"></p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><strong>Hexo</strong>在5.0+中删除了<em>hexo-renderer-swig</em>，因此需要自己安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-swig</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">解决Hexo {% extends &#39;_layout.swig&#39; %}....{% endblock %} 报错</summary>
    
    
    
    
    <category term="Hexo" scheme="https://1.15.86.100/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>XLNet-PPT</title>
    <link href="https://1.15.86.100/2021/03/28/XLNet-PPT/"/>
    <id>https://1.15.86.100/2021/03/28/XLNet-PPT/</id>
    <published>2021-03-28T12:54:18.000Z</published>
    <updated>2021-03-28T13:05:35.819Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/03/28/XLNet-PPT/PPT1.PNG" alt="1"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%872.PNG" alt="2"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%873.PNG" alt="3"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%874.PNG" alt="4"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%875.PNG" alt="5"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%876.PNG" alt="6"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%877.PNG" alt="7"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%878.PNG" alt="8"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%879.PNG" alt="9"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8710.PNG" alt="10"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8711.PNG" alt="11"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8712.PNG" alt="12"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8713.PNG" alt="13"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8714.PNG" alt="14"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8715.PNG" alt="15"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8716.PNG" alt="16"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8717.PNG" alt="17"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8718.PNG" alt="18"></p>]]></content>
    
    
    <summary type="html">介绍XLNet的PPT</summary>
    
    
    
    
    <category term="NLP" scheme="https://1.15.86.100/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>XLNet</title>
    <link href="https://1.15.86.100/2021/03/28/XLNet/"/>
    <id>https://1.15.86.100/2021/03/28/XLNet/</id>
    <published>2021-03-28T12:44:55.000Z</published>
    <updated>2021-03-28T14:55:56.901Z</updated>
    
    <content type="html"><![CDATA[<h1 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h1><p><a href="https://arxiv.org/pdf/1906.08237.pdf">XLNET</a></p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="AR-AutoRegressive"><a href="#AR-AutoRegressive" class="headerlink" title="AR(AutoRegressive)"></a>AR(AutoRegressive)</h3><p>用模型估计一个文本库的<strong>概率分布</strong></p><p>给定<strong>文本序列</strong>：</p><p>$ \bold{x}=[x_1,x_2,\dots,x_T] $</p><p><strong>AR</strong>模型会将<strong>似然</strong>分解成<strong>前向的乘积(forward product)</strong>：</p><p>$p(\bold{x})=\prod<em>{t=1}^Tp(x_t|\bold{x}</em>{&lt;t})$</p><p>或者<strong>后向乘积(backward product)</strong>：</p><p>$p(\bold{x})=\prod<em>{t=T}^1p(x_t|\bold{x}</em>{&gt;t})$</p><p>而其中每个的<strong>概率分布</strong>可以用带参数的模型(神经网络等)来建模。</p><p><strong>AR</strong>通过最大化<strong>对数似然估计</strong>进行预训练：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \mathrm{log}\ p_{\theta}(\bold{x})=\sum_{t=1}^T\mathrm{log}\ p_{\theta}(x_t|\bold{x}_{<t})=\sum_{t=1}^T\mathrm{log}\frac{\mathrm{exp}(h_{\theta}(\bold{x}_{1:t-1})^Te(x_t))}{\sum_{x'}\mathrm{exp}(h_{\theta}(\bold{x}_{1:t-1})^Te(x'))}</script><p><strong>其中</strong></p><p>$h<em>{\theta}(\bold{x}</em>{1:t-1})$：是用<em>神经网络模型</em>计算的文本的表征，可以通过<strong>RNN</strong>，<strong>Transformer</strong>等计算；</p><p>$e(x)$：是$x$的<strong>embedding</strong>；</p><p><strong>缺陷</strong>：<strong>AR</strong>模型只能<strong>单向(uni-directional)建模</strong></p><h3 id="AE-AutoEncoding"><a href="#AE-AutoEncoding" class="headerlink" title="AE(AutoEncoding)"></a>AE(AutoEncoding)</h3><p><strong>AE（自编码器）</strong>模型不会建立<strong>清晰的概率密度估计(explicit density estimation)</strong>，而是对原始数据进行<strong>重构(reconstruct)</strong>，例如：<strong>BERT</strong></p><p><strong>BERT</strong>：基于<strong>降噪自编码器（denoising auto-encoding）</strong>，给定输入，每次<strong>15%</strong>的<strong>字词(token)</strong>会被替换成特殊的标记<strong>[MASK]</strong>，然后模型会被训练去从输入的<strong>不全(corrupted)的数据</strong>中恢复原始的字词。因为<strong>BERT</strong>不用做<strong>密度估计(density estimation)</strong>，所以可以双向建模。</p><p>最优化训练公式如下：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \mathrm{log}\ p_{\theta}(\bar{\bold{x}}|\hat{\bold{x}})\approx \sum_{t=1}^Tm_t\mathrm{log}\ p_{\theta}(x_t|\hat{\bold{x}})=\sum_{t=1}^Tm_t\mathrm{log}\frac{\mathrm{exp}(H_\theta(\hat{\bold{x}})_t^Te(x_t))}{\sum_{x'}\mathrm{exp}(H_\theta(\hat{\bold{x}})_t^Te(x'))}</script><p>其中：</p><p>$\hat{\bold{x}}$：<strong>mask</strong>后的文本；</p><p>$\bar{\bold{x}}$：被<strong>mask</strong>的文本；</p><p>$m_t$：<strong>指示变量</strong>，当$x_t$是被<strong>mask</strong>的时，$m_t=1$，其他情况下，$m_t=0$；</p><p>$H_\theta$：Transformer模型；</p><p><strong>缺陷</strong>：</p><ul><li><strong>BERT</strong>预训练使用的标记<strong>[MASK]</strong>不会出现在真正数据中，会导致<strong>预训练-微调差异(pretrain-finetune discrepancy)</strong></li><li>输入中不包括待预测字词（被mask掉），<strong>BERT</strong>模型不能像<strong>AR</strong>中一样用乘法规则建模<strong>联合概率(joint probability)</strong></li><li><strong>BERT</strong>假设待预测的字词即$\bar{x}$<strong>互相独立</strong></li></ul><p><strong>优点</strong>：</p><ul><li><strong>BERT</strong>每个位置的字词可以注意到双向的上下文所有的文本；</li></ul><h2 id="XLNet-1"><a href="#XLNet-1" class="headerlink" title="XLNet"></a>XLNet</h2><ul><li><strong>XLNet</strong>最大化基于所有可能的<strong>分解顺序的排列(permutation of the factorization order)</strong>的<strong>极大对数似然估计</strong>，由于这个原因每个位置的字词可以学习左右的文本；</li><li><strong>XLNet</strong>预训练不会通过mask部分字词的方法，因此避免<strong>预训练-微调差异</strong>；</li><li><strong>XLNet</strong>集成了<strong>段RNN机制(segment recurrence)</strong>和<strong>Transformer-XL</strong>；</li></ul><h3 id="Permutation-Language-Modeling"><a href="#Permutation-Language-Modeling" class="headerlink" title="Permutation Language Modeling"></a>Permutation Language Modeling</h3><p>一般来说，对于一个长度为$T$的序列$\bold{x}$，<strong>AE</strong>的<strong>因子分解</strong>存在$T!$个不同的排列顺序。直觉上，如果模型参数能够从所有的排列中学得，那么这个模型就可以从双向的所有位置获得信息。</p><p>根据这个想法，得出以下优化公式：</p><script type="math/tex; mode=display">\mathrm{max}_\theta\ \ \ \mathbb{E}_{z\sim\mathcal{Z}_T}[\sum_{t=1}^T\mathrm{log}\ p_\theta(x_{z_t}|\bold{x}_{\bold{z}_{<t}})]</script><p>其中：</p><p>$\mathcal{Z}_T$：长度为$T$的序列的所有排列方式；</p><p><img src="/2021/03/28/XLNet/image-20210322211204908.png" alt="image-20210322211204908"></p><p>实践中，对于一个文本序列$\bold{x}$，我们<strong>采样</strong>出一个排列顺序，然后按照这个顺序将$p_\theta(\bold{x})$的似然估计进行<strong>因子分解</strong>，因为$\theta$参数是共享的，所以理论上$x_i$会”看见“每个$x_j,x_i\neq x_j$。</p><p><strong>排列采样</strong>在<strong>Transformer</strong>的实践中通过<strong>Attention</strong>的<strong>mask</strong>矩阵实现。</p><p>这样就避免了<strong>独立性假设</strong>和<strong>预训练-微调差异</strong>。</p><h4 id="Remark-on-Permutation"><a href="#Remark-on-Permutation" class="headerlink" title="Remark on Permutation"></a>Remark on Permutation</h4><p>上面的<strong>排列采样</strong>仅针对<strong>因子分解</strong>计算，不是序列本身的顺序。同时为了保留序列原本顺序的信息，这里使用基于原本顺序的<strong>位置编码</strong>。</p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><h4 id="Target-Aware-Representations"><a href="#Target-Aware-Representations" class="headerlink" title="Target-Aware Representations"></a>Target-Aware Representations</h4><p><strong>模型公式</strong>：</p><script type="math/tex; mode=display">p_\theta(\bold{X}_{z_t}=x|\bold{x}_{\bold{z}_{<t}})=\frac{\mathrm{exp}(e(x)^Th_\theta(\bold{x}_{\bold{z}_{<t}}))}{\sum_{x'}\mathrm{exp}(e(x')^Tg_\theta(\bold{x}_{\bold{z}_{<t}}))}</script><p>其中$\bold{x}<em>{\bold{z}</em>{&lt;t}}$不依赖其即将预测的<strong>位置信息</strong>，$\bold{z}$是按照<strong>因子分解顺序</strong>排列的。因此这里需要加入位置信息；所以将公式修改为：</p><script type="math/tex; mode=display">p_\theta(\bold{X}_{z_t}=x|\bold{x}_{\bold{z}_{<t}})=\frac{\mathrm{exp}(e(x)^Tg_\theta(\bold{x}_{\bold{z}_{<t}},z_t))}{\sum_{x'}\mathrm{exp}(e(x')^Tg_\theta(\bold{x}_{\bold{z}_{<t}},z_t))}</script><p>其中：</p><p>$g<em>\theta(\bold{x}</em>{\bold{z}_{&lt;t}}, z_t)$：表示将预测目标的位置编码$z_t$加入的新的表征。</p><p>这可以看成<strong>站在</strong>目标位置$z<em>t$，然后依靠位置信息去收集文本信息$\bold{x}</em>{\bold{z}_{&lt;t}}$。</p><h4 id="Two-Stream-Self-Attention"><a href="#Two-Stream-Self-Attention" class="headerlink" title="Two-Stream Self-Attention"></a>Two-Stream Self-Attention</h4><p>使用<strong>Target-Aware Representation</strong>会出现两个问题：</p><ul><li>为了预测$z<em>t$位置的字词$x</em>{z<em>{t}}$，$g</em>\theta(\bold{x}<em>{\bold{z}</em>{&lt;t}},z<em>t)$不能使用$x</em>{z_{t}}$的信息；</li><li>为了预测其他位置的字词$x<em>{z_t},j&gt;t$，$g</em>\theta(\bold{x}<em>{\bold{z}</em>{&lt;t}},z<em>t)$需要加入$x</em>{z_{t}}$的信息。</li></ul><p>为了解决上面问题，这里提出了<strong>Two-Stream Self-Attention</strong>方法</p><p><img src="/2021/03/28/XLNet/image-20210325230153410.png" alt="image-20210325230153410"></p><ul><li><strong>content representation</strong>$h<em>\theta(\bold{x}</em>{\bold{z}<em>{\le t}})$，这个和<strong>Transformer</strong>中的类似，它保留了上游文本信息和$x</em>{z_t}$的信息；</li><li><strong>query representation</strong>$g<em>\theta(\bold{x}</em>{\bold{z}<em>t},z_t)$，这个只保留了$\bold{z}</em>{&lt;t}$的信息。</li></ul><p>在计算上：</p><p>第一层的<strong>query stream</strong>被初始化为可训练的向量，$g_i^{(0)}=w$；</p><p>第一层的<strong>content stream</strong>被初始化为对应的<strong>词向量</strong>，$h_i^{(0)}=e(x_i)$；</p><p>对于每个<strong>self-attention layer</strong>，其更新方式为：</p><script type="math/tex; mode=display">g_{z_t}^{(m)}=\mathrm{Attention}(Q=g_{z_t}^{(m-1)},KV=h_{\bold{z}_{<t}}^{(m-1)};\theta)</script><script type="math/tex; mode=display">h_{z_t}^{(m)}=\mathrm{Attention}(Q=h_{z_t}^{(m-1)},KV=h_{\bold{z}_{\le t}}^{(m-1)};\theta)</script><p><strong>content representation</strong>和标准的<strong>self-attention</strong>一样，所以在<strong>微调</strong>的时候我们会丢掉<strong>query stream</strong>，只使用<strong>content stream</strong>作为普通的<strong>Transformer</strong>看，同时我们将$g_{z_t}^{(M)}$作为最终的预测结果。</p><h4 id="Partial-Prediction"><a href="#Partial-Prediction" class="headerlink" title="Partial Prediction"></a>Partial Prediction</h4><p>利用语句排列来训练模型存在<strong>难收敛(slow convergence)</strong>的问题，为了解决这个问题，将$\bold{z}$分为$\bold{z}<em>{\le c}$和$\bold{z}</em>{&gt;c}$，然后最大化以下概率：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \ \mathbb{E}_{\bold{z}\sim \mathcal{Z}_T}[\mathrm{log}p_{\theta}(\bold{x}_{\bold{z}_{>c}}|\bold{x}_{\bold{z}_{\le c}})]=\mathbb{E}_{\bold{z}\sim \mathcal{Z}_T}[\sum_{t=c+1}^{\bold{z}}\mathrm{log}p_{\theta}(x_{z_t}|\bold{x}_{\bold{z}_{<t}})]</script><p>预测$\bold{z}_{&gt;c}$的原因是因为，能够传递足够的信息。一般会设置一个超参数$K$，$1/K$的字词被选择用做预测。</p><h3 id="Incorporating-Ideas-from-Transformer-XL"><a href="#Incorporating-Ideas-from-Transformer-XL" class="headerlink" title="Incorporating Ideas from Transformer-XL"></a>Incorporating Ideas from Transformer-XL</h3><p>这里使用了<strong>Transformer-XL</strong>的两个技巧：<strong>相对位置编码（relative positional encoding scheme）</strong>和<strong>段循环机制（segment recurrence mechanism）</strong></p><ul><li><p><strong>相对位置编码</strong>：即前文的$g<em>\theta(\bold{x}</em>{\bold{z}_t},z_t)$</p></li><li><p><strong>段循环机制</strong>：</p><p>  <img src="/2021/03/28/XLNet/image-20210328204303326.png" alt="image-20210328204303326"></p><p>  假设有长文本中的两段文本，长文本为：$\bold{s}$，两端文本为：$\tilde{\bold{x}}=\bold{s}<em>{1:T},\bold{x}=\bold{s}</em>{T+1:2T}$，两端文本对应的采样的排列顺序为$\tilde{\bold{z}}=[1\dots T],\bold{z}=[T+1\dots 2T]$</p><p>  于是基于$\tilde{\bold{z}}$，先处理第一段，获得<strong>文本表征</strong>$\tilde{\bold{h}}^{(m)}$($m$代表层数)，然后对于下一段，计算公式为：</p><script type="math/tex; mode=display">h_{z_t}^{(m)}\leftarrow \mathrm{Attention}(Q=h_{z_t}^{(m-1)},KV=[\tilde{\bold{h}}^{(m-1)},\bold{h}_{\bold{z}_{\le t}}^{(m-1)}];\theta)</script><p>  $[\dots]$：表示将矩阵按照<strong>序列长度的维度</strong>拼接起来。</p></li></ul><h3 id="Modeling-Multiple-Segments"><a href="#Modeling-Multiple-Segments" class="headerlink" title="Modeling Multiple Segments"></a>Modeling Multiple Segments</h3><p>预训练时，对于多段文本，<strong>XLNet</strong>随机采样两段文本（可能从同一上下文，也可能从不同上下文）然后将其拼接成一段文本（方法和<strong>BERT</strong>一样：<strong>[A, SEP, B, SEP, CLS]</strong>）</p><h4 id="Relative-Segment-Encoding"><a href="#Relative-Segment-Encoding" class="headerlink" title="Relative Segment Encoding"></a>Relative Segment Encoding</h4><p>给定两个位置$i,j$，如果这两个位置是来自同一段，则用编码$s<em>{i,j}=s</em>{+}$，否则用编码$s<em>{i,j}=s</em>{-}$。这两个编码都是学得的参数。</p><p>当$i$查询$j$时，段位置编码$s<em>{i,j}$会被用于计算<strong>注意权重（attention weight）</strong>$a</em>{i,j}=(\bold{q}<em>i+\bold{b})^T\bold{s}</em>{i,j}$。</p><p>$\bold{q}_i$是查询向量，$\bold{b}$是待学习的依赖于头的偏差向量<strong>（learnable head-specific bias vector）</strong>。最后该权重会被加到最终的<strong>注意权重</strong>上。</p><p>该方法有两个作用：</p><ul><li>提高泛化能力；</li><li>让模型能够适应多段文本任务，因为多段文本不能用绝对位置编码；</li></ul>]]></content>
    
    
    <summary type="html">Google Brain重磅出品，集自回归语言模型和自编码语言模型之大成，又灵活融入Transformer-XL长文本建模能力！</summary>
    
    
    
    
    <category term="NLP" scheme="https://1.15.86.100/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>SQL-install</title>
    <link href="https://1.15.86.100/2020/10/29/SQL-install/"/>
    <id>https://1.15.86.100/2020/10/29/SQL-install/</id>
    <published>2020-10-29T11:49:01.000Z</published>
    <updated>2021-03-27T14:40:20.193Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h1><h3 id="下载："><a href="#下载：" class="headerlink" title="下载："></a>下载：</h3><ul><li><p>在<a href="https://dev.mysql.com/downloads/mysql/">MySQL官网</a>下载<strong>zip</strong>文件；</p></li><li><p>本地直接解压；</p></li></ul><h3 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h3><ul><li>配置<strong>环境变量</strong>：添加解压文件的<strong>bin</strong>文件夹路径至<strong>PATH</strong>；</li><li>初始化，生成DATA文件：<em>mysqld –initialize-insecure –user=mysql</em></li><li>网络连接：<em>mysqld -install</em></li><li>启动服务：<em>net start mysql</em>【在bin目录下】</li></ul><h3 id="登录："><a href="#登录：" class="headerlink" title="登录："></a>登录：</h3><ul><li><p>登录【不用密码】：<em>mysql -u root -p</em></p></li><li><p>查询用户密码：<em>select host,user,authentication_string from mysql.user;</em></p></li><li><p>设置<strong>root</strong>用户密码：<strong>ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘123456’;</strong></p></li><li><p>保存修改：<em>flush privileges;</em></p></li></ul><h3 id="基本操作："><a href="#基本操作：" class="headerlink" title="基本操作："></a>基本操作：</h3><ul><li><strong>退出</strong>：<em>quit</em></li><li></li></ul>]]></content>
    
    
    <summary type="html">SQL</summary>
    
    
    
    
    <category term="SQL" scheme="https://1.15.86.100/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>LDA(Fisher)</title>
    <link href="https://1.15.86.100/2020/07/07/LDA-Fisher/"/>
    <id>https://1.15.86.100/2020/07/07/LDA-Fisher/</id>
    <published>2020-07-07T07:58:22.000Z</published>
    <updated>2021-03-27T14:39:47.317Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LDA-Fisher"><a href="#LDA-Fisher" class="headerlink" title="LDA(Fisher)"></a>LDA(Fisher)</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>LDA(Linear Discriminant Analysis)<strong>，又称</strong>Fisher判别方法</strong>。是一种经典的<strong>线性判别方法</strong>。该方法主要思想是：将样例投影到一维直线上，使得<strong>同类样例</strong>的投影点尽可能<strong>接近</strong>和<strong>密集</strong>；<strong>异类</strong>投影点尽可能<strong>远离</strong>。</p><p><img src="/2020/07/07/LDA-Fisher/.%5CLDA-Fisher%5Cfisher.png" alt="fisher"></p><h3 id="计算推导"><a href="#计算推导" class="headerlink" title="计算推导"></a>计算推导</h3><p>假设已知样本 $C_1$ 和 $C_2$ ，$|C_1|、|C_2|$ 分别两类样本数据的总数。</p><p>则两类样例的<strong>类中心</strong>分别为：</p><p>$$\mu_1=\frac{1}{|C_1|}\sum_{x\in{C_1}}x$$</p><p>$$\mu_2=\frac{1}{|C_2|}\sum_{x\in{C_2}}x$$</p><p>假设最佳的投影方向为 $w$ 则，样本点 $x$ 投影到 $w$ 上的点的坐标为：$y=w^Tx$</p><p>所以，投影后的<strong>类中心</strong>为：</p><p>$$m_k=\frac{1}{|C_k|}\sum_{x\in{C_k}}w^Tx=w^T\frac{1}{|C_k|}\sum_{x\in{C_k}}x=w^T\mu_k$$</p><h4 id="类间距离"><a href="#类间距离" class="headerlink" title="类间距离"></a>类间距离</h4><p><strong>类中心</strong>的<strong>间距</strong>为：</p><p>$$d_{(1, 2)}=(m_1-m_2)^2=(m_1-m_2)(m_1-m_2)^T=w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw=w^TS_bw$$</p><p>其中，$S_b$ 为<strong>类间散度矩阵</strong>：</p><p>$$S_b=(\mu_1-\mu_2)(\mu_1-\mu_2)^T$$</p><h4 id="类内距离"><a href="#类内距离" class="headerlink" title="类内距离"></a>类内距离</h4><p><strong>类内距离</strong>用类内样本的方差来衡量，对于第 $k$ 个类，方差为：</p><p>$$\begin{split}S_k^2=\sum_{x\in{C_k}}(y-m_k)^2=\sum_{x\in{C_k}}(w^T(x-\mu_k))^2\=\sum_{x\in{C_k}}(w^T(x-\mu_k))((x-\mu_k)^Tw)\=w^T[\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T]w \end{split}$$</p><p>所有类别<strong>类内距离</strong>之和为：</p><p>$$\sum_{k\in n}S_k^2=w^T[\sum_{k\in n}\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T]w$$</p><p><strong>类内散度矩阵</strong>为：</p><p>$$S_w=\sum\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T$$</p><h4 id="最优化"><a href="#最优化" class="headerlink" title="最优化"></a>最优化</h4><p>我们的优化目的是增加<strong>类间距离</strong>，减小<strong>类内距离</strong>，所有可以最大化函数：</p><p>$$J(w)=\frac{(m_1-m_2)^2}{S_1^2+S_2^2}=\frac{w^TS_bw}{w^TS_ww}$$</p>]]></content>
    
    
    <summary type="html">LDA</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PowerShell</title>
    <link href="https://1.15.86.100/2020/03/08/PowerShell/"/>
    <id>https://1.15.86.100/2020/03/08/PowerShell/</id>
    <published>2020-03-08T02:40:06.000Z</published>
    <updated>2020-04-05T08:56:23.854Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PowerShell"><a href="#PowerShell" class="headerlink" title="PowerShell"></a>PowerShell</h1><h2 id="basic-command-lines"><a href="#basic-command-lines" class="headerlink" title="basic command lines"></a>basic command lines</h2><ol><li><p>get information about the power-shell (version…)</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$psversiontable</span></span><br></pre></td></tr></table></figure></li><li><p>enter ps through CMD</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;powershell</span><br></pre></td></tr></table></figure></li><li><p>basic math operation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1024</span>*<span class="number">1024</span></span><br></pre></td></tr></table></figure></li><li><p>get service information</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-service</span></span><br></pre></td></tr></table></figure></li><li><p>print environment variables</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$env:path</span></span><br></pre></td></tr></table></figure></li><li><p>get all of the commands</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-command</span></span><br><span class="line"><span class="built_in">gcm</span></span><br></pre></td></tr></table></figure></li><li><p>help</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-help</span> [<span class="type">command</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>history commands</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-history</span></span><br></pre></td></tr></table></figure></li><li><p>find the real name of a short name</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-alias</span> <span class="literal">-name</span> [<span class="type">short</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>set alias</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set-alias</span> <span class="literal">-name</span> [<span class="built_in">new-name</span>] <span class="literal">-value</span> [<span class="type">old</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>input</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$input</span>=<span class="built_in">read-host</span> <span class="string">&quot;please input&quot;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Shortcut-Key"><a href="#Shortcut-Key" class="headerlink" title="Shortcut Key"></a>Shortcut Key</h2><ol><li><p>cancel the progress running</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ctrl+C</span><br></pre></td></tr></table></figure></li></ol><h2 id="Special-methods"><a href="#Special-methods" class="headerlink" title="Special methods"></a>Special methods</h2><h4 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a | b</span><br></pre></td></tr></table></figure><h4 id="redirection"><a href="#redirection" class="headerlink" title="redirection"></a>redirection</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="comment">#append</span></span><br><span class="line">&gt;&gt;<span class="comment">#overwrite</span></span><br></pre></td></tr></table></figure><h4 id="format"><a href="#format" class="headerlink" title="format"></a>format</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;...&#123;0&#125;...&#123;1&#125;...&quot;</span> <span class="operator">-f</span> <span class="variable">$first</span>, <span class="variable">$second</span>, ...</span><br></pre></td></tr></table></figure><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><ol><li><p>definition</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span>[<span class="type">variable</span>-<span class="type">name</span>]=[<span class="type">variable</span>-<span class="type">value</span>]<span class="comment">#$a equals $A</span></span><br></pre></td></tr></table></figure></li><li><p>check the variables</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> variables:</span><br></pre></td></tr></table></figure></li><li><p>array</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$arr</span>=<span class="selector-tag">@</span>()<span class="comment">#empty array</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>..<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>, <span class="string">&#x27;hello&#x27;</span>, <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>.count<span class="comment">#return array&#x27;s numbers</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>+=[<span class="built_in">new-element</span>]</span><br></pre></td></tr></table></figure></li><li><p>string</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$str</span>.split(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.endswith(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.contains(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.compareto(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.indexof(<span class="string">&quot;[char]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.insert(position<span class="literal">-num</span>, <span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.replace(<span class="string">&quot;old-char&quot;</span>, <span class="string">&quot;new-char&quot;</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="Operation"><a href="#Operation" class="headerlink" title="Operation"></a>Operation</h2><ol><li><p>comparation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-eq</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#equal?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-ne</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#not equal?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-gt</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#greater?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-lt</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#less?</span></span><br></pre></td></tr></table></figure></li><li><p>bool operation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">-not</span> [<span class="type">variable</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-and</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-or</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-xor</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br></pre></td></tr></table></figure></li></ol><h2 id="Basic-Grammar"><a href="#Basic-Grammar" class="headerlink" title="Basic Grammar"></a>Basic Grammar</h2><ol><li><p>if</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(conditon)</span><br><span class="line">&#123;expr1&#125;</span><br><span class="line"><span class="keyword">elseif</span></span><br><span class="line">&#123;expr2&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;expr3&#125;</span><br></pre></td></tr></table></figure></li><li><p>switch</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(key<span class="literal">-variable</span>)</span><br><span class="line">&#123;</span><br><span class="line">&#123;<span class="variable">$_</span> condition1&#125;&#123;expr1&#125;</span><br><span class="line">&#123;<span class="variable">$_</span> condition2&#125;&#123;expr2&#125;</span><br><span class="line">default&#123;expr3&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>foreach</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">foreach</span>(<span class="variable">$each</span> <span class="keyword">in</span> [<span class="built_in">array</span>-<span class="type">variable</span>])</span><br><span class="line">&#123;</span><br><span class="line">expr</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>while</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(condition)</span><br><span class="line">&#123;</span><br><span class="line">expr</span><br><span class="line">[<span class="type">break</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>for</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(expr1; condition; expr2)</span><br><span class="line">&#123;</span><br><span class="line">expr3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h2><ol><li><p>definition</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">fun-name</span><span class="params">([parameter])</span></span></span><br><span class="line">&#123;</span><br><span class="line">expression</span><br><span class="line"><span class="keyword">return</span> [<span class="type">return</span>-<span class="type">value</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>using</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">variable1</span>] [<span class="title">variable2</span>] ...</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">The basic methods of powershell</summary>
    
    
    
    
    <category term="powershell" scheme="https://1.15.86.100/tags/powershell/"/>
    
  </entry>
  
  <entry>
    <title>HMM-algorithm</title>
    <link href="https://1.15.86.100/2020/01/06/HMM-algorithm/"/>
    <id>https://1.15.86.100/2020/01/06/HMM-algorithm/</id>
    <published>2020-01-06T03:04:29.000Z</published>
    <updated>2020-01-06T03:11:33.528Z</updated>
    
    <content type="html"><![CDATA[<h1 id="隐马尔可夫模型-HMM"><a href="#隐马尔可夫模型-HMM" class="headerlink" title="隐马尔可夫模型(HMM)"></a>隐马尔可夫模型(HMM)</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>隐马尔可夫模型(Hidden-Markov-Model)<strong>是一种</strong>概率图</strong>模型，在<strong>深度学习</strong>出现之前，该模型被广泛应用于<em>语音识别，文本标注</em>等方面。</p><p>隐马尔可夫模型是关于时序的概率图模型。</p>]]></content>
    
    
    <summary type="html">An introduction to the Hidden Markov Algorithm</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux</title>
    <link href="https://1.15.86.100/2019/12/17/Linux/"/>
    <id>https://1.15.86.100/2019/12/17/Linux/</id>
    <published>2019-12-17T03:40:07.000Z</published>
    <updated>2021-04-18T08:13:04.158Z</updated>
    
    <content type="html"><![CDATA[<h3 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取当前目录下文件</span></span><br><span class="line">ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入子文件夹</span></span><br><span class="line"><span class="built_in">cd</span> [folder name]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 返回上一目录</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回上两级目录</span></span><br><span class="line"><span class="built_in">cd</span> ../..</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回home目录</span></span><br><span class="line"><span class="built_in">cd</span>/<span class="built_in">cd</span> ~ </span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回指定目录</span></span><br><span class="line"><span class="built_in">cd</span> - [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文件内容</span></span><br><span class="line">cat [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件头</span></span><br><span class="line">head [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件尾</span></span><br><span class="line">tail [file name]</span><br><span class="line"></span><br><span class="line">tail -f [file name] <span class="comment">#实时显示文件尾，跟随日志变化</span></span><br><span class="line"><span class="comment"># 修改/编写文件内容</span></span><br><span class="line">vim [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改文件名</span></span><br><span class="line">mv [old file name] [new file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建文件</span></span><br><span class="line">touch [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建文件夹</span></span><br><span class="line">mkdir [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件</span></span><br><span class="line">rm [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除目录下所有文件</span></span><br><span class="line">rm *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归删除文件夹</span></span><br><span class="line">rm -r [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件</span></span><br><span class="line">cp [old file name] [new file name]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示文件夹内文件大小</span></span><br><span class="line">ls -lh</span><br></pre></td></tr></tbody></table></figure><h3 id="Vim-操作"><a href="#Vim-操作" class="headerlink" title="Vim 操作"></a>Vim 操作</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入插入模式</span></span><br><span class="line">按i键</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出插入模式</span></span><br><span class="line">按Eac键</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出Vim</span></span><br><span class="line">:wq + Enter</span><br></pre></td></tr></tbody></table></figure><h3 id="进程操作"><a href="#进程操作" class="headerlink" title="进程操作"></a>进程操作</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让进程在后台运行</span></span><br><span class="line">nuhup [<span class="built_in">command</span>] $</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有进程记录</span></span><br><span class="line">ps -aux</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死进程</span></span><br><span class="line"><span class="built_in">kill</span> -9 [进程id]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示挂起进程</span></span><br><span class="line"><span class="built_in">jobs</span> -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死当前bash内运行的进程</span></span><br><span class="line">Ctrl+c</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂起当前bash内运行的进程</span></span><br><span class="line">Ctrl+z</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行刚刚挂起的进程</span></span><br><span class="line">Ctrl+y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据pid查看进程</span></span><br><span class="line">ps -ef|grep [pid]</span><br></pre></td></tr></tbody></table></figure><h3 id="查看历史命令"><a href="#查看历史命令" class="headerlink" title="查看历史命令"></a>查看历史命令</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">history</span>[num]</span><br></pre></td></tr></tbody></table></figure><h3 id="查看显卡利用情况"><a href="#查看显卡利用情况" class="headerlink" title="查看显卡利用情况"></a>查看显卡利用情况</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></tbody></table></figure><h3 id="查看文件大小"><a href="#查看文件大小" class="headerlink" title="查看文件大小"></a>查看文件大小</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">du -ah</span><br></pre></td></tr></tbody></table></figure><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实时查看GPU显存利用率，需要用pip按照gpustat</span></span><br><span class="line">watch --color -n1 gpustat -cpu</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看显存利用率</span></span><br><span class="line">nvidia-smi</span><br></pre></td></tr></tbody></table></figure><h3 id="远程传文件"><a href="#远程传文件" class="headerlink" title="远程传文件"></a>远程传文件</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 本地传文件到远程</span></span><br><span class="line">scp [-r] local_path user@255.255.255.255:server_path</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 远程传到本地</span></span><br><span class="line">scp [-r] user@255.255.255.255:server_path local_path</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <summary type="html">Basic CommandLine for Linux</summary>
    
    
    
    
    <category term="Linux" scheme="https://1.15.86.100/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch</title>
    <link href="https://1.15.86.100/2019/12/15/Pytorch/"/>
    <id>https://1.15.86.100/2019/12/15/Pytorch/</id>
    <published>2019-12-15T04:43:52.000Z</published>
    <updated>2021-04-02T09:13:48.559Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><h3 id="background"><a href="#background" class="headerlink" title="background"></a>background</h3><p>Torch是一个使用Lua语言的神经网络</p><h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h3 id="import"><a href="#import" class="headerlink" title="import"></a>import</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="data-type"><a href="#data-type" class="headerlink" title="data type"></a>data type</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create empty tensor</span></span><br><span class="line">torch.empty(a, b, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create rand tensor</span></span><br><span class="line">torch.rand(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create zero tensor</span></span><br><span class="line">torch.zeros(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create ones tensor</span></span><br><span class="line">torch.ones(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create from list</span></span><br><span class="line">torch.tensor(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create rand tensor according a tensor</span></span><br><span class="line">torch.rand_like(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the size of a tensor</span></span><br><span class="line">x.size() <span class="comment"># return a tuple</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># resize the tensor</span></span><br><span class="line">x.view(-<span class="number">1</span>, b)</span><br></pre></td></tr></tbody></table></figure><h3 id="tansformation"><a href="#tansformation" class="headerlink" title="tansformation"></a>tansformation</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy to tensor</span></span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line"><span class="comment"># the ndarray of numpy and the tensor of torch share the same storage space</span></span><br><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor to numpy</span></span><br><span class="line">np_data = torch_data.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># int to tensor</span></span><br><span class="line">torch_data = torch.IntTensor(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># float to tensor</span></span><br><span class="line">torch_data = torch.FloatTensor(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor to python(only one element in torch_data)</span></span><br><span class="line">data = torch_data.item()</span><br></pre></td></tr></tbody></table></figure><h3 id="basic-math-method"><a href="#basic-math-method" class="headerlink" title="basic math method"></a>basic math method</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matrix multiplication</span></span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)</span><br><span class="line">ans = torch.mm(tensor, tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate mean</span></span><br><span class="line">mean = torch.mean(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add</span></span><br><span class="line">torch.add(a, b)</span><br><span class="line"><span class="built_in">print</span>(a + b)</span><br><span class="line">a.add_(b) <span class="comment"># change a</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="comment"># any func that can change the tensor has a '_' in its name</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="activation-function"><a href="#activation-function" class="headerlink" title="activation function"></a>activation function</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># relu</span></span><br><span class="line">torch.relu(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid</span></span><br><span class="line">torch.sigmoid(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tanh</span></span><br><span class="line">torch.tanh(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax</span></span><br><span class="line">torch.softmax(tensor)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># regression</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self</span>):</span></span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    net = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(net)</span><br><span class="line">    </span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.5</span>)</span><br><span class="line">    loss_func = torch.nn.MSELoss()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        prediction = net(x)</span><br><span class="line">        </span><br><span class="line">        loss = loss_func(prediction, y)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># classification</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># turn on or turn of autograd</span></span><br><span class="line">torch_data.requires_grad_()</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">requires_grad = <span class="literal">True</span></span><br><span class="line">requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># backward</span></span><br><span class="line">out.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get a same tensor without requiring gradient</span></span><br><span class="line">torch_data.detach()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <summary type="html">The basic methods of pytorch</summary>
    
    
    
    
    <category term="Python" scheme="https://1.15.86.100/tags/Python/"/>
    
    <category term="Pytorch" scheme="https://1.15.86.100/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Variational Inference</title>
    <link href="https://1.15.86.100/2019/12/14/Variational-Inference/"/>
    <id>https://1.15.86.100/2019/12/14/Variational-Inference/</id>
    <published>2019-12-14T15:19:54.000Z</published>
    <updated>2019-12-15T04:46:55.567Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Variational-Inference-变分推断"><a href="#Variational-Inference-变分推断" class="headerlink" title="Variational Inference(变分推断)"></a>Variational Inference(变分推断)</h1><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>$X\quad:observed\ data$</p><p>$Z\quad:latent\ variable+parameter$</p><p>$(X+Z)\quad:complete\ data$</p><p>$$log(P(x))=logP(x,z)-logP(z|x)=log\frac{P(x,z)}{q(z)}-log\frac{P(z|x)}{q(z)}$$</p><p>$$Left=\int_zlogP(x)q(z)dz=logP(x)$$</p><p>$$Right=\int_zq(z)log\frac{P(x,z)}{q(z)}dz-\int_zq(z)log\frac{P(z|x)}{q(z)}dz$$</p><p>$$ELBO(evidence\ lower\ bound)=\int_zq(z)log\frac{P(x,z)}{q(z)}$$</p><p>$$KL(q||p)=-\int_zq(z)log\frac{P(z|x)}{q(z)}dz\ge0$$</p><p>$$\quad\mathscr{L}(q)+KL(q||p)$$</p><p>令 $q(z)=\prod_{i=1}^mq_i(z_i)$</p><p>有 $\mathscr{L}(q)=\int_zq(z)logP(x,z)dz-\int_zq(z)logq(z)dz$</p>]]></content>
    
    
    <summary type="html">An Introduction to Variational Inference</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://1.15.86.100/tags/machine-learning/"/>
    
  </entry>
  
</feed>
