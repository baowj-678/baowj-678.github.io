<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BaoWJ&#39;s Blog</title>
  
  
  <link href="https://1.15.86.100/atom.xml" rel="self"/>
  
  <link href="https://1.15.86.100/"/>
  <updated>2021-05-15T13:56:50.696Z</updated>
  <id>https://1.15.86.100/</id>
  
  <author>
    <name>Bao Wenjie</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Naive Bayes</title>
    <link href="https://1.15.86.100/2021/05/15/Naive-Bayes/"/>
    <id>https://1.15.86.100/2021/05/15/Naive-Bayes/</id>
    <published>2021-05-15T09:10:22.000Z</published>
    <updated>2021-05-15T13:56:50.696Z</updated>
    
    <content type="html"><![CDATA[<h1 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h1><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>利用<strong>条件独立性假设</strong>和<strong>贝叶斯公式</strong>，将对<strong>测试集</strong>的<strong>概率预测</strong>变为可计算的，再利用<strong>训练集</strong>求得。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>根据给定数据<script type="math/tex">X^*\in\mathbb{R}^d</script>，预测出其标签值<script type="math/tex">y^*\in\{0,1,\cdots,n\}</script>。</p><p>易得，预测结果为<script type="math/tex">y'</script>的概率为：</p><script type="math/tex; mode=display">P_\theta(y=y'|X=X^*)</script><p>由<strong>贝叶斯定理</strong>：</p><script type="math/tex; mode=display">p(a|b)=\frac{p(b|a)\cdot p(a)}{p(b)}</script><p>得：</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{p(X^*|y')\cdot p(y')}{p(X^*)}</script><p>由<strong>条件独立性假设</strong>，得：</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{p(x_1=x_1^*|y')\cdot p(x_2=x_2^*|y')\cdots p(x_d=x_d^*|y')\cdot p(y')}{p(X^*)}</script><p>其中，<script type="math/tex">p(x_i=x_i^*|y'),p(x_i=x_i^*)</script>可以由<strong>训练集</strong>求得<strong>后验概率</strong>。</p><h3 id="条件独立性假设"><a href="#条件独立性假设" class="headerlink" title="条件独立性假设"></a>条件独立性假设</h3><p>朴素贝叶斯法对条件概率分布做了 <strong>条件独立性假设</strong>，该<strong>条件独立性假设</strong>不等于<strong>独立性假设</strong>。</p><p>它是说 【<strong>用于分类的特征</strong>】在【<strong>类确定</strong>】的条件下都 是【<strong>条件独立</strong>】的，即：在给定<script type="math/tex">y</script>的条件下<script type="math/tex">x_i,x_j</script>是<strong>条件独立</strong>的。</p><h3 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h3><p>假设原始数据服从<strong>高斯分布</strong>，有以下概率公式：</p><script type="math/tex; mode=display">p(x_i=x|y=y')=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}</script><p>其中，<script type="math/tex">\mu,\sigma^2</script>是<script type="math/tex">y=y'</script>的第<script type="math/tex">i</script>个属性的数据分布的<strong>均值</strong>和<strong>方差</strong>。</p><h3 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h3><p>若特征值不是代表着简单的<strong>离散类型</strong>（如男性、女性），而是一种<strong>计数形式</strong>的特殊类型（身高，薪水等级），可以使用<strong>多项式模型</strong>。</p><p>该模型一般用于<strong>文本分类</strong>，<strong>属性</strong>是<strong>单词</strong>，<strong>属性值</strong>是<strong>单词出现次数</strong>。</p><script type="math/tex; mode=display">p(x_i|y=y')=\frac{N_{yi}+\alpha}{N_y+\alpha n}</script><p>其中，<script type="math/tex">N_{yi}=\sum_{x\in T}x_i</script>是训练集<script type="math/tex">D</script>中，标签为<script type="math/tex">y'</script>且第<script type="math/tex">i</script>个属性值求和（即单词<script type="math/tex">i</script>出现次数），<script type="math/tex">N_y=\sum_{i=1}^dN_{yi}</script>（所有单词出现总次数）。</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{m!}{x_1!\cdot x_2!\cdots x_d!}\prod_{i=1}^d(p^{x_i=x}(x_i|y=y'))</script><h3 id="伯努利模型"><a href="#伯努利模型" class="headerlink" title="伯努利模型"></a>伯努利模型</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;朴素贝叶斯（Naive-Bayes）&quot;&gt;&lt;a href=&quot;#朴素贝叶斯（Naive-Bayes）&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯（Naive Bayes）&quot;&gt;&lt;/a&gt;朴素贝叶斯（Naive Bayes）&lt;/h1&gt;&lt;h3 id=&quot;介</summary>
      
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Naive Bayes" scheme="https://1.15.86.100/tags/Naive-Bayes/"/>
    
  </entry>
  
  <entry>
    <title>KNN</title>
    <link href="https://1.15.86.100/2021/05/15/KNN/"/>
    <id>https://1.15.86.100/2021/05/15/KNN/</id>
    <published>2021-05-15T08:19:02.000Z</published>
    <updated>2021-05-15T08:37:25.987Z</updated>
    
    <content type="html"><![CDATA[<h1 id="K近邻（KNN）"><a href="#K近邻（KNN）" class="headerlink" title="K近邻（KNN）"></a>K近邻（KNN）</h1><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>给定<strong>训练集</strong><script type="math/tex">D</script>，当对<strong>测试样本</strong>进行<strong>分类</strong>时，找到<strong>训练集</strong>中与该<strong>样本</strong>最相似的<script type="math/tex">k</script>个<strong>样本（k近邻）</strong>，根据<script type="math/tex">k</script>个<strong>样本</strong>的标签确定测试样本的标签。</p><p><img src="/2021/05/15/KNN/image-20210515162556039.png" alt="image-20210515162556039"></p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ul><li>输入训练集<script type="math/tex">D</script>；</li><li>确定<script type="math/tex">k</script>的大小和距离计算方法<script type="math/tex">d(\cdot)</script>；</li><li>从<strong>训练集</strong>中得到前<script type="math/tex">k</script>个与测试样本最近的样本（<script type="math/tex">k</script>近邻）；</li><li>根据<script type="math/tex">k</script>个最相似的训练样本的类别，通过<strong>投票</strong>的方式来确定测试样本的类别；</li></ul><h3 id="常用距离度量方法"><a href="#常用距离度量方法" class="headerlink" title="常用距离度量方法"></a>常用距离度量方法</h3><h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sqrt{\sum_{i=1}^d(x_{1i}-x_{2i})^2}</script><h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sum_{i=1}^d|x_{1i}-x_{2i}|</script><h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sqrt{(x_1-x_2)^T\Sigma^{-1}(x_1-x_2)}</script><h4 id="余弦距离"><a href="#余弦距离" class="headerlink" title="余弦距离"></a>余弦距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\frac{x_1^Tx_2}{||x_1||_2||x_2||_2}</script><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>对<strong>异常数据</strong>不敏感，具有较好的<strong>抗噪性</strong>；</li><li><strong>KNN</strong>算法的计算<strong>效率低</strong>；</li><li>当<strong>训练集较小</strong>的时候，<strong>KNN</strong>算法易导致<strong>过拟合</strong>；</li></ul><h2 id="Kd树"><a href="#Kd树" class="headerlink" title="Kd树"></a>Kd树</h2>]]></content>
    
    
    <summary type="html">KNN介绍</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="KNN" scheme="https://1.15.86.100/tags/KNN/"/>
    
    <category term="Supervised Learning" scheme="https://1.15.86.100/tags/Supervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Decision Tree</title>
    <link href="https://1.15.86.100/2021/05/15/Decision-Tree/"/>
    <id>https://1.15.86.100/2021/05/15/Decision-Tree/</id>
    <published>2021-05-15T08:17:08.000Z</published>
    <updated>2021-05-15T08:18:28.651Z</updated>
    
    <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h3 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h3><p><strong>决策树</strong>是一种描述对<em>实例</em>分类的树状结构，由结点（node）和有向边（directed edge）组成。在进行分类时，对每个实例，从<strong>根节点</strong>（root）开始，根据<em>该实例</em>是否满足结点的调节，选择不同的分支进入下个结点，最终到达<strong>叶节点</strong>（leaf）得到分类结果。</p><h4 id="信息增益："><a href="#信息增益：" class="headerlink" title="信息增益："></a>信息增益：</h4><p><strong>熵（entropy）</strong>表示随机变量<strong>不确定性</strong>的度量，设$X$是一个取有限个值的离散随机变量，其概率分布</p>]]></content>
    
    
    <summary type="html">决策树介绍</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Decision Tree" scheme="https://1.15.86.100/tags/Decision-Tree/"/>
    
  </entry>
  
  <entry>
    <title>K-means</title>
    <link href="https://1.15.86.100/2021/05/15/K-means/"/>
    <id>https://1.15.86.100/2021/05/15/K-means/</id>
    <published>2021-05-15T08:10:22.000Z</published>
    <updated>2021-05-15T08:10:22.143Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Discretization</title>
    <link href="https://1.15.86.100/2021/05/15/Discretization/"/>
    <id>https://1.15.86.100/2021/05/15/Discretization/</id>
    <published>2021-05-15T03:11:48.000Z</published>
    <updated>2021-05-15T04:40:27.625Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据离散化"><a href="#数据离散化" class="headerlink" title="数据离散化"></a>数据离散化</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>将<strong>连续性特征</strong>转换成为<strong>离散型特征</strong>的过程称为<strong>特征离散化（data discretization）</strong>。</p><h2 id="无监督离散化"><a href="#无监督离散化" class="headerlink" title="无监督离散化"></a>无监督离散化</h2><h3 id="等距离散化"><a href="#等距离散化" class="headerlink" title="等距离散化"></a>等距离散化</h3><p>该方法根据<strong>连续特征</strong>的取值，将其均匀地划分成<script type="math/tex">k</script>个区间，每个区间的<strong>区间宽度<script type="math/tex">w</script></strong>相等：</p><script type="math/tex; mode=display">w=\frac{x_{max}-x_{min}}{k}</script><p><strong>特点</strong>：</p><ul><li>对数据要求高；</li><li>对离群值敏感；</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cut()</span><br></pre></td></tr></tbody></table></figure><h3 id="等频离散化"><a href="#等频离散化" class="headerlink" title="等频离散化"></a>等频离散化</h3><p>根据<strong>连续性特征</strong>取值的总数是<script type="math/tex">N</script>，仍然将其划分为<script type="math/tex">k</script>个区段，每个区段包含的数据个数<script type="math/tex">n</script>为:</p><script type="math/tex; mode=display">n=\frac{N}{k}</script><p><strong>特点</strong>：</p><ul><li>保证了每个区间段有相同的样本数；</li><li>取值相近的样本会被划分到不同区间；</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="聚类离散化"><a href="#聚类离散化" class="headerlink" title="聚类离散化"></a>聚类离散化</h3><p>离散化<strong>连续性特征</strong>时，如果<strong>相似的样本</strong>能落到<strong>相同的区间段</strong>内，则这样的划分可以更好地代表原始数据的信息，因此可以考虑利用<strong>聚类</strong>对连续性特征进行离散化处理。</p><h2 id="有监督离散化"><a href="#有监督离散化" class="headerlink" title="有监督离散化"></a>有监督离散化</h2><h3 id="信息增益离散化"><a href="#信息增益离散化" class="headerlink" title="信息增益离散化"></a>信息增益离散化</h3><p>该方法源自于<strong>决策树</strong>模型，在建立决策树时，遍历每一个特征，选择熵最小也就是<strong>信息增益最大</strong>的特征作为正式分裂节点。</p><h4 id="算法步骤："><a href="#算法步骤：" class="headerlink" title="算法步骤："></a>算法步骤：</h4><ul><li>对连续型特征进行排序；</li><li>把特征的每一个取值作为候选<strong>切分点</strong>，计算出相应的<strong>熵</strong>，选择熵最小的取值作为正式的切分点，将原来的区间一分为二；</li><li>递归处理第二步中得到的两个新区间段，直到每个区间段内特征的类别一样为止；</li><li>合并相邻的，<strong>类的熵值为0且特征类别相同</strong>的区段，重新计算新区间段类的熵值；</li><li>重复第四步到满足终止条件（决策树的深度或叶子数）；</li></ul><h3 id="卡方离散化"><a href="#卡方离散化" class="headerlink" title="卡方离散化"></a>卡方离散化</h3><p>基于<strong>卡方</strong>的离散化方法是采用<strong>自底向上</strong>的合并策略，首先将特征的取值看作单独的区间，然后逐一递归进行区间合并。</p><h4 id="ChiMerge方法步骤："><a href="#ChiMerge方法步骤：" class="headerlink" title="ChiMerge方法步骤："></a>ChiMerge方法步骤：</h4><ul><li>将连续型特征的每一个取值看作是一个单独的区间段，并进行排序；</li><li>针对每对相邻的区间段，计算卡方统计量。卡方值最小或者低于设定阈值的相邻区间段合并：</li><li>对于新的区间段，递归进行第1,2步，只到满足终止条件；</li></ul><h3 id="类别属性依赖最大化（CAIM）离散化"><a href="#类别属性依赖最大化（CAIM）离散化" class="headerlink" title="类别属性依赖最大化（CAIM）离散化"></a>类别属性依赖最大化（CAIM）离散化</h3>]]></content>
    
    
    <summary type="html">数据离散化是特征工程或者数据预处理中很关键的一个环节，这里会对数据离散化进行简单介绍。</summary>
    
    
    
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
    <category term="Discretization" scheme="https://1.15.86.100/tags/Discretization/"/>
    
  </entry>
  
  <entry>
    <title>Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
    <link href="https://1.15.86.100/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/"/>
    <id>https://1.15.86.100/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/</id>
    <published>2021-05-09T08:32:26.000Z</published>
    <updated>2021-05-09T12:47:16.349Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models1"><a href="#Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models1" class="headerlink" title="Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models1"></a>Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models<a href="#refer-anchor-1"><sup>1</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>目前最好的方式是将<strong>对话系统</strong>看成一个<strong>部分可观测的马尔可夫决策过程（Partially Observable Markov Decision Process，POMDP）</strong>。</p><p>但是大部分部署的<strong>对话系统</strong>都是基于<strong>人工指定的关于状态和行为的特征</strong>。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>将一个<strong>对话（dialogue）</strong>看成由两个<strong>对话者（interlocutor）</strong>发出的<script type="math/tex">M</script>个<strong>话语（utterance）</strong><script type="math/tex">D={U_1,\cdots,U_M}</script>构成的<strong>序列（sequence）</strong>。其中每个<strong>话语</strong><script type="math/tex">U_m=\{w_{m,1},\cdots,w_{m,n}\}</script>，其中<script type="math/tex">w_{m,n}</script>表示第<script type="math/tex">m</script>句话的第<script type="math/tex">n</script>个位置的<strong>token（包括单词和说话动作speech act）</strong>。</p><p>因此，一段对话<script type="math/tex">D</script>可以被分解为：</p><script type="math/tex; mode=display">P_\theta(U_1,\cdots,U_M)=\prod_{m=1}^MP_\theta(U_m|U_{<m})</script><script type="math/tex; mode=display">=\prod_{m=1}^M\prod_{n=1}^NP_\theta(w_{m,n}|w_{m,<n},U_{<m})</script><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><script type="math/tex; mode=display">h_n=f(h_{n-1}, w_n)=\tanh(Hh_{n-1}+I_{w_n})</script><p>其中，<script type="math/tex">h_n\in\mathbb{R}^{d_h}</script>是<strong>隐状态</strong>，<script type="math/tex">I</script>是词向量，是对从1到n的语句片段的表征。</p><p>对下一个单词的预测：</p><script type="math/tex; mode=display">P_\theta(w_{n+1}=v|w\le n)=\frac{\exp(g(h_n,v))}{\sum_{v'}\exp(g(h_n,v'))}</script><script type="math/tex; mode=display">g(h_n,v)=O^T_{w_n}h_n</script><p>其中，<script type="math/tex">O</script>是词向量。</p><h3 id="Hierarchical-Recurrent-Encoder-Decoder"><a href="#Hierarchical-Recurrent-Encoder-Decoder" class="headerlink" title="Hierarchical Recurrent Encoder-Decoder"></a>Hierarchical Recurrent Encoder-Decoder</h3><p>每个对话看成由<strong>语句</strong>组成，而每个<strong>语句</strong>又由<strong>字词</strong>组成。所以可以利用<strong>RNN</strong>在两个层次建模。<img src="/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/image-20210509200101676.png" alt="image-20210509200101676"></p><ul><li><strong>encoder RNN</strong>：将每个<strong>语句</strong>编码成一个<strong>语句向量（utterance vector）</strong>【最后的hidden state】；</li><li><strong>context RNN</strong>：记录经过的每个语句；</li><li><strong>decoder RNN</strong>：进行下一个语句的预测。</li></ul><h3 id="Bi-HRED"><a href="#Bi-HRED" class="headerlink" title="Bi-HRED"></a>Bi-HRED</h3><p>将上面的<strong>encoder RNN</strong>变成双向的。最终的<strong>语句向量</strong>，有两种方法取的：</p><ul><li>直接将双向RNN最后的<strong>hidden state</strong>拼接起来；</li><li>采用<script type="math/tex">L_2</script><strong>池化</strong>：<script type="math/tex">\sqrt{\frac{\sum_{n=1}^{N_m}h_n^2}{N_m}}</script>，<script type="math/tex">h_n</script>是位置n的<strong>hidden state</strong>，然后再拼接；</li></ul><h3 id="Bootstrapping-from-Word-Embeddings-and-Subtitles-Q-A"><a href="#Bootstrapping-from-Word-Embeddings-and-Subtitles-Q-A" class="headerlink" title="Bootstrapping from Word Embeddings and Subtitles Q-A"></a>Bootstrapping from Word Embeddings and Subtitles Q-A</h3><ul><li>使用在包括1000亿个单词的<strong>Google News</strong>数据集上训练的<strong>Word2Vec</strong>词向量，引入常识知识；</li><li>在<strong>非对话语料库</strong>上进行<strong>预训练</strong>；</li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><p>定义<strong>单词困惑度（word perplexity）</strong>：</p><script type="math/tex; mode=display">\exp{(-\frac{1}{N_w}\sum_{n=1}^N\log P_\theta(U_1^n,U_2^n,U_3^n))}</script><p>其中，<script type="math/tex">\theta</script>是参数，<script type="math/tex">\{U_1^n,U_2^n,U_3^n\}_{n=1}^N</script>是三元组的数据集，<script type="math/tex">N_w</script>是整个数据集的单词个数。</p><p><strong>困惑度越低模型越好</strong>。</p><h3 id="MAP-Output"><a href="#MAP-Output" class="headerlink" title="MAP Output"></a>MAP Output</h3><p>利用<strong>集束搜索（beam search）</strong>得到预测结果：</p><p><img src="/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/image-20210509203744611.png" alt="image-20210509203744611"></p><p>发现：结果比较合理，但大部分结果都是<strong>客套话（generic）</strong>，比如<em>I don’t know,I’m sorry</em>。</p><p>原因：</p><ul><li><strong>数据不够（data scarcity）</strong>：该模型比较大，可能需要更多数据；</li><li><strong>大部分token是标点符号代词（pronouns）</strong>：训练时，每个token都是地位相同的，导致模型训练不够好；</li><li><strong>三元组太短了</strong>：模型需要更多的信息。</li></ul><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1507.04808v2">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</a><p></p>]]></content>
    
    
    <summary type="html">本文是对Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models论文的阅读总结。</summary>
    
    
    
    
    <category term="Dialogue" scheme="https://1.15.86.100/tags/Dialogue/"/>
    
    <category term="RNN" scheme="https://1.15.86.100/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>Sklearn Naive Bayes</title>
    <link href="https://1.15.86.100/2021/04/28/Sklearn-Naive-Bayes/"/>
    <id>https://1.15.86.100/2021/04/28/Sklearn-Naive-Bayes/</id>
    <published>2021-04-28T08:15:21.000Z</published>
    <updated>2021-04-28T08:16:57.295Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h1>]]></content>
    
    
    <summary type="html">本文是对朴素贝叶斯(Naive Bayes)的简单介绍[结合sklearn文档]。</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Sklearn" scheme="https://1.15.86.100/tags/Sklearn/"/>
    
    <category term="Naive Bayes" scheme="https://1.15.86.100/tags/Naive-Bayes/"/>
    
  </entry>
  
  <entry>
    <title>Consistency by Agreement in Zero-shot Neural Machine Translation</title>
    <link href="https://1.15.86.100/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/"/>
    <id>https://1.15.86.100/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/</id>
    <published>2021-04-24T07:30:30.000Z</published>
    <updated>2021-04-26T15:52:01.644Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation"><a href="#Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation" class="headerlink" title="Consistency by Agreement in Zero-shot Neural Machine Translation[*]"></a>Consistency by Agreement in Zero-shot Neural Machine Translation<a href="#refer-*"><sup>[*]</sup></a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>一般对于多语言间（例如<strong>k</strong>中语言）间的互相翻译，需要训练<strong>k*k</strong>个<strong>NMT</strong>。</p><p>后来有人提出<strong>zero-shot translation</strong>，即给定三种语言<strong>German（De）、English（En）、French（Fr）</strong>，然后在<strong>平行文本（De，En）、（En，Fr）</strong>，同时使得该模型能够学得<strong>（De，Fr）</strong>翻译的方法。</p><ul><li><strong>零射学习（zero-shot learning）</strong>：对学习目标不提供任何样本。</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="定义【1】（Expected-Zero-shot-Consistency）"><a href="#定义【1】（Expected-Zero-shot-Consistency）" class="headerlink" title="定义【1】（Expected Zero-shot Consistency）"></a>定义【1】（Expected Zero-shot Consistency）</h3><p>令<script type="math/tex">\varepsilon_s\ \varepsilon_0</script>分别表示<strong>监督（supervised）</strong>和<strong>零射（zero-shot）</strong>任务，<script type="math/tex">\mathcal{l(\cdot)}</script>表示非负的<strong>损失函数</strong>，<script type="math/tex">\mathcal{M}</script>是使用<strong>最大期望监督损失（maximum expected supervised loss）</strong>的模型，并满足下列约束：</p><script type="math/tex; mode=display">\mathrm{max}_{(i,j)\in \varepsilon_s}\mathbb{E}_{x_i,x_j}[l(\mathcal{M})]<\epsilon</script><p>当对于<script type="math/tex">k(\epsilon)>0</script>满足：</p><script type="math/tex; mode=display">\max_{(i,j)\in \varepsilon_0}\mathbb{E}_{x_i,x_j}[l(M)]<k(\epsilon)</script><p>且当<script type="math/tex">\epsilon\to0</script>时，<script type="math/tex">k(\epsilon)\to 0</script></p><p>直观上说明，就是如果<strong>监督任务上的低loss会使零射任务上的loss降低</strong>则称<strong>zero-shot consistent</strong>。</p><h3 id="新方法"><a href="#新方法" class="headerlink" title="新方法"></a>新方法</h3><p>考虑对于四种语言（En，Es，Fr，Ru）的任务：</p><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426201028430.png" alt="image-20210426201028430" style="zoom:67%;"></p><p>考虑对<strong>En-Fr</strong>的联合似然：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}^{ind}(\theta)=\log[\mathbb{P}_\theta(\mathrm{x}_{Fr}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{x}_{En}|\mathrm{x}_{Fr})]</script><script type="math/tex; mode=display">=\log[\sum_{z'_{Es},z'_{Ru}}\mathbb{P}_\theta(\mathrm{x}_{Fr},\mathrm{z}'_{Es},\mathrm{z}'_{Ru}|\mathrm{x}_{En})]\times</script><script type="math/tex; mode=display">\sum_{z''_{Es},z''_{Ru}}\mathbb{P}_\theta(\mathrm{x}_{En},\mathrm{z}''_{Es},\mathrm{z}''_{Ru}|\mathrm{x}_{Fr})</script><p>其中<strong>Es，Ru</strong>是<strong>隐译文（latent translations）</strong>，同时这里假设<script type="math/tex">En\to Fr</script>和<script type="math/tex">Fr\to En</script>是<strong>相互独立</strong>的。</p><p>为了简便起见，令<script type="math/tex">\mathrm{z}:=(\mathrm{z}_{Es},\mathrm{z}_{Ru})</script></p><p>于是得到下面简化的公式：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}^{agree}(\theta)=\log\sum_\mathrm{z}[\mathbb{P}_\theta(\mathrm{x}_{Fr},\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{x}_{En},\mathrm{z}|\mathrm{x}_{Fr})] \tag{2}</script><p>接着利用下列公式：</p><script type="math/tex; mode=display">\mathbb{P}(x,z|y)=\mathbb{P}(x|z,y)\mathbb{P}(z|y)</script><script type="math/tex; mode=display">\mathbb{P}(x_{Fr}|z,x_{En})\approx \mathbb{P}(x_{Fr}|x_{En})</script><p>将<font color="red">(2)</font>式变形为：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}(\theta)\approx \tag{3}</script><script type="math/tex; mode=display">\log \mathbb{P}_\theta(\mathrm{x}_{Fr}|\mathrm{x}_{En})+\log\mathbb{P}_\theta(\mathrm{x}_{En}|\mathrm{x}_{Fr})+\  (composite\ likelihood\ terms)</script><script type="math/tex; mode=display">\log\sum_\mathrm{z}\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{Fr})\ (agreement\ term)</script><h4 id="Lower-bound"><a href="#Lower-bound" class="headerlink" title="Lower bound"></a>Lower bound</h4><p>对上式的<strong>(agreement term)</strong>利用<strong>Jensen不等式</strong>得到下面不等式：</p><script type="math/tex; mode=display">\log\sum_\mathrm{z}\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{Fr})</script><script type="math/tex; mode=display">\ge \mathbb{E}_{\mathrm{z}_{Es}|\mathrm{x}_{En}}[\log \mathbb{P}_\theta(\mathrm{z}_{Es}|\mathrm{x}_{Fr})]+</script><script type="math/tex; mode=display">\mathbb{E}_{\mathrm{z}_{Ru}|\mathrm{x}_{En}}[\log\mathbb{P}_\theta(\mathrm{z}_{Ru}|\mathrm{x}_{Fr})]</script><h3 id="定理【2】（Agreement-Zero-shot-Consistency）"><a href="#定理【2】（Agreement-Zero-shot-Consistency）" class="headerlink" title="定理【2】（Agreement Zero-shot Consistency）"></a>定理【2】（Agreement Zero-shot Consistency）</h3><script type="math/tex; mode=display">L_1,L_2,L_3$$是三种语言，其中$$L_1 \leftrightarrow L_2,L_2\leftrightarrow L_3$$是监督学习，$$L_1\leftrightarrow L_3$$是零射学习的内容。如果$$\mathbb{E}_{\mathrm{x_1,x_2,x_3}}[\mathcal{L}_{12}^{agree}(\theta)+\mathcal{L}_{23}^{agree}{(\theta)}]$$存在边界$$\epsilon>0$$，则：$$\mathbb{E}_{\mathrm{x_1,x_3}}[-\log\mathbb{P}_\theta(\mathrm{x_3}|\mathrm{x_1})]\le k(\epsilon)\ where \ k(\epsilon)\to 0\ as \ \epsilon\to 0</script><h3 id="模型总体结构"><a href="#模型总体结构" class="headerlink" title="模型总体结构"></a>模型总体结构</h3><p>给定平行句子<script type="math/tex">\mathrm{x_{En},x_{Fr}}</script>，以及辅助语言<script type="math/tex">\mathrm{x_{Es}}</script>。</p><h4 id="agreement-term"><a href="#agreement-term" class="headerlink" title="agreement term"></a>agreement term</h4><p><font color="red">(3)</font>式的<strong>agreement term</strong>按下列步骤计算：</p><p>首先将<strong>Es标签</strong>连接（concatenate）到<script type="math/tex">\mathrm{x_{En},x_{Fr}}</script>句子上，然后<strong>编码（encode）</strong>这些句子，使之翻译成<strong>Es</strong>语言</p><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426233520953.png" alt="image-20210426233520953" style="zoom:67%;"></p><p>然后，对编码的句子进行<strong>解码（decode）</strong>，得到辅助语言的翻译内容<script type="math/tex">\mathrm{z_{Es}(x_{En}),z_{Es}(x_{Fr})}</script>。</p><p>接着，用<script type="math/tex">\mathrm{(x_{Fr},z_{Es}(x_{En})),(x_{En},z_{Es}(x_{Fr}))}</script>分别作为两个<strong>平行语料</strong>训练<script type="math/tex">\mathrm{En\to Es,Fr\to Es}</script>两个翻译器，可以计算出：</p><script type="math/tex; mode=display">\log\mathbb{P}_\theta(\mathrm{z_{Es}(x_{Fr})|x_{En}})</script><script type="math/tex; mode=display">\log\mathbb{P}_\theta(\mathrm{z_{Es}(x_{En})|x_{Fr}})</script><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426234435972.png" alt="image-20210426234435972" style="zoom:67%;"></p><h4 id="算法流程如下："><a href="#算法流程如下：" class="headerlink" title="算法流程如下："></a>算法流程如下：</h4><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426234553607.png" alt="image-20210426234553607" style="zoom:67%;"></p><p>在每一轮迭代中，我们希望：</p><ol><li>提高梯度下降对<strong>zero-shot</strong>的影响；</li><li>降低梯度下降对<strong>监督学习</strong>的影响；</li></ol><p>为此分别使用：</p><ol><li><strong>连续贪心解码（greedy continuous decoding）</strong></li><li><strong>停止梯度下降（stop-gradient）</strong></li></ol><p><strong>REF</strong></p><p></p><div id="#refer-*"></div>[*]<a href="https://www.aclweb.org/anthology/N19-1121/">Consistency by Agreement in Zero-shot Neural Machine Translation</a><p></p>]]></content>
    
    
    <summary type="html">本文是对Consistency by Agreement in Zero-shot Neural Machine Translation论文的阅读总结。</summary>
    
    
    
    
    <category term="Paper" scheme="https://1.15.86.100/tags/Paper/"/>
    
    <category term="Neural Machine Translation" scheme="https://1.15.86.100/tags/Neural-Machine-Translation/"/>
    
    <category term="Zero-shot" scheme="https://1.15.86.100/tags/Zero-shot/"/>
    
  </entry>
  
  <entry>
    <title>Triangular Architecture for Rare Language Translation</title>
    <link href="https://1.15.86.100/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/"/>
    <id>https://1.15.86.100/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/</id>
    <published>2021-04-23T07:58:19.000Z</published>
    <updated>2021-04-23T08:52:07.727Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Triangular-Architecture-for-Rare-Language-Translation"><a href="#Triangular-Architecture-for-Rare-Language-Translation" class="headerlink" title="Triangular Architecture for Rare Language Translation[*]"></a>Triangular Architecture for Rare Language Translation<a href="#refer-*"><sup>[*]</sup></a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>作者提出了一种引入<strong>（Y，Z），（X，Y）</strong>平行文本，来增强<strong>（X，Z）</strong>语言机器翻译的<strong>三角形结构</strong>模型<strong>TA-NMT</strong>，。</p><p><img src="/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/image-20210423161225412.png" alt="image-20210423161225412" style="zoom:67%;"></p><p><strong>实线</strong>代表丰富的文本，<strong>虚线</strong>代表少量的文本。</p><p>该论文主要思路是将<script type="math/tex">X\to Y</script>分解成训练两个少量文本的模型<script type="math/tex">X\to Z</script>和<script type="math/tex">Y\to Z</script>。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul><li><p><strong>NMT</strong>首先将输入句子<strong>encode</strong>成<strong>vector</strong>，然后<strong>decoder</strong>在其基础上生成目标句子；</p></li><li><p>为了解决<strong>数据少（data sparsity）</strong>的问题，提出了以下几种方法：</p><ul><li><strong>单语言文本（monolingual data）</strong>；</li><li><strong>反向翻译（back translation）</strong>；</li><li><strong>联合训练（joint training）</strong>；</li><li><strong>对偶学习（dual learning）</strong>；</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="X-to-Y的loss"><a href="#X-to-Y的loss" class="headerlink" title="X \to Y的loss"></a><script type="math/tex">X \to Y</script>的loss</h3><script type="math/tex; mode=display">L(\Theta;D)=\sum_{(x,y)\in D}\mathrm{log}p(y|x)</script><script type="math/tex; mode=display">=\sum_{(x,y)\in D}\mathrm{log}\sum_z p(z|x)p(y|z)</script><script type="math/tex; mode=display">=\sum_{(x,y)\in D}\mathrm{log}\sum_zQ(z)\frac{p(z|x)p(y|z)}{Q(z)}\tag{1}</script><script type="math/tex; mode=display">\ge \sum_{(x,y)\in D}\sum_zQ(z)\mathrm{log}\frac{p(z|x)p(y|z)}{Q(z)}\ (Jensen's\ Inequation)</script><script type="math/tex; mode=display">=\mathcal{L}(Q)</script><p><strong>其中</strong>：</p><ul><li><script type="math/tex">Q(z)</script>是<script type="math/tex">z</script>的<strong>后验分布</strong>，满足<script type="math/tex">\sum_zQ(z)=1</script>；</li><li><script type="math/tex">p(y|x,z) \approx p(y|z)</script>；</li><li><script type="math/tex">D</script>：整个训练集；</li></ul><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>用<strong>EM算法</strong>来最大化<strong>下界</strong><script type="math/tex">\mathcal{L}(Q)</script>。</p><p>这里令<script type="math/tex">Q(z)=p(z|x)</script>来近似：</p><h4 id="M步"><a href="#M步" class="headerlink" title="M步"></a>M步</h4><script type="math/tex; mode=display">\Theta_{y|z}=\mathrm{argmax}_{\Theta_{y|x}}\mathcal{L}(Q)</script><script type="math/tex; mode=display">=\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}\sum_z p(z|x)\mathrm{log}p(y|z)</script><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}E_{z\sim p(z|x)\mathrm{log}p(y|z)} \tag{2}</script><h4 id="E步"><a href="#E步" class="headerlink" title="E步"></a>E步</h4><p>因为：<script type="math/tex">L(\Theta;D)-\mathcal{L}(Q)=\sum_zQ(z)\mathrm{log}\frac{Q(z)}{p(z|y)}</script></p><script type="math/tex; mode=display">=KL(Q(z)||p(z|y))=KL(p(z|x)||p(z|y))</script><p>所以在<strong>E步</strong>，需要最小化上式：</p><script type="math/tex; mode=display">\Theta_{z|x}=\mathrm{argmin}_{\Theta_{z|x}}KL(p(z|x)||p(z|y)) \tag{4}</script><h3 id="双向训练"><a href="#双向训练" class="headerlink" title="双向训练"></a>双向训练</h3><h4 id="X-to-Y方向"><a href="#X-to-Y方向" class="headerlink" title="X\to Y方向"></a><script type="math/tex">X\to Y</script>方向</h4><h5 id="E：最优化-Theta-z-x"><a href="#E：最优化-Theta-z-x" class="headerlink" title="E：最优化\Theta_{z|x}"></a>E：最优化<script type="math/tex">\Theta_{z|x}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{z|x}}KL(p(z|x)||p(z|y)) \tag{5}</script><h5 id="M：最优化-Theta-y-z"><a href="#M：最优化-Theta-y-z" class="headerlink" title="M：最优化\Theta_{y|z}"></a>M：最优化<script type="math/tex">\Theta_{y|z}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}E_{z\sim p(z|x)}\mathrm{log}p(y|z) \tag{6}</script><h4 id="Y-to-X方向"><a href="#Y-to-X方向" class="headerlink" title="Y\to X方向"></a><script type="math/tex">Y\to X</script>方向</h4><h5 id="E：最优化-Theta-z-y"><a href="#E：最优化-Theta-z-y" class="headerlink" title="E：最优化\Theta_{z|y}"></a>E：最优化<script type="math/tex">\Theta_{z|y}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{z|y}}KL(p(z|y)||p(z|x)) \tag{7}</script><h5 id="M：最优化-Theta-x-z"><a href="#M：最优化-Theta-x-z" class="headerlink" title="M：最优化\Theta_{x|z}"></a>M：最优化<script type="math/tex">\Theta_{x|z}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{x|z}}\sum_{(x,y)\in D}E_{z\sim p(z|y)}\mathrm{log}p(x|z) \tag{8}</script><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>训练过程的难点在于<strong>分解候选词的搜索空间（exponential search space of the translation candidates）</strong>，这里使用<strong>采样</strong>方法。</p><p>将<font color="red">(5)(7)</font>式改写成如下形式：</p><script type="math/tex; mode=display">\bigtriangledown_{\Theta_{z|x}}KL(p(z|x)||p(z|y))</script><script type="math/tex; mode=display">=E_{z\sim p(z|x)}\mathrm{log}\frac{p(z|x)}{p(z|y)}\bigtriangledown_{\Theta_{z|x}}\mathrm{log}p(z|x)\tag{9}</script><script type="math/tex; mode=display">\bigtriangledown_{\Theta_{z|y}}KL(p(z|y)||p(z|x))</script><script type="math/tex; mode=display">=E_{z\sim p(z|y)}\mathrm{log}\frac{p(z|y)}{p(z|x)}\bigtriangledown_{\Theta_{z|y}}\mathrm{log}p(z|y)\tag{10}</script><p>因为数据的<strong>target</strong>是由模型生成的，效果不好，使用不使用<strong>BLEU</strong>，而是使用<strong>IBM</strong>loss函数。</p><p><strong>REF</strong>:</p><p></p><div id="#refer-*"></div>[*]<a href="https://arxiv.org/abs/1805.04813">Triangular Architecture for Rare Language Translation</a><p></p>]]></content>
    
    
    <summary type="html">本文是对Triangular Architecture for Rare Language Translation论文的阅读总结。</summary>
    
    
    
    
    <category term="Paper" scheme="https://1.15.86.100/tags/Paper/"/>
    
    <category term="Monolingual Data" scheme="https://1.15.86.100/tags/Monolingual-Data/"/>
    
  </entry>
  
  <entry>
    <title>KL-Divergence</title>
    <link href="https://1.15.86.100/2021/04/20/KL-Divergence/"/>
    <id>https://1.15.86.100/2021/04/20/KL-Divergence/</id>
    <published>2021-04-20T12:44:22.000Z</published>
    <updated>2021-04-20T12:58:43.752Z</updated>
    
    <content type="html"><![CDATA[<h1 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL Divergence"></a>KL Divergence</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><h3 id="连续形式"><a href="#连续形式" class="headerlink" title="连续形式"></a>连续形式</h3><script type="math/tex; mode=display">\mathrm{KL}(P||Q)=\int_x P(x)\mathrm{log}\frac{P(x)}{Q(x)}</script><h3 id="离散形式"><a href="#离散形式" class="headerlink" title="离散形式"></a>离散形式</h3><script type="math/tex; mode=display">\mathrm{KL}(P||Q)=\sum_{i=1}^N P(x_i)\mathrm{log}\frac{P(x_i)}{Q(x_i)}</script><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><p>只有在<script type="math/tex">P(x)=Q(x)</script>时等于0，其他任何时候都大于0。</p>]]></content>
    
    
    <summary type="html">本文主要介绍KL散度概念</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Math" scheme="https://1.15.86.100/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
    <link href="https://1.15.86.100/2021/04/20/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate/"/>
    <id>https://1.15.86.100/2021/04/20/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate/</id>
    <published>2021-04-20T12:29:29.000Z</published>
    <updated>2021-04-20T12:38:26.919Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"><a href="#Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate" class="headerlink" title="Neural Machine Translation by Jointly Learning to Align and Translate[*]"></a>Neural Machine Translation by Jointly Learning to Align and Translate<a href="#refer-*"><sup>[*]</sup></a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文是<strong>Attention</strong>的开山之作，主要是为了解决<strong>RNN</strong>对长句子表现不佳而发表的，论文作者之一是鼎鼎大名的<strong>图灵奖</strong>得主<strong>Bengio</strong>。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p></p><div id="#refer-*"></div>[*]<a href="https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5?p2df">Neural Machine Translation by Jointly Learning to Align and Translate</a><p></p>]]></content>
    
    
    <summary type="html">本文是对Neural Machine Translation by Jointly Learning to Align and Translate的阅读总结。</summary>
    
    
    
    
    <category term="Paper" scheme="https://1.15.86.100/tags/Paper/"/>
    
    <category term="Neural Machine Translation" scheme="https://1.15.86.100/tags/Neural-Machine-Translation/"/>
    
    <category term="Jointly Learning" scheme="https://1.15.86.100/tags/Jointly-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Understanding Back-Translation at Scale</title>
    <link href="https://1.15.86.100/2021/04/20/Understanding-Back-Translation-at-Scale/"/>
    <id>https://1.15.86.100/2021/04/20/Understanding-Back-Translation-at-Scale/</id>
    <published>2021-04-20T07:45:06.000Z</published>
    <updated>2021-04-20T09:12:04.414Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Understanding-Back-Translation-at-Scale"><a href="#Understanding-Back-Translation-at-Scale" class="headerlink" title="Understanding Back-Translation at Scale[*]"></a>Understanding Back-Translation at Scale<a href="#refer-anchor-*"><sup>[*]</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="单语言文本"><a href="#单语言文本" class="headerlink" title="单语言文本"></a>单语言文本</h3><p>单语言文本可以用来提高<strong>统计机器翻译（statistical machine translation）</strong>的流畅度。</p><p>常用的方法有：</p><ul><li><strong>模型融合（language model fusion）</strong><a href="#refer-anchor-1"><sup>[1]</sup></a><a href="#refer-anchor-2"><sup>[2]</sup></a>；</li><li><strong>反向翻译（back translation）</strong><a href="#refer-anchor-3"><sup>[3]</sup></a>；</li><li><strong>结对学习（dual learning）</strong><a href="#refer-anchor-4"><sup>[4]</sup></a>；</li></ul><h3 id="反向翻译"><a href="#反向翻译" class="headerlink" title="反向翻译"></a>反向翻译</h3><p><strong>反向翻译</strong>需要训练一个<strong>目标语言到源语言（target-to-source）</strong>的系统，利用这个系统从<strong>目标语言</strong>的单语言文本来生成额外的<strong>平行语料（synthetic parallel data）</strong>。然后利用这些生成的语料来补充<strong>人工语料（human bitext）</strong>来训练<strong>源语言到目标语言（source-to-target）</strong>的系统【真正需要的】。</p><h2 id="Generating-synthetic-sources"><a href="#Generating-synthetic-sources" class="headerlink" title="Generating synthetic sources"></a>Generating synthetic sources</h2><h3 id="最大后验估计（MAP）"><a href="#最大后验估计（MAP）" class="headerlink" title="最大后验估计（MAP）"></a>最大后验估计（MAP）</h3><p>传统的<strong>MAP</strong>方法可能导致译文的<strong>丰富度（less rich）</strong>降低。</p><p><strong>集束搜索（beam）</strong>和<strong>贪心算法（greedy）</strong>主要关心模型分布的头部样本，这样就会导致生成的语句分布出现偏差。</p><h3 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h3><p>该文章提出了两种采样方法以及添加一种噪声的方法。</p><ul><li><strong>非限制性采样（unrestricted sampling）</strong>，从文本中随机采样；</li><li><strong>限制性采样（restricted sampling）</strong>，每步会挑选出可能性最大的<strong>k</strong>个词（token），然后从这些词中随机采样一个作为结果；</li><li><strong>集束搜索+噪声</strong>，向集束搜索得到的句子添加噪声<ul><li>0.1的概率删除一个单词；</li><li>0.1的概率将一个单词替换成<strong>填充单词（filler token）</strong>；</li><li>按均匀分布随机调换两个相隔不超过三个位置的单词；</li></ul></li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment[#]"></a>Experiment<a href="#refer-anchor-#"><sup>[#]</sup></a></h2><p>使用<strong>Transformer</strong>模型进行实验。</p><h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><h3 id="合成数据生成方法"><a href="#合成数据生成方法" class="headerlink" title="合成数据生成方法"></a>合成数据生成方法</h3><p>该使用用5种方法分别进行生成<strong>合成数据（synthetic data）</strong>：</p><ul><li><strong>贪心算法</strong>【greedy】；</li><li>k=5的<strong>集束搜索</strong>【beam】；</li><li><strong>非限制采样</strong>【sampling】；</li><li>从k=10个最高可能性中随机采样【top10】；</li><li>在集束搜索上添加噪声【beam+nosing】；</li></ul><p><img src="/2021/04/20/Understanding-Back-Translation-at-Scale/image-20210420162836335.png" alt="image-20210420162836335" style="zoom:67%;"></p><p>结果显示<strong>sampling</strong>、<strong>beam+nosing</strong>更好。</p><h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><p><strong>集束搜索</strong>关注可能性非常大的输出，这就导致文本的<strong>丰富性（richness）</strong>和<strong>多样性（diversity）</strong>降低。</p><p><strong>采样</strong>和<strong>添加噪声</strong>可以比卖你这些问题。</p><h3 id="语料的领域对结果影响"><a href="#语料的领域对结果影响" class="headerlink" title="语料的领域对结果影响"></a>语料的领域对结果影响</h3><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><p>分别将下列<strong>sampling</strong>生成的语料添加到 原来<strong>640k</strong>平行语料中，构成不同的训练集来训练相同的翻译器：</p><ul><li>剩下的平行数据【bitext】</li><li>将剩下的平行数据<strong>反向翻译</strong>得到的数据【BT-bitext】</li><li>将<strong>newscrawl</strong>经过<strong>反向翻译</strong>得到的数据【BT-news】</li></ul><p>将训练得到的翻译器在不同验证集实验，实验结果如下：</p><p><img src="/2021/04/20/Understanding-Back-Translation-at-Scale/image-20210420170028156.png" alt="image-20210420170028156"></p><p><strong>反向翻译</strong>的<strong>生成数据</strong>对模型有很大提升，特别是当数据领域相同时。</p><h3 id="平行语料缺乏情况"><a href="#平行语料缺乏情况" class="headerlink" title="平行语料缺乏情况"></a>平行语料缺乏情况</h3><h4 id="实验设置-1"><a href="#实验设置-1" class="headerlink" title="实验设置"></a>实验设置</h4><p>从训练数据中随机采样<strong>80K</strong>和<strong>640K</strong>句子对，然后分别添加由<strong>sampling</strong>和<strong>beam</strong>得到的<strong>合成文本</strong>【添加数量根据下图横坐标】。结果如下：</p><p><img src="/2021/04/20/Understanding-Back-Translation-at-Scale/image-20210420164558902.png" alt="image-20210420164558902" style="zoom:67%;"></p><h4 id="结果分析-1"><a href="#结果分析-1" class="headerlink" title="结果分析"></a>结果分析</h4><p>发现<strong>平行文本</strong>为<strong>640k</strong>和<strong>5M</strong>时，<strong>sampling</strong>都比<strong>beam</strong>好，但当<strong>平行文本</strong>是<strong>80k</strong>时相反。</p><p>原因是：</p><p><strong>80k</strong>时，模型训练效果较差，<strong>反向翻译</strong>的噪声会极大影响模型的效果。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p><strong>随机采样</strong>和<strong>集束搜索+噪声</strong>相比<strong>贪心算法</strong>和<strong>集束搜索</strong>效果更好。</p><p><strong>REF</strong>:</p><p></p><div id="refer-anchor-*"></div> [*] <a href="https://www.researchgate.net/publication/327280445_Understanding_Back-Translation_at_Scale">Understanding Back-Translation at Scale</a><p></p><p></p><div id="refer-anchor-#"></div> [#] <a href="https://github.com/pytorch/fairseq">Code</a><p></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1503.03535">On Using Monolingual Corpora in Neural Machine Translation</a><p></p><p></p><div id="refer-anchor-2"></div> [2] <a href="https://dl.acm.org/doi/10.1016/j.csl.2017.01.014">On integrating a language model into neural machine translation</a><p></p><p></p><div id="refer-anchor-3"></div> [3] <a href="https://www.aclweb.org/anthology/P16-1009/">Improving Neural Machine Translation Models with Monolingual Data</a><p></p><p></p><div id="refer-anchor-4"></div> [4] <a href="https://www.researchgate.net/publication/335400882_Semi-supervised_Learning_for_Neural_Machine_Translation">Semi-supervised Learning for Neural Machine Translation</a><p></p>]]></content>
    
    
    <summary type="html">本文是对Understanding Back-Translation at Scale论文的阅读总结。</summary>
    
    
    
    
    <category term="Paper" scheme="https://1.15.86.100/tags/Paper/"/>
    
    <category term="Back Translation" scheme="https://1.15.86.100/tags/Back-Translation/"/>
    
  </entry>
  
  <entry>
    <title>Joint Training for Neural Machine Translation Models with Monolingual Data</title>
    <link href="https://1.15.86.100/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/"/>
    <id>https://1.15.86.100/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/</id>
    <published>2021-04-19T13:20:51.000Z</published>
    <updated>2021-04-20T13:30:04.861Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data1"><a href="#Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data1" class="headerlink" title="Joint Training for Neural Machine Translation Models with Monolingual Data1"></a>Joint Training for Neural Machine Translation Models with Monolingual Data<a href="#refer-anchor-1"><sup>1</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="背景关键词"><a href="#背景关键词" class="headerlink" title="背景关键词"></a>背景关键词</h3><ul><li><strong>encoder-decoder</strong></li><li><strong>NMT</strong></li></ul><h3 id="传统NMT面临的问题"><a href="#传统NMT面临的问题" class="headerlink" title="传统NMT面临的问题"></a>传统NMT面临的问题</h3><ul><li>传统<strong>NMT</strong>所需要的<strong>平行文本（parallel data）</strong>缺乏；</li><li>传统<strong>NMT</strong>高度依赖高质量的<strong>平行文本</strong>，并且在<strong>平行文本</strong>缺乏和<strong>特定领域（domain-specific）</strong>的任务上表现很差；</li><li>但<strong>单语言（Monolingual Data）</strong>【即该语言文本没有对应的译文数据】文本丰富，易收集且多样；</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>本模型采用<strong>联合训练（Joint Training）</strong>和<strong>反向翻译（Back Translation）</strong>的方式，提出一种新的<strong>联合训练</strong>方法。</p><p>该模型使用两个<strong>NMT</strong>模型来<strong>联合训练</strong>：</p><p><strong>A-model</strong>：从<strong>源语言</strong>到<strong>目标语言（source-to-target）</strong>的<strong>NMT</strong>模型；</p><p><strong>B-model</strong>：从<strong>目标语言</strong>到<strong>源语言（target-to-source）</strong>的<strong>NMT</strong>模型；</p><h3 id="联合训练方法"><a href="#联合训练方法" class="headerlink" title="联合训练方法"></a>联合训练方法</h3><p>在模型开始时，利用少量的<strong>平行文本D</strong>训练出两个翻译器<strong>A-model</strong>和<strong>B-model</strong>。</p><p>然后，该模型的<strong>联合训练</strong>是采用<strong>迭代训练（iterative）</strong>的过程【如下图】。在每个迭代中：</p><ul><li><strong>B-model</strong>利用<strong>目标语言</strong>的单文本得到对应的<strong>源语言</strong>文本，该生成的平行文本再加上平行文本D作为<strong>A-model</strong>的<strong>伪训练数据（pseudo-training data）</strong>。即<strong>A-model</strong>利用<strong>B-model</strong>生成的<strong>源语言</strong>文本作为输入，而原来的单文本作为目标，进行训练；</li><li>同理，<strong>A-model</strong>利用<strong>源语言</strong>的单文本得到对应的<strong>目标语言</strong>文本，该生成的平行文本再加上平行文本D作为<strong>B-model</strong>的<strong>伪训练数据</strong>。</li></ul><p><img src="/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/image-20210419233220188.png" alt="image-20210419233220188"></p><p><strong>图中</strong>：</p><p>在第一轮迭代之前：<script type="math/tex">M^0_{x\to y},M^0_{y\to x}</script>，都是利用<strong>平行文本</strong><script type="math/tex">D=\{x^n,y^n\}</script>预训练得到；</p><p>在每一轮迭代中：</p><ul><li><script type="math/tex">M^i_{x\to y}</script>利用<script type="math/tex">X={x^{(s)}}</script>，得到翻译后的文本<script type="math/tex">Y'_i={y_i^{(s)}}</script>；</li><li><script type="math/tex">M^i_{y\to x}</script>利用<script type="math/tex">Y={y^{(s)}}</script>，得到翻译后的文本<script type="math/tex">X'_i={x_i^{(s)}}</script>；</li><li>利用<script type="math/tex">\{X_i',Y\}\cup D</script>训练模型<script type="math/tex">M^i_{x\to y}</script>；</li><li>利用<script type="math/tex">\{Y_i',X\}\cup D</script>训练模型<script type="math/tex">M^i_{y\to x}</script>；</li></ul><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><img src="/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/image-20210419234535538.png" alt="image-20210419234535538"></p><h2 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h2><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ul><li><strong>平行语料</strong>：<script type="math/tex">D=\{(x^{(n)}, y^{(n)})\}_{n=1}^N</script></li><li><strong>目标语言的单语言语料</strong>：<script type="math/tex">Y=\{y^{(t)}\}_{t=1}^T</script></li></ul><h3 id="优化过程"><a href="#优化过程" class="headerlink" title="优化过程"></a>优化过程</h3><p>优化目标如下：</p><script type="math/tex; mode=display">L^*(\theta_{x\to y})=\sum_{n=1}^N\mathrm{log}p(y^{(n)}|x^{(n)})+\sum_{t=1}^T\mathrm{log}p(y^{(t)}) \tag{1}</script><p>右边第一项是<strong>平行语料</strong>训练的<strong>似然</strong>，第二项是单语言中<strong>目标语言一边</strong>的<strong>似然</strong>。</p><p>接着，我们引入另一模型将<strong>目标语言</strong>文本 翻译成的<strong>源语言</strong>文本 作为<strong>目标语言</strong>的<strong>隐状态（hidden state）</strong>。</p><p>然后，将<script type="math/tex">\mathrm{log}p(y^{(t)})</script>进行分解，得到下列公式：</p><script type="math/tex; mode=display">\mathrm{log}p(y^{(t)})=\mathrm{log}\sum_xp(x,y^{(t)})=\mathrm{log}\sum_xQ(x)\frac{p(x,y^{(t)})}{Q(x)}</script><script type="math/tex; mode=display">\ge \sum_xQ(x)\mathrm{log}\frac{p(x, y^{(t)})}{Q(x)}\ \  (\bold{Jensen's\ inequality})</script><script type="math/tex; mode=display">= \sum_x[Q(x)\mathrm{log}p(y^{(t)}|x)-KL(Q(x)||p(x))] \tag{2}</script><p>其中：</p><ul><li><script type="math/tex">x</script>：目标文本对应的源文本；</li><li><script type="math/tex">Q(x)</script>：<script type="math/tex">x</script>的分布的估计；</li><li><script type="math/tex">p(x)</script>：<script type="math/tex">x</script>的<strong>边缘分布（marginal distribution）</strong>；</li><li><script type="math/tex">KL(\cdot|\cdot)</script>：<a href="http://1.15.86.100/2021/04/20/KL-Divergence/"><strong>KL散度（Kullback-Leibler Divergence）</strong></a></li></ul><p>为了使得式<font color="red">(2)</font>的等号成立，<script type="math/tex">Q(x)</script>必须满足下列条件：</p><script type="math/tex; mode=display">\frac{p(x,y^{(t)})}{Q(x)}=c\ (constant) \tag{3}</script><p>同时，对上式变形得到：</p><script type="math/tex; mode=display">Q(x)=\frac{p(x,y^{(t)})}{c}=\frac{p(x,y^{(t)})}{\sum_xp(x,y^{(t)})}=p^*(x|y^{(t)})</script><p>其中<script type="math/tex">p^*(x|y^{(t)})</script>表示真实的<strong>目标语言到源语言（target-to-source）</strong>的概率，但实际上难以求出，因此使用<strong>机器翻译模型</strong>计算的<script type="math/tex">p(x|y^{(t)})</script>作为替代。</p><p>同时，<strong>KL散度</strong>可以忽略，所以有：</p><script type="math/tex; mode=display">L(\theta_{x\to y})=\sum_{n=1}^N\mathrm{log}p(y^{(n)}|x^{(n)})+\sum_{t=1}^T\sum_xp(x|y^{(t)})\mathrm{log}p(y^{(t)}|x) \tag{4}</script><p>其中第一部分是<strong>极大似然估计（MLE）</strong>，第二部分可以用<strong>EM算法</strong>估计。</p><p>综上：</p><p>对于<script type="math/tex">M_{x\to y}</script>，优化目标为：</p><script type="math/tex; mode=display">L(\theta_{x\to y})=\sum_{n=1}^N\mathrm{log}p(y^{(n)|}|x^{(n)})+\sum_{t=1}^T\mathrm{log}p(y^{(t)}|M_{y\to x}(y^{t}))</script><p>对于<script type="math/tex">M_{y\to x}</script>，优化目标为：</p><script type="math/tex; mode=display">L(\theta_{y\to x})=\sum_{n=1}^N\mathrm{log}p(x^{(n)|}|y^{(n)})+\sum_{t=1}^T\mathrm{log}p(x^{(t)}|M_{x\to y}(x^{t}))</script><p>总的优化目标为：</p><script type="math/tex; mode=display">L(\theta)=L(\theta_{x\to y})+L(\theta_{y\to x})</script><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/image-20210420212934733.png" alt="image-20210420212934733"></p><p><strong>JT-NMT</strong>是本模型。</p><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1803.00353v1">Joint Training for Neural Machine Translation Modelswith Monolingual Data</a><p></p>]]></content>
    
    
    <summary type="html">本文是对 Joint Training for Neural Machine Translation Models with Monolingual Data 论文的阅读总结。</summary>
    
    
    
    
    <category term="Paper" scheme="https://1.15.86.100/tags/Paper/"/>
    
    <category term="Neural Machine Translation" scheme="https://1.15.86.100/tags/Neural-Machine-Translation/"/>
    
    <category term="Back Translation" scheme="https://1.15.86.100/tags/Back-Translation/"/>
    
    <category term="Semi-supervised Learning" scheme="https://1.15.86.100/tags/Semi-supervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Nginx</title>
    <link href="https://1.15.86.100/2021/04/18/Nginx/"/>
    <id>https://1.15.86.100/2021/04/18/Nginx/</id>
    <published>2021-04-18T08:17:19.000Z</published>
    <updated>2021-04-18T09:00:32.020Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="配置文件位置"><a href="#配置文件位置" class="headerlink" title="配置文件位置"></a>配置文件位置</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/nginx/conf.d</span><br></pre></td></tr></tbody></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx start</span><br></pre></td></tr></tbody></table></figure><h3 id="停止"><a href="#停止" class="headerlink" title="停止"></a>停止</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx stop</span><br></pre></td></tr></tbody></table></figure><h3 id="重启"><a href="#重启" class="headerlink" title="重启"></a>重启</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx [restart|reload|force-reload|status|configtest|rotate|upgrade]</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <summary type="html">Nginx相关操作</summary>
    
    
    
    
    <category term="Nginx, Linux" scheme="https://1.15.86.100/tags/Nginx-Linux/"/>
    
  </entry>
  
  <entry>
    <title>ML Metrics</title>
    <link href="https://1.15.86.100/2021/04/17/ML-Metrics/"/>
    <id>https://1.15.86.100/2021/04/17/ML-Metrics/</id>
    <published>2021-04-17T02:12:13.000Z</published>
    <updated>2021-05-16T08:00:59.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ML-Metrics"><a href="#ML-Metrics" class="headerlink" title="ML Metrics"></a>ML Metrics</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h2 id="Classification-Metrics（分类）"><a href="#Classification-Metrics（分类）" class="headerlink" title="Classification Metrics（分类）"></a>Classification Metrics（分类）</h2><h3 id="confusion-matrix"><a href="#confusion-matrix" class="headerlink" title="confusion matrix"></a>confusion matrix</h3><p><img src="/2021/04/17/ML-Metrics/image-20210418230321354.png" alt="image-20210418230321354" style="zoom: 50%;"></p><ul><li><strong>TP（真正例）</strong>：将<strong>正类</strong>样本预测<strong>正确（为正）</strong>；</li><li><strong>FN（假反例）</strong>：将<strong>正例</strong>样本预测<strong>错误（为反）</strong>；</li><li><strong>FP（假正例）</strong>：将<strong>反例</strong>样本预测<strong>错误（为正）</strong>；</li><li><strong>TN（真反例）</strong>：将<strong>反例</strong>样本预测<strong>正确（为反）</strong>；</li></ul><h3 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h3><p><strong>准确率</strong></p><p><strong>预测正确</strong>的样本数占<strong>样本总数</strong>的比例。</p><script type="math/tex; mode=display">ACC=\frac{TP+TN}{TP+FN+FP+TN}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=<span class="literal">True</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Error-rate"><a href="#Error-rate" class="headerlink" title="Error rate"></a>Error rate</h3><p><strong>错误率</strong></p><p><strong>预测错误</strong>的样本数占<strong>样本总数</strong>的比例。</p><script type="math/tex; mode=display">Error=\frac{FP+FN}{TP+FN+FP+TN}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> - sklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=<span class="literal">True</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h3><p><strong>召回率、查全率</strong>、<strong>灵敏度（sensitivity）</strong>、<strong>真阳性率</strong></p><p><strong>真实为正</strong>中<strong>预测为正</strong>的比例。</p><script type="math/tex; mode=display">Recall=\frac{TP}{TP+FN}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.recall_score(y_true, y_pred, *, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">'binary'</span>, sample_weight=<span class="literal">None</span>, zero_division=<span class="string">'warn'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Precision"><a href="#Precision" class="headerlink" title="Precision"></a>Precision</h3><p><strong>精确度、查准率</strong></p><p><strong>预测为正</strong>中<strong>真实为正</strong>的比例。</p><script type="math/tex; mode=display">Precision=\frac{TP}{TP+FP}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.precision_score(y_true, y_pred, *, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">'binary'</span>, sample_weight=<span class="literal">None</span>, zero_division=<span class="string">'warn'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Specificity"><a href="#Specificity" class="headerlink" title="Specificity"></a>Specificity</h3><p><strong>特异度</strong>、<strong>真阴性率</strong></p><p><strong>真实为反</strong>中<strong>预测为反</strong>的比例。</p><script type="math/tex; mode=display">Sepcificity=\frac{TN}{TN+FP}</script><hr><h3 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h3><p>可以看成对<strong>精确度</strong>和<strong>召回率</strong>的加权平均。</p><script type="math/tex; mode=display">F1=2\cdot \frac{recall\cdot precision}{recall + precision}</script><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.f1_score(y_true, y_pred, *, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">'binary'</span>, sample_weight=<span class="literal">None</span>, zero_division=<span class="string">'warn'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Regression-Metrics（回归）"><a href="#Regression-Metrics（回归）" class="headerlink" title="Regression Metrics（回归）"></a>Regression Metrics（回归）</h2><h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h3><p><strong>均方差</strong>误差。</p><script type="math/tex; mode=display">\mathrm{MSE}(y,\hat{y})=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2</script><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error(y_true, y_pred, *, sample_weight=<span class="literal">None</span>, multioutput=<span class="string">'uniform_average'</span>, squared=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><p><strong>平均绝对值误差</strong>。</p><script type="math/tex; mode=display">\mathrm{MAE}(y,\hat{y})=\frac{1}{N}\sum_{i=1}^N|y_i-\hat{y_i}|</script><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_absolute_error(y_true, y_pred, *, sample_weight=<span class="literal">None</span>, multioutput=<span class="string">'uniform_average'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="MSLE"><a href="#MSLE" class="headerlink" title="MSLE"></a>MSLE</h3><p><strong>对数均方误差</strong>。</p><script type="math/tex; mode=display">\mathrm{MSLE}(y,\hat{y})=\frac{1}{N}\sum_{i=1}^N(\ln(1+y_i)-\ln(1+\hat{y_i}))^2</script><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_log_error(y_true, y_pred, *, sample_weight=<span class="literal">None</span>, multioutput=<span class="string">'uniform_average'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Max-Error"><a href="#Max-Error" class="headerlink" title="Max Error"></a>Max Error</h3><p><strong>最大误差</strong>。</p><script type="math/tex; mode=display">\mathrm{Max\_Error}(y,\hat{y})=\max(|y_i-\hat{y_i}|)</script><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.max_error(y_true, y_pred)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="R-2"><a href="#R-2" class="headerlink" title="R^2"></a><script type="math/tex">R^2</script></h3><p><strong>决定系数</strong>。</p><script type="math/tex; mode=display">R^2(y,\hat{y_i})=1-\frac{\sum_{i=1}^N(y_i-\hat{y_i})^2}{\sum_{i=1}^N(y_i-\bar{y})^2}</script><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.r2_score(y_true, y_pred, *, sample_weight=<span class="literal">None</span>, multioutput=<span class="string">'uniform_average'</span>)</span><br></pre></td></tr></tbody></table></figure><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://scikit-learn.org/stable/modules/classes.html?highlight=metric#module-sklearn.metrics">sklearn.metrics</a><p></p>]]></content>
    
    
    <summary type="html">本文主要介绍机器学习性能度量方法（主要依赖sklearn.metrics）</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Metrics" scheme="https://1.15.86.100/tags/Metrics/"/>
    
  </entry>
  
  <entry>
    <title>Standardization</title>
    <link href="https://1.15.86.100/2021/04/03/Standardization/"/>
    <id>https://1.15.86.100/2021/04/03/Standardization/</id>
    <published>2021-04-03T08:16:38.000Z</published>
    <updated>2021-05-15T03:03:22.777Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>数据标准化</strong>就是把数据集中服从不同”分布“的各个属性空间，通过数学变换到同一的”分布空间“，一般映射到<strong>[0,1]</strong>范围。</p><h3 id="数据标准化的必要性"><a href="#数据标准化的必要性" class="headerlink" title="数据标准化的必要性"></a>数据标准化的必要性</h3><p>例如给定一条数据，其不同属性的值分别为</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">100, 1.0, 0.1, 0.01</span><br></pre></td></tr></tbody></table></figure><p>那么，假如每个属性都有一个0.01的微小扰动，则第四个属性的扰动对整个数据的影响远远大于第一个属性。</p><p>同时，对于机器学习模型，数据未标准化会对模型产生影响。</p><h2 id="数据标准化方法"><a href="#数据标准化方法" class="headerlink" title="数据标准化方法"></a>数据标准化方法</h2><p>数据集：</p><script type="math/tex; mode=display">X=\{x_1,x_2, \dots,x_n\}</script><h3 id="min-max方法"><a href="#min-max方法" class="headerlink" title="min-max方法"></a>min-max方法</h3><p>即通过<strong>线性变换</strong>将数据映射到<strong>[0,1]</strong>范围。</p><script type="math/tex; mode=display">f(x_i)=\frac{x_i-\mathrm{min}(X)}{\mathrm{max}(X)-\mathrm{min}(X)}\ for\ 1\le i \le n</script><p><strong>适用范围</strong>：将数据简单变换到某一个范围，当有新数据加入时，<strong>最大值</strong>，<strong>最小值</strong>会发生变化。</p><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p2.png" alt=""></p><h3 id="z-score方法"><a href="#z-score方法" class="headerlink" title="z-score方法"></a>z-score方法</h3><p>通过<strong>按比例缩放</strong>的形式，将数据映射到<strong>[-x,x]</strong>范围，要求<script type="math/tex">\mu=0,\sigma=1</script>。</p><script type="math/tex; mode=display">f(x_i)=\frac{x_i- \mu}{\sigma},\mu=\mathrm{mean}(X),\sigma=\sqrt{\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2}</script><p><strong>适用范围</strong>：适用于数据中<strong>最大值</strong>、<strong>最小值</strong>未知，数据<strong>分布离散</strong>的情况。</p><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p3.png" alt=""></p><h3 id="小数定标标准化"><a href="#小数定标标准化" class="headerlink" title="小数定标标准化"></a>小数定标标准化</h3><p>将数据全部投影到小数。</p><script type="math/tex; mode=display">f(x_i)=\frac{x_i}{10^k}$$，其中$$k$$是数据中**最大绝对值**的位数。### logistic方法通过函数映射，将数据映射到**[0,1]**范围。$$f(x_i)=\frac{1}{1+e^{-x_i}}</script><p><img src="/2021/04/03/Standardization/p1.png" alt=""></p><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p4.png" alt=""></p><p><strong>Code</strong></p><p><a href="https://github.com/baowj-678/Machine-Learning/tree/master/Standardization">github:baowj-678</a></p>]]></content>
    
    
    <summary type="html">数据标准化是特征工程或者数据预处理中很关键的一个环节，这里会对数据标准化进行简单介绍。</summary>
    
    
    
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>Feature Selection: Embedding</title>
    <link href="https://1.15.86.100/2021/04/02/Feature-Selection-Embedding/"/>
    <id>https://1.15.86.100/2021/04/02/Feature-Selection-Embedding/</id>
    <published>2021-04-02T12:28:50.000Z</published>
    <updated>2021-04-03T13:18:00.392Z</updated>
    
    <content type="html"><![CDATA[<h1 id="嵌入式特征选择"><a href="#嵌入式特征选择" class="headerlink" title="嵌入式特征选择"></a>嵌入式特征选择</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>嵌入式特征选择（Embedding）</strong>是将特征选择和模型训练融为一体，也就是模型中包括了特征选择的过程，而训练时只需将原始数据全部输入到模型中即可。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>给定数据集</p><script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\}</script><script type="math/tex; mode=display">x_i\in \mathbb{R}^d</script><script type="math/tex; mode=display">y_i\in \mathbb{R}</script><p>如果使用<strong>线性回归模型</strong>，则优化目标为：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d</script><p>而上述模型可能会陷入<strong>过拟合（over fitting）</strong>，为了解决这个问题，会引入<strong>L1正则化</strong>和<strong>L2正则化</strong>；</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p><strong>L2正则化</strong>将优化目标修改为：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d+\lambda||w||_2^2,\lambda>0</script><p>该模型称为<strong>岭回归</strong>，能降低过拟合风险。</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>将<strong>L2正则化</strong>的<strong>2范数</strong>替换成<strong>1范数</strong>，就得到<strong>L1正则化</strong>：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d+\lambda||w||_1,\lambda>0</script><p>该模型称为<strong>LASSO</strong>。</p><p><strong>L1正则化</strong>除了能降低<strong>过拟合</strong>风险，而且该模型还会倾向于<strong>”稀疏解“</strong>，即该模型倾向于只使用原数据的某些<strong>子属性</strong>，这就间接等同于进行了<strong>特征选择</strong>。</p><p>下面介绍为何会倾向<strong>稀疏解</strong>。</p><p><img src="/2021/04/02/Feature-Selection-Embedding/p1.png" alt="image-20210403211035916"></p><p>假设该模型只有两个参数<script type="math/tex">w_1,w_2</script>，则其<strong>L1范数</strong>和<strong>L2范数</strong>等值线如图所示。<strong>平方项</strong>（即原始的最优化目标）等值线近似如图所示。可以发现，平方误差加上范数误差，对于<strong>L1范数</strong>来说，最小值易在<strong>两个坐标轴上取的</strong>，而<strong>L2范数</strong>偏向在<strong>第一象限取的</strong>。当最优值在坐标轴取的时，<script type="math/tex">w_1,w_2</script>其中一个值就为0，也就是该特征被剔除了，所以易得到<strong>稀疏解</strong>。</p><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p253</p>]]></content>
    
    
    <summary type="html">介绍嵌入式(Embedding)特征选择的方法和L1正则化相关问题。</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
    <category term="L1" scheme="https://1.15.86.100/tags/L1/"/>
    
  </entry>
  
  <entry>
    <title>Feature Selection: Wrapper</title>
    <link href="https://1.15.86.100/2021/04/02/Feature-Selection-Wrapper/"/>
    <id>https://1.15.86.100/2021/04/02/Feature-Selection-Wrapper/</id>
    <published>2021-04-02T12:28:38.000Z</published>
    <updated>2021-04-02T12:57:57.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="包裹式特征选择"><a href="#包裹式特征选择" class="headerlink" title="包裹式特征选择"></a>包裹式特征选择</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>包裹式特征选择</strong>（Wrapper）是特征选择的三大方法之一，其主要思路是：利用选择的特征的子数据集进行训练，并把训练结果直接作为评判特征选择的标准。</p><p>所以在<strong>特定模型</strong>的最终结果上，<strong>包裹式特征选择</strong>比<strong>选择式</strong>更好。</p><h2 id="LVW"><a href="#LVW" class="headerlink" title="LVW"></a>LVW</h2><p><strong>拉斯维加斯方法（LVW，Las Vegas Wrapper）</strong>是一种典型的<strong>包裹式特征选择</strong>方法。</p><p>该方法每次随机选取特征子集，然后送入模型，如果模型损失下降则更新特征子集，依次迭代进行。</p><hr><p><strong>输入</strong>：数据集<script type="math/tex">D</script>；</p><p>​            选择的特征集<script type="math/tex">A</script>；</p><p>​            某个学习器<script type="math/tex">\zeta</script>；</p><p>​            停止条件控制参数<script type="math/tex">T</script>【搜索次数】；</p><p><strong>过程</strong>：</p><ol><li><script type="math/tex">E=\infty</script>；</li><li><script type="math/tex">d=|A|</script>；</li><li><script type="math/tex">A^*=A</script>；</li><li><script type="math/tex">t=0</script>；</li><li><strong>while</strong> <script type="math/tex">\ t < T</script> <strong>do</strong></li><li>​    随机产生<strong>特征子集</strong><script type="math/tex">A'</script>；</li><li>​    <script type="math/tex">d'=|A'|</script>；</li><li>​    <script type="math/tex">E'=\mathrm{Cross Validation}(\zeta(D^{A'}))</script>【计算<strong>loss</strong>】；</li><li>​    <strong>if</strong> <script type="math/tex">(E'<E)\ or\ ((E'=E)\ and\ (d'<d))</script> 【<strong>loss</strong>降低，或者<strong>loss</strong>不变，特征数量减少】<strong>then</strong></li><li>​         <script type="math/tex">t=0</script>；【更新】</li><li>​        <script type="math/tex">E=E'</script>；</li><li>​        <script type="math/tex">d=d'</script>；</li><li>​        <script type="math/tex">A^*=A'</script>；</li><li>​    <strong>else</strong></li><li>​        <script type="math/tex">t=t+1</script></li><li>​    <strong>end if</strong></li><li><strong>end while</strong></li></ol><p><strong>输出</strong>：特征子集 <script type="math/tex">A^*</script></p><hr><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p251</p>]]></content>
    
    
    <summary type="html">介绍包裹式(Wrapper)特征选择的方法，以及典型的算法拉斯维加斯方法(LVW)</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Feature Engineering" scheme="https://1.15.86.100/tags/Feature-Engineering/"/>
    
    <category term="LVW" scheme="https://1.15.86.100/tags/LVW/"/>
    
  </entry>
  
  <entry>
    <title>Server</title>
    <link href="https://1.15.86.100/2021/04/02/Server/"/>
    <id>https://1.15.86.100/2021/04/02/Server/</id>
    <published>2021-04-02T09:29:32.000Z</published>
    <updated>2021-04-02T09:31:07.237Z</updated>
    
    <content type="html"><![CDATA[<h1 id="服务器管理"><a href="#服务器管理" class="headerlink" title="服务器管理"></a>服务器管理</h1><h3 id="查看IP登录情况"><a href="#查看IP登录情况" class="headerlink" title="查看IP登录情况"></a>查看IP登录情况</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> -n 指定登录次数前多少名</span></span><br><span class="line">awk '{print $1}' blog-log |sort |uniq -c|sort -nr|head -n 10000</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
    <summary type="html">服务器管理云云</summary>
    
    
    
    
    <category term="Server" scheme="https://1.15.86.100/tags/Server/"/>
    
  </entry>
  
  <entry>
    <title>Deep Forest</title>
    <link href="https://1.15.86.100/2021/04/02/Deep-Forest/"/>
    <id>https://1.15.86.100/2021/04/02/Deep-Forest/</id>
    <published>2021-04-02T08:56:45.000Z</published>
    <updated>2021-04-02T09:12:19.289Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Deep-Forest-christmas-tree"><a href="#Deep-Forest-christmas-tree" class="headerlink" title="Deep Forest:christmas_tree:"></a>Deep Forest<span class="github-emoji"><span>🎄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f384.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h1><h3 id="简介-ticket"><a href="#简介-ticket" class="headerlink" title="简介:ticket:"></a>简介<span class="github-emoji"><span>🎫</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ab.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><p>用<strong>深度神经网络(DNN)</strong>的思路来组织<strong>随机森林(RF)</strong>，极大地提高了随机森林的准确率。</p><h3 id="安装-wrench"><a href="#安装-wrench" class="headerlink" title="安装:wrench:"></a>安装<span class="github-emoji"><span>🔧</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install deep-forest</span><br></pre></td></tr></tbody></table></figure><h3 id="函数-funeral-urn"><a href="#函数-funeral-urn" class="headerlink" title="函数:funeral_urn:"></a>函数<span class="github-emoji"><span>⚱</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/26b1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><p>用法详见<strong>测试代码</strong></p><ul><li><p><strong>deepforest.CascadeForestClassifier</strong>：对<strong>Deep Forest</strong>的实现；</p></li><li><p><strong>deepforest.DecisionTreeClassifier</strong>：<strong>Deep Forest</strong>的树的实现；</p><p>​    </p></li></ul><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>分类的类别需从<strong>0</strong>开始标记，即<strong>label={0,1,2…}</strong></p><h3 id="原论文阅读-page-with-curl"><a href="#原论文阅读-page-with-curl" class="headerlink" title="原论文阅读:page_with_curl:"></a>原<a href="https://arxiv.org/pdf/1702.08835.pdf">论文</a>阅读<span class="github-emoji"><span>📃</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><h4 id="background"><a href="#background" class="headerlink" title="background"></a>background</h4><ul><li><p>对<strong>深度神经网络(DNN)</strong>成功的原因分析：</p><ol><li>一层一层的堆叠（<em>layer-by-layer processing</em>）；</li><li>模型内部的数据表征方式的转变【例如LSTM中<strong>词向量</strong>的传递和维度变化】（<em>in-model feature transformation</em>）；</li><li>足够的模型复杂度（<em>sufficient model complexity</em>）；</li></ol></li><li><p><strong>DNN</strong>的缺陷：</p><ol><li>超参数太多（too many hyper-parameters），模型表现十分依赖参数选择和训练（parameter tuning）；</li><li>需要大量训练数据（a huge amount of training data）；</li><li><strong>黑箱系统</strong>（玄学），很难进行理论分析（theoretical analysis）；</li><li>模型结构的确定先于模型训练；</li></ol></li></ul><h4 id="inspiration"><a href="#inspiration" class="headerlink" title="inspiration"></a>inspiration</h4><p><strong>从DNN</strong>：</p><ul><li><p>从<strong>DNN</strong>中观察到，在<strong>DNN</strong>每层的传播中，<strong>数据特征（feature）</strong>越来越集中，越来越抽象。</p></li><li><p><strong>DNN</strong>的成功和<strong>模型复杂度</strong>关系不大，否则<em>为什么无限扩大模型参数量并不能提升模型效果？</em>；</p></li><li><strong>DNN</strong>的层次性和<strong>决策树</strong>的层次性不一样：<ul><li><strong>决策树</strong>始终利用的是输入的数据，并没有对<strong>数据表征（feature）</strong>做出任何改变（work on the original feature representation），没有出现（<em>in-model feature transformation</em>）；</li><li><strong>DNN</strong>每一层的输出都会对<strong>数据表征（feature）</strong>做出改变；</li></ul></li></ul><p><strong>从集成学习（Ensemble Learning）</strong></p><ul><li><p>要做好集成学习，每个<strong>学习单元（learner）</strong>要做到<strong>准确（accurate）</strong> <strong>多样（diverse）</strong>；</p></li><li><p>实践中常常会通过技巧提高模型的<strong>多样性</strong>：</p><ul><li><p><strong>数据采样（data sample manipulation）</strong>：</p><p>  从原始数据集中采样出不同的<strong>子训练集</strong>来训练不同的<strong>学习单元（learner）</strong>；</p><p>  例如：</p><p>  <strong>Bagging</strong>中的<strong>bootstrap sampling</strong>；</p><p>  <strong>AdaBoost</strong>中的<strong>sequential importance sampling</strong>；</p></li><li><p><strong>输入特征采样（input feature manipulation）</strong>：</p><p>  从原始的数据特征中采样出不同的子特征（feature）生成<strong>子空间（subspace）</strong>，训练不同的<strong>学习单元（learner）</strong>；</p></li><li><p><strong>学习参数区别（learning parameter manipulation）</strong>：</p><p>  不同的<strong>学习单元（learner）</strong>采用不同的参数训练；</p></li><li><p><strong>输出表征区别（output representation manipulation）</strong>：</p><p>  对不同的学习单元使用不同的<strong>表征（representation）</strong>；</p></li></ul></li></ul><h4 id="gcForest"><a href="#gcForest" class="headerlink" title="gcForest"></a>gcForest</h4><p><strong>层次森林结构（Cascade Forest Structure）</strong></p><p><img src="/2021/04/02/Deep-Forest/p1.png" alt="pic"></p><ul><li><p>每一层从其前面的层获取数据，再将数据传递到下一层；</p></li><li><p>每一层都是<strong>随机森林</strong>的集成；</p></li><li><p>每个森林中的树个数作为超参数；</p></li><li><p>图中：</p><ul><li><p>黑色森林是<strong>随机森林（random forest）</strong>；</p><p>  每个森林包括500棵<strong>随机树</strong>，树的每个节点从随机选择的$\sqrt{d}$（d是特征个数）个<strong>候选特征</strong>中按照<strong>gini</strong>系数选择一个特征来切分；</p></li><li><p>蓝色森林是<strong>完全随机树森林（completely-random tree forest）</strong>；</p><p>  每个森林包括500棵<strong>完全随机树</strong>，树的每个节点会从<strong>所有的特征</strong>中选择一个特征切分出来，树生长直到完全是叶子；</p></li><li><p>假设数据分为三类，每个<strong>随机森林</strong>将输出<strong>三维向量</strong>，然后将所有向量连接（concatenate）作为输出；</p></li></ul></li><li><p>每个<strong>随机森林</strong>的输出是所有树的平均，如下图：<img src="/2021/04/02/Deep-Forest/p2.png" alt="pic"></p></li><li><p>为了减小<strong>过拟合（overfitting）</strong>风险，每个<strong>随机森林</strong>的输出都使用<strong>K折交叉验证（k-fold cross validation）</strong>：</p><ul><li>每个条数据会被训练<em>k-1</em>次，生成<em>k-1</em>个向量，然后平均作为该树的输出；</li><li>交叉验证的结果作为判定条件，如果模型效果相对上一层有提高则继续扩展下一层，否则结束；</li></ul></li></ul><p><strong>卷积特征提取（Multi-Grained Scanning）</strong></p><p><img src="/2021/04/02/Deep-Forest/p3.png" alt="pic"></p><ul><li><p>用一个一维或者二维的窗口扫描原数据，将窗口数据提取出来作为<strong>新特征</strong>；</p></li><li><p>将<strong>新特征</strong>送入训练，再将结果连接起来，作为最终的输出结果；</p></li><li><p>有可能某些<strong>新特征</strong>与结果丝毫没有关系（例如：需要识别一张图片的<em>汽车</em>，但提取出来的小片段不包含任何相关内容），这时，可以把<strong>新特征</strong>看成一种<strong>output representation manipulation</strong>，可以提高模型的多样性；</p></li><li><p>当<strong>新特征</strong>太多时，可以对其进行<strong>采样</strong>；</p></li><li><p>模型中通常使用不同大小的窗口进行特征提取，如下图：</p><p>  <img src="/2021/04/02/Deep-Forest/p4.png" alt="pic"></p></li></ul><p><strong>Code</strong>：</p><p><a href="https://github.com/baowj-678/Machine-Learning/tree/master/Deep-Forest">github:baowj-678</a></p><p><strong>REF</strong>：</p><p><a href="https://arxiv.org/pdf/1702.08835.pdf">arXiv:1702.08835 [cs.LG]</a></p>]]></content>
    
    
    <summary type="html">用深度神经网络(DNN)的思路来组织随机森(RF)，可以极大地提高了随机森林的准确率。</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Decision Tree" scheme="https://1.15.86.100/tags/Decision-Tree/"/>
    
    <category term="Random Forest" scheme="https://1.15.86.100/tags/Random-Forest/"/>
    
  </entry>
  
</feed>
