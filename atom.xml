<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BaoWJ&#39;s Blog</title>
  
  
  <link href="https://1.15.86.100/atom.xml" rel="self"/>
  
  <link href="https://1.15.86.100/"/>
  <updated>2021-03-31T08:29:34.058Z</updated>
  <id>https://1.15.86.100/</id>
  
  <author>
    <name>Bao Wenjie</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Feature Selection: Relief</title>
    <link href="https://1.15.86.100/2021/03/30/Feature-Selection-Relief/"/>
    <id>https://1.15.86.100/2021/03/30/Feature-Selection-Relief/</id>
    <published>2021-03-30T11:22:38.000Z</published>
    <updated>2021-03-31T08:29:34.058Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Relief"><a href="#Relief" class="headerlink" title="Relief"></a>Relief</h1><p><strong>Relief（Relevant Features）</strong>是一种著名的<strong>过滤式特征选择</strong>方法，所谓<strong>过滤式</strong>就是先对<strong>数据集</strong>进行<strong>特征选择</strong>，然后再利用选择的数据集训练学习器。</p><p><strong>Relief</strong>设置了一个“相关统计量”，来度量一个特征的重要性，</p><h2 id="Relief算法过程（二分类）"><a href="#Relief算法过程（二分类）" class="headerlink" title="Relief算法过程（二分类）"></a>Relief算法过程（二分类）</h2><p>给定训练集：</p><script type="math/tex; mode=display">\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m\}</script><p>对于每个实例$x_i$：</p><p><strong>Relief</strong>，在<script type="math/tex">x_i</script>的<strong>同类样本</strong>（即$y<em>i$相同）中寻找<strong>最近邻 </strong> $$x</em>{i,nh}$$，称为<strong>猜中近邻（near-hit）</strong>；</p><p>然后，在<script type="math/tex">x_i</script>的<strong>异类样本</strong>（即$y<em>i$不同）中寻找<strong>最近邻</strong> $$x</em>{i,nm}$$​，称为<strong>猜错近邻（near-miss）</strong>；</p><p>则，对于属性$j$，其<strong>相关统计量</strong>为：</p><script type="math/tex; mode=display">\delta^j=\sum_idiff(x_i^j,x_{i,nm}^j)^2-diff(x_i^j,x_{i,nh}^j)^2</script><p>其中：</p><p>$x_a^j$：样本实例$x_a$在属性$j$上的取值；</p><p>$diff(a,b)$：$a$，$b$之间的差值；</p><ul><li>若是<strong>连续数据</strong>：<script type="math/tex">diff(a,b)=|a-b|</script></li><li>若是<strong>离散数据</strong>：<script type="math/tex">diff(a,b)=0,if\ a=b</script>；<script type="math/tex">diff(a,b)=1,if\ a\ne b</script></li></ul><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><ul><li>如果<script type="math/tex">sum_idiff(x_i^j,x_{i,nm}^j)^2-diff(x_i^j,x_{i,nh}^j)^2>0</script>，即<script type="math/tex">x_i</script>，与其<strong>猜中近邻</strong>$x<em>{i,nh}$在属性$j$上的<strong>距离</strong>小于<strong>猜错近邻</strong>$x</em>{i,nm}$。则说明属性$j$对于区分类别是有益的，所以其<strong>相关统计量</strong>增大；</li><li>相反情况，其<strong>相关统计量</strong>减小；</li><li>最终，<strong>相关统计量</strong>值越大的属性，说明其作用越显著；</li></ul><h3 id="Relief-F算法（多分类）"><a href="#Relief-F算法（多分类）" class="headerlink" title="Relief-F算法（多分类）"></a>Relief-F算法（多分类）</h3><p>给定数据集：</p><ul><li><p>${(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m}$</p></li><li><p>其类别集：$y\in {1,2,\dots,|\mathcal{Y}|}$</p></li></ul><p>对于每个实例$x_i$：</p><p><strong>Relief-F</strong>，在<script type="math/tex">x_i</script>的<strong>同类样本</strong>（即<script type="math/tex">y_i</script>相同）中寻找<strong>最近邻</strong><script type="math/tex">x_{i,nh}</script>，称为<strong>猜中近邻</strong>；</p><p>然后，在<script type="math/tex">x_i</script>的每个不同类（共<script type="math/tex">|\mathcal{Y}|-1</script>个）寻找一个<strong>最近邻</strong><script type="math/tex">x_{i,l,nm},l\in\{1,2,\dots,|\mathcal{Y}|;l\ne k\}</script>，称为<strong>猜错近邻</strong>；</p><p>于是，改进的<strong>相关统计量</strong>为：</p><script type="math/tex; mode=display">\delta^j=\sum_i(\sum_{l\ne k}(p_l\times diff(x_i^j,x_{i,l,nm}^j)^2)-diff(x_i^j.x_{i,nh}^j)^2)</script><p>其中：</p><p>$p_l$：第$l$类样本在数据集$D$中所占的比例。</p><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p249</p>]]></content>
    
    
    <summary type="html">Relief是一种著名的过滤式特征选择的方法。</summary>
    
    
    
    
    <category term="Machine Learning,Feature Engineering" scheme="https://1.15.86.100/tags/Machine-Learning-Feature-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>Hexo ERROR:hexo-renderer-swig</title>
    <link href="https://1.15.86.100/2021/03/28/Hexo-ERROR-hexo-renderer-swig/"/>
    <id>https://1.15.86.100/2021/03/28/Hexo-ERROR-hexo-renderer-swig/</id>
    <published>2021-03-28T14:14:10.000Z</published>
    <updated>2021-03-28T14:19:14.200Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hexo-ERROR-hexo-renderer-swig"><a href="#Hexo-ERROR-hexo-renderer-swig" class="headerlink" title="Hexo ERROR: hexo-renderer-swig"></a>Hexo ERROR: hexo-renderer-swig</h1><h3 id="报错内容"><a href="#报错内容" class="headerlink" title="报错内容"></a>报错内容</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends &#x27;_layout.swig&#x27; %&#125; &#123;% import &#x27;_macro/post.swig&#x27; as post_template %&#125; &#123;% import &#x27;_macro/sidebar.swig&#x27; as sidebar_template %&#125; &#123;% block title %&#125;&#123;&#123; config.title &#125;&#125;&#123;% if theme.index_with_subtitle and config.subtitle %&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;% endif %&#125;&#123;% endblock %&#125; &#123;% block page_class %&#125; &#123;% if is_home() %&#125;page-home&#123;% endif -%&#125; &#123;% endblock %&#125; &#123;% block content %&#125;</span><br><span class="line">&#123;% for post in page.posts %&#125; &#123;&#123; post_template.render(post, true) &#125;&#125; &#123;% endfor %&#125;</span><br><span class="line">&#123;% include &#x27;_partials/pagination.swig&#x27; %&#125; &#123;% endblock %&#125; &#123;% block sidebar %&#125; &#123;&#123; sidebar_template.render(false) &#125;&#125; &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p><img src="/2021/03/28/Hexo-ERROR-hexo-renderer-swig/image-20210328221727193.png" alt="image-20210328221727193"></p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><strong>Hexo</strong>在5.0+中删除了<em>hexo-renderer-swig</em>，因此需要自己安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-swig</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">解决Hexo {% extends &#39;_layout.swig&#39; %}....{% endblock %} 报错</summary>
    
    
    
    
    <category term="Hexo" scheme="https://1.15.86.100/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>XLNet-PPT</title>
    <link href="https://1.15.86.100/2021/03/28/XLNet-PPT/"/>
    <id>https://1.15.86.100/2021/03/28/XLNet-PPT/</id>
    <published>2021-03-28T12:54:18.000Z</published>
    <updated>2021-03-28T13:05:35.819Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/03/28/XLNet-PPT/PPT1.PNG" alt="1"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%872.PNG" alt="2"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%873.PNG" alt="3"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%874.PNG" alt="4"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%875.PNG" alt="5"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%876.PNG" alt="6"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%877.PNG" alt="7"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%878.PNG" alt="8"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%879.PNG" alt="9"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8710.PNG" alt="10"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8711.PNG" alt="11"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8712.PNG" alt="12"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8713.PNG" alt="13"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8714.PNG" alt="14"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8715.PNG" alt="15"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8716.PNG" alt="16"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8717.PNG" alt="17"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8718.PNG" alt="18"></p>]]></content>
    
    
    <summary type="html">介绍XLNet的PPT</summary>
    
    
    
    
    <category term="NLP" scheme="https://1.15.86.100/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>XLNet</title>
    <link href="https://1.15.86.100/2021/03/28/XLNet/"/>
    <id>https://1.15.86.100/2021/03/28/XLNet/</id>
    <published>2021-03-28T12:44:55.000Z</published>
    <updated>2021-03-28T14:55:56.901Z</updated>
    
    <content type="html"><![CDATA[<h1 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h1><p><a href="https://arxiv.org/pdf/1906.08237.pdf">XLNET</a></p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="AR-AutoRegressive"><a href="#AR-AutoRegressive" class="headerlink" title="AR(AutoRegressive)"></a>AR(AutoRegressive)</h3><p>用模型估计一个文本库的<strong>概率分布</strong></p><p>给定<strong>文本序列</strong>：</p><p>$ \bold{x}=[x_1,x_2,\dots,x_T] $</p><p><strong>AR</strong>模型会将<strong>似然</strong>分解成<strong>前向的乘积(forward product)</strong>：</p><p>$p(\bold{x})=\prod<em>{t=1}^Tp(x_t|\bold{x}</em>{&lt;t})$</p><p>或者<strong>后向乘积(backward product)</strong>：</p><p>$p(\bold{x})=\prod<em>{t=T}^1p(x_t|\bold{x}</em>{&gt;t})$</p><p>而其中每个的<strong>概率分布</strong>可以用带参数的模型(神经网络等)来建模。</p><p><strong>AR</strong>通过最大化<strong>对数似然估计</strong>进行预训练：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \mathrm{log}\ p_{\theta}(\bold{x})=\sum_{t=1}^T\mathrm{log}\ p_{\theta}(x_t|\bold{x}_{<t})=\sum_{t=1}^T\mathrm{log}\frac{\mathrm{exp}(h_{\theta}(\bold{x}_{1:t-1})^Te(x_t))}{\sum_{x'}\mathrm{exp}(h_{\theta}(\bold{x}_{1:t-1})^Te(x'))}</script><p><strong>其中</strong></p><p>$h<em>{\theta}(\bold{x}</em>{1:t-1})$：是用<em>神经网络模型</em>计算的文本的表征，可以通过<strong>RNN</strong>，<strong>Transformer</strong>等计算；</p><p>$e(x)$：是$x$的<strong>embedding</strong>；</p><p><strong>缺陷</strong>：<strong>AR</strong>模型只能<strong>单向(uni-directional)建模</strong></p><h3 id="AE-AutoEncoding"><a href="#AE-AutoEncoding" class="headerlink" title="AE(AutoEncoding)"></a>AE(AutoEncoding)</h3><p><strong>AE（自编码器）</strong>模型不会建立<strong>清晰的概率密度估计(explicit density estimation)</strong>，而是对原始数据进行<strong>重构(reconstruct)</strong>，例如：<strong>BERT</strong></p><p><strong>BERT</strong>：基于<strong>降噪自编码器（denoising auto-encoding）</strong>，给定输入，每次<strong>15%</strong>的<strong>字词(token)</strong>会被替换成特殊的标记<strong>[MASK]</strong>，然后模型会被训练去从输入的<strong>不全(corrupted)的数据</strong>中恢复原始的字词。因为<strong>BERT</strong>不用做<strong>密度估计(density estimation)</strong>，所以可以双向建模。</p><p>最优化训练公式如下：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \mathrm{log}\ p_{\theta}(\bar{\bold{x}}|\hat{\bold{x}})\approx \sum_{t=1}^Tm_t\mathrm{log}\ p_{\theta}(x_t|\hat{\bold{x}})=\sum_{t=1}^Tm_t\mathrm{log}\frac{\mathrm{exp}(H_\theta(\hat{\bold{x}})_t^Te(x_t))}{\sum_{x'}\mathrm{exp}(H_\theta(\hat{\bold{x}})_t^Te(x'))}</script><p>其中：</p><p>$\hat{\bold{x}}$：<strong>mask</strong>后的文本；</p><p>$\bar{\bold{x}}$：被<strong>mask</strong>的文本；</p><p>$m_t$：<strong>指示变量</strong>，当$x_t$是被<strong>mask</strong>的时，$m_t=1$，其他情况下，$m_t=0$；</p><p>$H_\theta$：Transformer模型；</p><p><strong>缺陷</strong>：</p><ul><li><strong>BERT</strong>预训练使用的标记<strong>[MASK]</strong>不会出现在真正数据中，会导致<strong>预训练-微调差异(pretrain-finetune discrepancy)</strong></li><li>输入中不包括待预测字词（被mask掉），<strong>BERT</strong>模型不能像<strong>AR</strong>中一样用乘法规则建模<strong>联合概率(joint probability)</strong></li><li><strong>BERT</strong>假设待预测的字词即$\bar{x}$<strong>互相独立</strong></li></ul><p><strong>优点</strong>：</p><ul><li><strong>BERT</strong>每个位置的字词可以注意到双向的上下文所有的文本；</li></ul><h2 id="XLNet-1"><a href="#XLNet-1" class="headerlink" title="XLNet"></a>XLNet</h2><ul><li><strong>XLNet</strong>最大化基于所有可能的<strong>分解顺序的排列(permutation of the factorization order)</strong>的<strong>极大对数似然估计</strong>，由于这个原因每个位置的字词可以学习左右的文本；</li><li><strong>XLNet</strong>预训练不会通过mask部分字词的方法，因此避免<strong>预训练-微调差异</strong>；</li><li><strong>XLNet</strong>集成了<strong>段RNN机制(segment recurrence)</strong>和<strong>Transformer-XL</strong>；</li></ul><h3 id="Permutation-Language-Modeling"><a href="#Permutation-Language-Modeling" class="headerlink" title="Permutation Language Modeling"></a>Permutation Language Modeling</h3><p>一般来说，对于一个长度为$T$的序列$\bold{x}$，<strong>AE</strong>的<strong>因子分解</strong>存在$T!$个不同的排列顺序。直觉上，如果模型参数能够从所有的排列中学得，那么这个模型就可以从双向的所有位置获得信息。</p><p>根据这个想法，得出以下优化公式：</p><script type="math/tex; mode=display">\mathrm{max}_\theta\ \ \ \mathbb{E}_{z\sim\mathcal{Z}_T}[\sum_{t=1}^T\mathrm{log}\ p_\theta(x_{z_t}|\bold{x}_{\bold{z}_{<t}})]</script><p>其中：</p><p>$\mathcal{Z}_T$：长度为$T$的序列的所有排列方式；</p><p><img src="/2021/03/28/XLNet/image-20210322211204908.png" alt="image-20210322211204908"></p><p>实践中，对于一个文本序列$\bold{x}$，我们<strong>采样</strong>出一个排列顺序，然后按照这个顺序将$p_\theta(\bold{x})$的似然估计进行<strong>因子分解</strong>，因为$\theta$参数是共享的，所以理论上$x_i$会”看见“每个$x_j,x_i\neq x_j$。</p><p><strong>排列采样</strong>在<strong>Transformer</strong>的实践中通过<strong>Attention</strong>的<strong>mask</strong>矩阵实现。</p><p>这样就避免了<strong>独立性假设</strong>和<strong>预训练-微调差异</strong>。</p><h4 id="Remark-on-Permutation"><a href="#Remark-on-Permutation" class="headerlink" title="Remark on Permutation"></a>Remark on Permutation</h4><p>上面的<strong>排列采样</strong>仅针对<strong>因子分解</strong>计算，不是序列本身的顺序。同时为了保留序列原本顺序的信息，这里使用基于原本顺序的<strong>位置编码</strong>。</p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><h4 id="Target-Aware-Representations"><a href="#Target-Aware-Representations" class="headerlink" title="Target-Aware Representations"></a>Target-Aware Representations</h4><p><strong>模型公式</strong>：</p><script type="math/tex; mode=display">p_\theta(\bold{X}_{z_t}=x|\bold{x}_{\bold{z}_{<t}})=\frac{\mathrm{exp}(e(x)^Th_\theta(\bold{x}_{\bold{z}_{<t}}))}{\sum_{x'}\mathrm{exp}(e(x')^Tg_\theta(\bold{x}_{\bold{z}_{<t}}))}</script><p>其中$\bold{x}<em>{\bold{z}</em>{&lt;t}}$不依赖其即将预测的<strong>位置信息</strong>，$\bold{z}$是按照<strong>因子分解顺序</strong>排列的。因此这里需要加入位置信息；所以将公式修改为：</p><script type="math/tex; mode=display">p_\theta(\bold{X}_{z_t}=x|\bold{x}_{\bold{z}_{<t}})=\frac{\mathrm{exp}(e(x)^Tg_\theta(\bold{x}_{\bold{z}_{<t}},z_t))}{\sum_{x'}\mathrm{exp}(e(x')^Tg_\theta(\bold{x}_{\bold{z}_{<t}},z_t))}</script><p>其中：</p><p>$g<em>\theta(\bold{x}</em>{\bold{z}_{&lt;t}}, z_t)$：表示将预测目标的位置编码$z_t$加入的新的表征。</p><p>这可以看成<strong>站在</strong>目标位置$z<em>t$，然后依靠位置信息去收集文本信息$\bold{x}</em>{\bold{z}_{&lt;t}}$。</p><h4 id="Two-Stream-Self-Attention"><a href="#Two-Stream-Self-Attention" class="headerlink" title="Two-Stream Self-Attention"></a>Two-Stream Self-Attention</h4><p>使用<strong>Target-Aware Representation</strong>会出现两个问题：</p><ul><li>为了预测$z<em>t$位置的字词$x</em>{z<em>{t}}$，$g</em>\theta(\bold{x}<em>{\bold{z}</em>{&lt;t}},z<em>t)$不能使用$x</em>{z_{t}}$的信息；</li><li>为了预测其他位置的字词$x<em>{z_t},j&gt;t$，$g</em>\theta(\bold{x}<em>{\bold{z}</em>{&lt;t}},z<em>t)$需要加入$x</em>{z_{t}}$的信息。</li></ul><p>为了解决上面问题，这里提出了<strong>Two-Stream Self-Attention</strong>方法</p><p><img src="/2021/03/28/XLNet/image-20210325230153410.png" alt="image-20210325230153410"></p><ul><li><strong>content representation</strong>$h<em>\theta(\bold{x}</em>{\bold{z}<em>{\le t}})$，这个和<strong>Transformer</strong>中的类似，它保留了上游文本信息和$x</em>{z_t}$的信息；</li><li><strong>query representation</strong>$g<em>\theta(\bold{x}</em>{\bold{z}<em>t},z_t)$，这个只保留了$\bold{z}</em>{&lt;t}$的信息。</li></ul><p>在计算上：</p><p>第一层的<strong>query stream</strong>被初始化为可训练的向量，$g_i^{(0)}=w$；</p><p>第一层的<strong>content stream</strong>被初始化为对应的<strong>词向量</strong>，$h_i^{(0)}=e(x_i)$；</p><p>对于每个<strong>self-attention layer</strong>，其更新方式为：</p><script type="math/tex; mode=display">g_{z_t}^{(m)}=\mathrm{Attention}(Q=g_{z_t}^{(m-1)},KV=h_{\bold{z}_{<t}}^{(m-1)};\theta)</script><script type="math/tex; mode=display">h_{z_t}^{(m)}=\mathrm{Attention}(Q=h_{z_t}^{(m-1)},KV=h_{\bold{z}_{\le t}}^{(m-1)};\theta)</script><p><strong>content representation</strong>和标准的<strong>self-attention</strong>一样，所以在<strong>微调</strong>的时候我们会丢掉<strong>query stream</strong>，只使用<strong>content stream</strong>作为普通的<strong>Transformer</strong>看，同时我们将$g_{z_t}^{(M)}$作为最终的预测结果。</p><h4 id="Partial-Prediction"><a href="#Partial-Prediction" class="headerlink" title="Partial Prediction"></a>Partial Prediction</h4><p>利用语句排列来训练模型存在<strong>难收敛(slow convergence)</strong>的问题，为了解决这个问题，将$\bold{z}$分为$\bold{z}<em>{\le c}$和$\bold{z}</em>{&gt;c}$，然后最大化以下概率：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \ \mathbb{E}_{\bold{z}\sim \mathcal{Z}_T}[\mathrm{log}p_{\theta}(\bold{x}_{\bold{z}_{>c}}|\bold{x}_{\bold{z}_{\le c}})]=\mathbb{E}_{\bold{z}\sim \mathcal{Z}_T}[\sum_{t=c+1}^{\bold{z}}\mathrm{log}p_{\theta}(x_{z_t}|\bold{x}_{\bold{z}_{<t}})]</script><p>预测$\bold{z}_{&gt;c}$的原因是因为，能够传递足够的信息。一般会设置一个超参数$K$，$1/K$的字词被选择用做预测。</p><h3 id="Incorporating-Ideas-from-Transformer-XL"><a href="#Incorporating-Ideas-from-Transformer-XL" class="headerlink" title="Incorporating Ideas from Transformer-XL"></a>Incorporating Ideas from Transformer-XL</h3><p>这里使用了<strong>Transformer-XL</strong>的两个技巧：<strong>相对位置编码（relative positional encoding scheme）</strong>和<strong>段循环机制（segment recurrence mechanism）</strong></p><ul><li><p><strong>相对位置编码</strong>：即前文的$g<em>\theta(\bold{x}</em>{\bold{z}_t},z_t)$</p></li><li><p><strong>段循环机制</strong>：</p><p>  <img src="/2021/03/28/XLNet/image-20210328204303326.png" alt="image-20210328204303326"></p><p>  假设有长文本中的两段文本，长文本为：$\bold{s}$，两端文本为：$\tilde{\bold{x}}=\bold{s}<em>{1:T},\bold{x}=\bold{s}</em>{T+1:2T}$，两端文本对应的采样的排列顺序为$\tilde{\bold{z}}=[1\dots T],\bold{z}=[T+1\dots 2T]$</p><p>  于是基于$\tilde{\bold{z}}$，先处理第一段，获得<strong>文本表征</strong>$\tilde{\bold{h}}^{(m)}$($m$代表层数)，然后对于下一段，计算公式为：</p><script type="math/tex; mode=display">h_{z_t}^{(m)}\leftarrow \mathrm{Attention}(Q=h_{z_t}^{(m-1)},KV=[\tilde{\bold{h}}^{(m-1)},\bold{h}_{\bold{z}_{\le t}}^{(m-1)}];\theta)</script><p>  $[\dots]$：表示将矩阵按照<strong>序列长度的维度</strong>拼接起来。</p></li></ul><h3 id="Modeling-Multiple-Segments"><a href="#Modeling-Multiple-Segments" class="headerlink" title="Modeling Multiple Segments"></a>Modeling Multiple Segments</h3><p>预训练时，对于多段文本，<strong>XLNet</strong>随机采样两段文本（可能从同一上下文，也可能从不同上下文）然后将其拼接成一段文本（方法和<strong>BERT</strong>一样：<strong>[A, SEP, B, SEP, CLS]</strong>）</p><h4 id="Relative-Segment-Encoding"><a href="#Relative-Segment-Encoding" class="headerlink" title="Relative Segment Encoding"></a>Relative Segment Encoding</h4><p>给定两个位置$i,j$，如果这两个位置是来自同一段，则用编码$s<em>{i,j}=s</em>{+}$，否则用编码$s<em>{i,j}=s</em>{-}$。这两个编码都是学得的参数。</p><p>当$i$查询$j$时，段位置编码$s<em>{i,j}$会被用于计算<strong>注意权重（attention weight）</strong>$a</em>{i,j}=(\bold{q}<em>i+\bold{b})^T\bold{s}</em>{i,j}$。</p><p>$\bold{q}_i$是查询向量，$\bold{b}$是待学习的依赖于头的偏差向量<strong>（learnable head-specific bias vector）</strong>。最后该权重会被加到最终的<strong>注意权重</strong>上。</p><p>该方法有两个作用：</p><ul><li>提高泛化能力；</li><li>让模型能够适应多段文本任务，因为多段文本不能用绝对位置编码；</li></ul>]]></content>
    
    
    <summary type="html">Google Brain重磅出品，集自回归语言模型和自编码语言模型之大成，又灵活融入Transformer-XL长文本建模能力！</summary>
    
    
    
    
    <category term="NLP" scheme="https://1.15.86.100/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>SQL-install</title>
    <link href="https://1.15.86.100/2020/10/29/SQL-install/"/>
    <id>https://1.15.86.100/2020/10/29/SQL-install/</id>
    <published>2020-10-29T11:49:01.000Z</published>
    <updated>2021-03-27T14:40:20.193Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h1><h3 id="下载："><a href="#下载：" class="headerlink" title="下载："></a>下载：</h3><ul><li><p>在<a href="https://dev.mysql.com/downloads/mysql/">MySQL官网</a>下载<strong>zip</strong>文件；</p></li><li><p>本地直接解压；</p></li></ul><h3 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h3><ul><li>配置<strong>环境变量</strong>：添加解压文件的<strong>bin</strong>文件夹路径至<strong>PATH</strong>；</li><li>初始化，生成DATA文件：<em>mysqld –initialize-insecure –user=mysql</em></li><li>网络连接：<em>mysqld -install</em></li><li>启动服务：<em>net start mysql</em>【在bin目录下】</li></ul><h3 id="登录："><a href="#登录：" class="headerlink" title="登录："></a>登录：</h3><ul><li><p>登录【不用密码】：<em>mysql -u root -p</em></p></li><li><p>查询用户密码：<em>select host,user,authentication_string from mysql.user;</em></p></li><li><p>设置<strong>root</strong>用户密码：<strong>ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘123456’;</strong></p></li><li><p>保存修改：<em>flush privileges;</em></p></li></ul><h3 id="基本操作："><a href="#基本操作：" class="headerlink" title="基本操作："></a>基本操作：</h3><ul><li><strong>退出</strong>：<em>quit</em></li><li></li></ul>]]></content>
    
    
    <summary type="html">SQL</summary>
    
    
    
    
    <category term="SQL" scheme="https://1.15.86.100/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>LDA(Fisher)</title>
    <link href="https://1.15.86.100/2020/07/07/LDA-Fisher/"/>
    <id>https://1.15.86.100/2020/07/07/LDA-Fisher/</id>
    <published>2020-07-07T07:58:22.000Z</published>
    <updated>2021-03-27T14:39:47.317Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LDA-Fisher"><a href="#LDA-Fisher" class="headerlink" title="LDA(Fisher)"></a>LDA(Fisher)</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>LDA(Linear Discriminant Analysis)<strong>，又称</strong>Fisher判别方法</strong>。是一种经典的<strong>线性判别方法</strong>。该方法主要思想是：将样例投影到一维直线上，使得<strong>同类样例</strong>的投影点尽可能<strong>接近</strong>和<strong>密集</strong>；<strong>异类</strong>投影点尽可能<strong>远离</strong>。</p><p><img src="/2020/07/07/LDA-Fisher/.%5CLDA-Fisher%5Cfisher.png" alt="fisher"></p><h3 id="计算推导"><a href="#计算推导" class="headerlink" title="计算推导"></a>计算推导</h3><p>假设已知样本 $C_1$ 和 $C_2$ ，$|C_1|、|C_2|$ 分别两类样本数据的总数。</p><p>则两类样例的<strong>类中心</strong>分别为：</p><p>$$\mu_1=\frac{1}{|C_1|}\sum_{x\in{C_1}}x$$</p><p>$$\mu_2=\frac{1}{|C_2|}\sum_{x\in{C_2}}x$$</p><p>假设最佳的投影方向为 $w$ 则，样本点 $x$ 投影到 $w$ 上的点的坐标为：$y=w^Tx$</p><p>所以，投影后的<strong>类中心</strong>为：</p><p>$$m_k=\frac{1}{|C_k|}\sum_{x\in{C_k}}w^Tx=w^T\frac{1}{|C_k|}\sum_{x\in{C_k}}x=w^T\mu_k$$</p><h4 id="类间距离"><a href="#类间距离" class="headerlink" title="类间距离"></a>类间距离</h4><p><strong>类中心</strong>的<strong>间距</strong>为：</p><p>$$d_{(1, 2)}=(m_1-m_2)^2=(m_1-m_2)(m_1-m_2)^T=w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw=w^TS_bw$$</p><p>其中，$S_b$ 为<strong>类间散度矩阵</strong>：</p><p>$$S_b=(\mu_1-\mu_2)(\mu_1-\mu_2)^T$$</p><h4 id="类内距离"><a href="#类内距离" class="headerlink" title="类内距离"></a>类内距离</h4><p><strong>类内距离</strong>用类内样本的方差来衡量，对于第 $k$ 个类，方差为：</p><p>$$\begin{split}S_k^2=\sum_{x\in{C_k}}(y-m_k)^2=\sum_{x\in{C_k}}(w^T(x-\mu_k))^2\=\sum_{x\in{C_k}}(w^T(x-\mu_k))((x-\mu_k)^Tw)\=w^T[\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T]w \end{split}$$</p><p>所有类别<strong>类内距离</strong>之和为：</p><p>$$\sum_{k\in n}S_k^2=w^T[\sum_{k\in n}\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T]w$$</p><p><strong>类内散度矩阵</strong>为：</p><p>$$S_w=\sum\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T$$</p><h4 id="最优化"><a href="#最优化" class="headerlink" title="最优化"></a>最优化</h4><p>我们的优化目的是增加<strong>类间距离</strong>，减小<strong>类内距离</strong>，所有可以最大化函数：</p><p>$$J(w)=\frac{(m_1-m_2)^2}{S_1^2+S_2^2}=\frac{w^TS_bw}{w^TS_ww}$$</p>]]></content>
    
    
    <summary type="html">LDA</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PowerShell</title>
    <link href="https://1.15.86.100/2020/03/08/PowerShell/"/>
    <id>https://1.15.86.100/2020/03/08/PowerShell/</id>
    <published>2020-03-08T02:40:06.000Z</published>
    <updated>2020-04-05T08:56:23.854Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PowerShell"><a href="#PowerShell" class="headerlink" title="PowerShell"></a>PowerShell</h1><h2 id="basic-command-lines"><a href="#basic-command-lines" class="headerlink" title="basic command lines"></a>basic command lines</h2><ol><li><p>get information about the power-shell (version…)</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$psversiontable</span></span><br></pre></td></tr></table></figure></li><li><p>enter ps through CMD</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;powershell</span><br></pre></td></tr></table></figure></li><li><p>basic math operation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1024</span>*<span class="number">1024</span></span><br></pre></td></tr></table></figure></li><li><p>get service information</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-service</span></span><br></pre></td></tr></table></figure></li><li><p>print environment variables</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$env:path</span></span><br></pre></td></tr></table></figure></li><li><p>get all of the commands</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-command</span></span><br><span class="line"><span class="built_in">gcm</span></span><br></pre></td></tr></table></figure></li><li><p>help</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-help</span> [<span class="type">command</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>history commands</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-history</span></span><br></pre></td></tr></table></figure></li><li><p>find the real name of a short name</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-alias</span> <span class="literal">-name</span> [<span class="type">short</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>set alias</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set-alias</span> <span class="literal">-name</span> [<span class="built_in">new-name</span>] <span class="literal">-value</span> [<span class="type">old</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>input</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$input</span>=<span class="built_in">read-host</span> <span class="string">&quot;please input&quot;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Shortcut-Key"><a href="#Shortcut-Key" class="headerlink" title="Shortcut Key"></a>Shortcut Key</h2><ol><li><p>cancel the progress running</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ctrl+C</span><br></pre></td></tr></table></figure></li></ol><h2 id="Special-methods"><a href="#Special-methods" class="headerlink" title="Special methods"></a>Special methods</h2><h4 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a | b</span><br></pre></td></tr></table></figure><h4 id="redirection"><a href="#redirection" class="headerlink" title="redirection"></a>redirection</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="comment">#append</span></span><br><span class="line">&gt;&gt;<span class="comment">#overwrite</span></span><br></pre></td></tr></table></figure><h4 id="format"><a href="#format" class="headerlink" title="format"></a>format</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;...&#123;0&#125;...&#123;1&#125;...&quot;</span> <span class="operator">-f</span> <span class="variable">$first</span>, <span class="variable">$second</span>, ...</span><br></pre></td></tr></table></figure><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><ol><li><p>definition</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span>[<span class="type">variable</span>-<span class="type">name</span>]=[<span class="type">variable</span>-<span class="type">value</span>]<span class="comment">#$a equals $A</span></span><br></pre></td></tr></table></figure></li><li><p>check the variables</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> variables:</span><br></pre></td></tr></table></figure></li><li><p>array</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$arr</span>=<span class="selector-tag">@</span>()<span class="comment">#empty array</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>..<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>, <span class="string">&#x27;hello&#x27;</span>, <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>.count<span class="comment">#return array&#x27;s numbers</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>+=[<span class="built_in">new-element</span>]</span><br></pre></td></tr></table></figure></li><li><p>string</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$str</span>.split(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.endswith(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.contains(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.compareto(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.indexof(<span class="string">&quot;[char]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.insert(position<span class="literal">-num</span>, <span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.replace(<span class="string">&quot;old-char&quot;</span>, <span class="string">&quot;new-char&quot;</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="Operation"><a href="#Operation" class="headerlink" title="Operation"></a>Operation</h2><ol><li><p>comparation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-eq</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#equal?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-ne</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#not equal?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-gt</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#greater?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-lt</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#less?</span></span><br></pre></td></tr></table></figure></li><li><p>bool operation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">-not</span> [<span class="type">variable</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-and</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-or</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-xor</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br></pre></td></tr></table></figure></li></ol><h2 id="Basic-Grammar"><a href="#Basic-Grammar" class="headerlink" title="Basic Grammar"></a>Basic Grammar</h2><ol><li><p>if</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(conditon)</span><br><span class="line">&#123;expr1&#125;</span><br><span class="line"><span class="keyword">elseif</span></span><br><span class="line">&#123;expr2&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;expr3&#125;</span><br></pre></td></tr></table></figure></li><li><p>switch</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(key<span class="literal">-variable</span>)</span><br><span class="line">&#123;</span><br><span class="line">&#123;<span class="variable">$_</span> condition1&#125;&#123;expr1&#125;</span><br><span class="line">&#123;<span class="variable">$_</span> condition2&#125;&#123;expr2&#125;</span><br><span class="line">default&#123;expr3&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>foreach</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">foreach</span>(<span class="variable">$each</span> <span class="keyword">in</span> [<span class="built_in">array</span>-<span class="type">variable</span>])</span><br><span class="line">&#123;</span><br><span class="line">expr</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>while</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(condition)</span><br><span class="line">&#123;</span><br><span class="line">expr</span><br><span class="line">[<span class="type">break</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>for</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(expr1; condition; expr2)</span><br><span class="line">&#123;</span><br><span class="line">expr3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h2><ol><li><p>definition</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">fun-name</span><span class="params">([parameter])</span></span></span><br><span class="line">&#123;</span><br><span class="line">expression</span><br><span class="line"><span class="keyword">return</span> [<span class="type">return</span>-<span class="type">value</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>using</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">variable1</span>] [<span class="title">variable2</span>] ...</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    <summary type="html">The basic methods of powershell</summary>
    
    
    
    
    <category term="powershell" scheme="https://1.15.86.100/tags/powershell/"/>
    
  </entry>
  
  <entry>
    <title>HMM-algorithm</title>
    <link href="https://1.15.86.100/2020/01/06/HMM-algorithm/"/>
    <id>https://1.15.86.100/2020/01/06/HMM-algorithm/</id>
    <published>2020-01-06T03:04:29.000Z</published>
    <updated>2020-01-06T03:11:33.528Z</updated>
    
    <content type="html"><![CDATA[<h1 id="隐马尔可夫模型-HMM"><a href="#隐马尔可夫模型-HMM" class="headerlink" title="隐马尔可夫模型(HMM)"></a>隐马尔可夫模型(HMM)</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>隐马尔可夫模型(Hidden-Markov-Model)<strong>是一种</strong>概率图</strong>模型，在<strong>深度学习</strong>出现之前，该模型被广泛应用于<em>语音识别，文本标注</em>等方面。</p><p>隐马尔可夫模型是关于时序的概率图模型。</p>]]></content>
    
    
    <summary type="html">An introduction to the Hidden Markov Algorithm</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Linux</title>
    <link href="https://1.15.86.100/2019/12/17/Linux/"/>
    <id>https://1.15.86.100/2019/12/17/Linux/</id>
    <published>2019-12-17T03:40:07.000Z</published>
    <updated>2020-07-04T14:08:44.421Z</updated>
    
    <content type="html"><![CDATA[<h3 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取当前目录下文件</span></span><br><span class="line">ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入子文件夹</span></span><br><span class="line"><span class="built_in">cd</span> [folder name]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 返回上一目录</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回上两级目录</span></span><br><span class="line"><span class="built_in">cd</span> ../..</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回home目录</span></span><br><span class="line"><span class="built_in">cd</span>/<span class="built_in">cd</span> ~ </span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回指定目录</span></span><br><span class="line"><span class="built_in">cd</span> - [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文件内容</span></span><br><span class="line">cat [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件头</span></span><br><span class="line">head [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件尾</span></span><br><span class="line">tail [file name]</span><br><span class="line">tail -f [file name] <span class="comment">#实时显示文件尾，跟随日志变化</span></span><br><span class="line"><span class="comment"># 修改/编写文件内容</span></span><br><span class="line">vim [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改文件名</span></span><br><span class="line">mv [old file name] [new file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建文件</span></span><br><span class="line">touch [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建文件夹</span></span><br><span class="line">mkdir [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件</span></span><br><span class="line">rm [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除目录下所有文件</span></span><br><span class="line">rm *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归删除文件夹</span></span><br><span class="line">rm -r [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件</span></span><br><span class="line">cp [old file name] [new file name]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示文件夹内文件大小</span></span><br><span class="line">ls -lh</span><br></pre></td></tr></table></figure><h3 id="Vim-操作"><a href="#Vim-操作" class="headerlink" title="Vim 操作"></a>Vim 操作</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入插入模式</span></span><br><span class="line">按i键</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出插入模式</span></span><br><span class="line">按Eac键</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出Vim</span></span><br><span class="line">:wq + Enter</span><br></pre></td></tr></table></figure><h3 id="进程操作"><a href="#进程操作" class="headerlink" title="进程操作"></a>进程操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让进程在后台运行</span></span><br><span class="line">nuhup [<span class="built_in">command</span>] $</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有进程记录</span></span><br><span class="line">ps -aux</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死进程</span></span><br><span class="line"><span class="built_in">kill</span> -9 [进程id]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示挂起进程</span></span><br><span class="line"><span class="built_in">jobs</span> -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死当前bash内运行的进程</span></span><br><span class="line">Ctrl+c</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂起当前bash内运行的进程</span></span><br><span class="line">Ctrl+z</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行刚刚挂起的进程</span></span><br><span class="line">Ctrl+y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据pid查看进程</span></span><br><span class="line">ps -ef|grep [pid]</span><br></pre></td></tr></table></figure><h3 id="查看历史命令"><a href="#查看历史命令" class="headerlink" title="查看历史命令"></a>查看历史命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">history</span>[num]</span><br></pre></td></tr></table></figure><h3 id="查看显卡利用情况"><a href="#查看显卡利用情况" class="headerlink" title="查看显卡利用情况"></a>查看显卡利用情况</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><h3 id="查看文件大小"><a href="#查看文件大小" class="headerlink" title="查看文件大小"></a>查看文件大小</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">du -ah</span><br></pre></td></tr></table></figure><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实时查看GPU显存利用率，需要用pip按照gpustat</span></span><br><span class="line">watch --color -n1 gpustat -cpu</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看显存利用率</span></span><br><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Basic CommandLine for Linux</summary>
    
    
    
    
    <category term="Linux" scheme="https://1.15.86.100/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch</title>
    <link href="https://1.15.86.100/2019/12/15/Pytorch/"/>
    <id>https://1.15.86.100/2019/12/15/Pytorch/</id>
    <published>2019-12-15T04:43:52.000Z</published>
    <updated>2020-01-04T02:07:01.828Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><h3 id="background"><a href="#background" class="headerlink" title="background"></a>background</h3><p>Torch是一个使用Lua语言的神经网络</p><h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h3 id="import"><a href="#import" class="headerlink" title="import"></a>import</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="data-type"><a href="#data-type" class="headerlink" title="data type"></a>data type</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create empty tensor</span></span><br><span class="line">torch.empty(a, b, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create rand tensor</span></span><br><span class="line">torch.rand(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create zero tensor</span></span><br><span class="line">torch.zeros(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create ones tensor</span></span><br><span class="line">torch.ones(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create from list</span></span><br><span class="line">torch.tensor(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create rand tensor according a tensor</span></span><br><span class="line">torch.rand_like(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the size of a tensor</span></span><br><span class="line">x.size() <span class="comment"># return a tuple</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># resize the tensor</span></span><br><span class="line">x.view(-<span class="number">1</span>, b)</span><br></pre></td></tr></table></figure><h3 id="tansformation"><a href="#tansformation" class="headerlink" title="tansformation"></a>tansformation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy to tensor</span></span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line"><span class="comment"># the ndarray of numpy and the tensor of torch share the same storage space</span></span><br><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor to numpy</span></span><br><span class="line">np_data = torch_data.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># int to tensor</span></span><br><span class="line">torch_data = torch.IntTensor(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># float to tensor</span></span><br><span class="line">torch_data = torch.FloatTensor(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor to python(only one element in torch_data)</span></span><br><span class="line">data = torch_data.item()</span><br></pre></td></tr></table></figure><h3 id="basic-math-method"><a href="#basic-math-method" class="headerlink" title="basic math method"></a>basic math method</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matrix multiplication</span></span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)</span><br><span class="line">ans = torch.mm(tensor, tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate mean</span></span><br><span class="line">mean = torch.mean(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add</span></span><br><span class="line">torch.add(a, b)</span><br><span class="line"><span class="built_in">print</span>(a + b)</span><br><span class="line">a.add_(b) <span class="comment"># change a</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="comment"># any func that can change the tensor has a &#x27;_&#x27; in its name</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="activation-function"><a href="#activation-function" class="headerlink" title="activation function"></a>activation function</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># relu</span></span><br><span class="line">torch.relu(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid</span></span><br><span class="line">torch.sigmoid(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tanh</span></span><br><span class="line">torch.tanh(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax</span></span><br><span class="line">torch.softmax(tensor)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># regression</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self</span>):</span></span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    net = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(net)</span><br><span class="line">    </span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.5</span>)</span><br><span class="line">    loss_func = torch.nn.MSELoss()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        prediction = net(x)</span><br><span class="line">        </span><br><span class="line">        loss = loss_func(prediction, y)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># classification</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># turn on or turn of autograd</span></span><br><span class="line">torch_data.requires_grad_()</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">requires_grad = <span class="literal">True</span></span><br><span class="line">requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># backward</span></span><br><span class="line">out.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get a same tensor without requiring gradient</span></span><br><span class="line">torch_data.detach()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">The basic methods of pytorch</summary>
    
    
    
    
    <category term="python" scheme="https://1.15.86.100/tags/python/"/>
    
    <category term="pytorch" scheme="https://1.15.86.100/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Variational Inference</title>
    <link href="https://1.15.86.100/2019/12/14/Variational-Inference/"/>
    <id>https://1.15.86.100/2019/12/14/Variational-Inference/</id>
    <published>2019-12-14T15:19:54.000Z</published>
    <updated>2019-12-15T04:46:55.567Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Variational-Inference-变分推断"><a href="#Variational-Inference-变分推断" class="headerlink" title="Variational Inference(变分推断)"></a>Variational Inference(变分推断)</h1><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>$X\quad:observed\ data$</p><p>$Z\quad:latent\ variable+parameter$</p><p>$(X+Z)\quad:complete\ data$</p><p>$$log(P(x))=logP(x,z)-logP(z|x)=log\frac{P(x,z)}{q(z)}-log\frac{P(z|x)}{q(z)}$$</p><p>$$Left=\int_zlogP(x)q(z)dz=logP(x)$$</p><p>$$Right=\int_zq(z)log\frac{P(x,z)}{q(z)}dz-\int_zq(z)log\frac{P(z|x)}{q(z)}dz$$</p><p>$$ELBO(evidence\ lower\ bound)=\int_zq(z)log\frac{P(x,z)}{q(z)}$$</p><p>$$KL(q||p)=-\int_zq(z)log\frac{P(z|x)}{q(z)}dz\ge0$$</p><p>$$\quad\mathscr{L}(q)+KL(q||p)$$</p><p>令 $q(z)=\prod_{i=1}^mq_i(z_i)$</p><p>有 $\mathscr{L}(q)=\int_zq(z)logP(x,z)dz-\int_zq(z)logq(z)dz$</p>]]></content>
    
    
    <summary type="html">An Introduction to Variational Inference</summary>
    
    
    
    
    <category term="machine-learning" scheme="https://1.15.86.100/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Multiple sequence alignment with hierarchical clustering</title>
    <link href="https://1.15.86.100/2019/12/05/Multiple-sequence-alignment-with-hierarchical-clustering/"/>
    <id>https://1.15.86.100/2019/12/05/Multiple-sequence-alignment-with-hierarchical-clustering/</id>
    <published>2019-12-05T01:40:50.000Z</published>
    <updated>2019-12-05T05:06:10.351Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Multiple-sequence-alignment-with-hierarchical-clustering"><a href="#Multiple-sequence-alignment-with-hierarchical-clustering" class="headerlink" title="Multiple sequence alignment with hierarchical clustering"></a>Multiple sequence alignment with hierarchical clustering</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p> 无论是在蛋白质还是核酸的<strong>多序列的匹配</strong>问题，用计算机来实现，都是非常容易和准确的。这个方法是基于<strong>两两匹配</strong>的普通的<strong>贪心算法</strong>。开始的时候，用<strong>双匹配</strong>的<strong>分数矩阵来</strong>来实现序列的<strong>层次聚类</strong>。<strong>最近</strong>的序列会被<strong>联合</strong>起来，从而生成<strong>联合序列</strong>的群体(group)。然后，当一个<strong>群体</strong>内的所以序列都被<strong>联合(Aligned)<strong>起来后，最近的</strong>群体(Group)<strong>的会被联合(aligned)起来。在</strong>多匹配</strong>中的两个匹配好的序列会生成一个新的矩阵，而这个矩阵被用来产生一个<strong>层次聚类</strong></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  核酸(nucleic acids)和蛋白质(proteins)，可以被一些<strong>升序排列</strong>的分子生物学上的<strong>数字序列</strong>来表示，而这些数字可以通过自动且快速的技术来获得。因此，一个<strong>升序序列</strong>需要被我们分析，而这个在没有<strong>数据分析</strong>的帮助是不可能的。</p><p>  确认某些部分和一个相同的<strong>族</strong>(family)里面的许多序列对应部分相似的过程是非常有趣的。比如，蛋白质序列的相似区域和许多活着的的微生物(organism)有着相同的功能，这在功能和结构的观点来看是非常重要的。</p><p>  这些分析都需要，<strong>序列的匹配</strong></p><p>  两个序列的匹配可以许多自从1970就出现的算法实现。但是，当有多于两个序列时，</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="两个序列的匹配算法"><a href="#两个序列的匹配算法" class="headerlink" title="两个序列的匹配算法"></a>两个序列的匹配算法</h3><p>假设存在两个序列 $A、B$ 长度分别为 $m,n$，并且 $A(i),B(j)$ 分别表示 对应序列中的第 $i$ 个和第 $j$ 的元素。对于每一个<strong>元素对</strong>(pair of elements) $A(i),B(j)$ ，其权值 $weight\ w(i,j)$ 可以通过一个适合的矩阵 $D$ 来给定，比如<strong>Dayhoff</strong> 的氨基酸<strong>突变数据矩阵</strong>（如果需要的话，可以通过增加一个适当的<strong>常数</strong>来使得矩阵<strong>非负</strong>）。由此 $w(i,j)=D(A(i),A(j))$。$w$ 的值不需要存储，当需要的时候从存储的矩阵计算获得。计算中使用的方法和<strong>Needleman and Wunsh</strong>和<strong>Murata</strong>的方法一样，是从<strong>元素</strong>(cell) $(m,n)$ 开始，<strong>后向</strong>(backward)计算，依次计算从每条不同的元素来的<strong>路</strong>(path)的最大权值之和(maximum total value)。</p><p>令 $S(i,j)$ 表示从所有的从元素 $(i,j)$ 到底部(bottom)或者右边(right side)的路径上的所有元素权值之和再减去 $g$ 乘以路上的<strong>间隔</strong>(gap)数，即 $\sum w-g(n-1)$，的<strong>最大值</strong></p><p>这个<strong>间隔惩罚</strong>(gap penalty)是<strong>Barton and Sternberg</strong>建议的独立于间隔的长度。</p><p>令 $M(i,j)$ 表示 $S$ 上所有满足 $(i,k) and(l,j)\quad(j\le k\le n\ and\ i\le l\le m)$元素的最大值，根据它的定义 $M(i,j)$ 就表示 $S$ 上所有满足  $(l,k),(l\ge i \ and \ k\ge j)$ 的元素最大值。</p><p>下面的算法就是用来计算 $S\ and\ M$的：</p><p>$$S(i,j)=w(i,j)+max(S(i+1,j+1,M(i+1,j+1)-g)$$</p><p>$$M(i,j)=max(S(i,j),M(i+1,j),M(i,j+1))$$</p><p>一旦矩阵 $S$ 被计算出来，就会执行一个回溯(traceback) 的过程，取寻找最好的路径上的最优元素。它的首选元素就是<strong>第一行或第一列的最大元素</strong>。这个值就是<strong>匹配的分数</strong>。在每个路径的末尾都不需要增加<strong>间隔惩罚</strong>(gap penalty)。</p><p>为了比较多于两个的序列，已经匹配好的会被利用一个<strong>匹配算法</strong>(alignment algorithm)一步一步的重新组合，这也是<strong>过程一</strong>的延申。</p><h3 id="两个联合序列类的联合"><a href="#两个联合序列类的联合" class="headerlink" title="两个联合序列类的联合"></a>两个联合序列类的联合</h3><p>令 $B_1,…B_p$ 为一个<strong>类</strong>(cluster)中的序列，$C_1,…,C_Q$ 为第二个类中的序列。当生成一个矩阵 $S$ 来<strong>联合</strong>(align) $C$ 序列和 $B$ 序列。我们需要引入一个<strong>分数体制</strong>(scoring scheme)，它包含所有的以前已经联合起来的序列的贡献，因此需要赋予已经联合起来的区域更多的权重。</p><p>令 $i,j$ 分别表示序列 $B,C$ 某个<strong>联合的产物</strong>(aligned residue)的位置。则：</p><p>$$w(i,j)=\frac{1}{PQ}\sum_{R=1}^{R=P}\sum^{S=Q}_{S=1}D(B_R(i),C_S(j))$$</p><p>其中 $D$ 表示<strong>氨基酸联合分数</strong>(amino acid pair scores)。例如，**(Ala-Val-Leu)<strong>和</strong>(Ala-Leu)**联合的分数就是 $[w(Ala\ vs.\ Ala)+(Ala\ vs.\ Ala)+(Val\ vs. Ala)+(Val\ vs.\ Leu)+(Leu\ vs.\ Ala)+(Leu\ vs.\ Leu)]*\frac{1}{6}$</p><p>矩阵 $S$ 和矩阵 $M$ 和前面的算法一样，但是需要用新的 $w$ 来计算。</p><p>一旦获得一个由 $P+Q$ 联合的<strong>类</strong>，就会替代 $P\ Q$ 序列。</p><h3 id="聚类的顺序"><a href="#聚类的顺序" class="headerlink" title="聚类的顺序"></a>聚类的顺序</h3><p>聚类的顺序会影响聚类的结果，因此我们需要选择一个好的聚类顺序。这里用到的方法是：使用两两比较的得分作为序列间的相似度的序号。原则就是：聚类从基本的序列出发，通过<strong>联合两个最近的类</strong>产生新的类。</p><p>假设 $A_1,A_2,…,A_N$ 是 $N$ 个待联合的序列，所有的<strong>两两比较</strong>已经执行好，并被存储在一个矩阵 $T_1$ 中。其中 $T_1(I,J)$ 表示 $A_I$ 和 $A_J$ 联合的分数。然后，联合的序列的类按如下定义：</p><p>步骤一，有N个类，每个类有1个序列。最优分数在矩阵 $T_1$ 中。序列 $A_I$ 和序列 $A_J$ 【他们的分数是最适合的】被<strong>联合</strong>(aligned)起来，并且两个序列的<strong>联合体</strong>(alignment)产生一个聚类，这个类代替了第 $I$ 个序列，第 $J$ 个序列则被删除。然后，产生新的<strong>分数矩阵</strong> $T_2$，它的维度是 $N-1$ ，并且它等于：将 $T_1$ 的<strong>第 $J$ 行和第 $J$ 列</strong>删除，且<strong>第 $I$ 行和第 $I$ 列</strong>重新从 $T_1$ 中的 <strong>第 $I\  J$行列</strong>产生。其中，$T_2(I,k)$ 是 $T_1(I,k)$ 和 $T_1(J,k)$ 的平均值，$T_2(I,k)$ 称为**类 $I$ <strong>对</strong>类 $J$ **的分数(cluster $I$ vs. cluster $K$)。</p><p>步骤 $s\quad(s=1,2…,N-1)$，现在有 $N-s+1$个序列的类，$T_S$ 表示分数矩阵，如果 $T_S$ 的最大元素是 $T_S(I,J)$，<strong>类 $I$<strong>和</strong>类 $J$</strong> 将会被<strong>联合</strong>，并且会产生一个新的**类 $I$**，类 $J$ 会被删除。$T_{S+1}$ 按如下算法生成：</p><p>$$T_{S+1}(K,L)=T_S(K,L)\quad  if\ K,L\not=I,J$$</p><p>$$T_{s+1}(J,K)and \ T_{s+1}(K,J)$$ 不存在</p><p>$$T_{s+1}(I,K) = T_s(K,I)=(N_IT_s(I,K)+N_JT_s(J,K))/(N_I+N_J)\quad if K\not=I,J$$</p><p>其中 $N_I$ 是**类 $I$**中的序列数，类似 $N_J$</p><h3 id="完整的算法"><a href="#完整的算法" class="headerlink" title="完整的算法"></a>完整的算法</h3><ol><li>初始化：执行所有的两两之间的比较，并且记录他们的分数；</li><li>用得分矩阵进行序列的聚类；</li><li>利用两两聚类的分数获取完整的联合体，使层次树生长；</li><li>联合体产生，计算多个联合体之间的分数</li><li>用这些新的分数计算新的层次聚类</li><li>如果新的聚类和旧的不一样，可以根据聚类情况产生新的联合体转至3，否则结束循环。</li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2>]]></content>
    
    
    <summary type="html">A Paper Multiple Sequence Alignment Problem</summary>
    
    
    
    
    <category term="Algorithm" scheme="https://1.15.86.100/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>mySQL Command Line</title>
    <link href="https://1.15.86.100/2019/11/09/mySQL-Command-Line/"/>
    <id>https://1.15.86.100/2019/11/09/mySQL-Command-Line/</id>
    <published>2019-11-08T16:06:44.000Z</published>
    <updated>2020-10-29T12:03:17.298Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h1><h5 id="新建数据库"><a href="#新建数据库" class="headerlink" title="新建数据库"></a>新建数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE library;</span><br></pre></td></tr></table></figure><h5 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP DATABASE library</span><br></pre></td></tr></table></figure><h5 id="使用数据库"><a href="#使用数据库" class="headerlink" title="使用数据库"></a>使用数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE library</span><br></pre></td></tr></table></figure><h5 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE book(name char(20), author char(20))</span><br></pre></td></tr></table></figure><h5 id="查看表的内容"><a href="#查看表的内容" class="headerlink" title="查看表的内容"></a>查看表的内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM book WHERE id &#x3D; 1;</span><br></pre></td></tr></table></figure><h5 id="插入内容"><a href="#插入内容" class="headerlink" title="插入内容"></a>插入内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO book VALUES(&#39;jave&#39;, &#39;kkk&#39;)</span><br></pre></td></tr></table></figure><h5 id="查看数据库内的表"><a href="#查看数据库内的表" class="headerlink" title="查看数据库内的表"></a>查看数据库内的表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW TABLES</span><br></pre></td></tr></table></figure><h5 id="查看数据库"><a href="#查看数据库" class="headerlink" title="查看数据库"></a>查看数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW DATABASES</span><br></pre></td></tr></table></figure><h5 id="显示表的结构"><a href="#显示表的结构" class="headerlink" title="显示表的结构"></a>显示表的结构</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DESC book</span><br></pre></td></tr></table></figure><h5 id="获取上一次插入数据的ID"><a href="#获取上一次插入数据的ID" class="headerlink" title="获取上一次插入数据的ID"></a>获取上一次插入数据的ID</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT LAST_INSERT_ID();</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">SQL</summary>
    
    
    
    
    <category term="SQL" scheme="https://1.15.86.100/tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>GitHub Command Line</title>
    <link href="https://1.15.86.100/2019/10/29/GitHub-Command-Line/"/>
    <id>https://1.15.86.100/2019/10/29/GitHub-Command-Line/</id>
    <published>2019-10-29T14:33:04.000Z</published>
    <updated>2021-01-13T11:40:03.491Z</updated>
    
    <content type="html"><![CDATA[<h3 id="链接仓库"><a href="#链接仓库" class="headerlink" title="链接仓库"></a>链接仓库</h3><ol><li><p>复制仓库的链接</p><p><img src="/2019/10/29/GitHub-Command-Line/one.png" alt="Position"></p></li><li><p>在需要链接的文件夹下面打开<strong>Git Bash</strong></p><p><img src="/2019/10/29/GitHub-Command-Line/two.png" alt="git"></p></li><li><p>命令行界面如下：</p><p><img src="/2019/10/29/GitHub-Command-Line/three.png" alt="GitBash"></p></li><li><p>运行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init<span class="comment">#初始化本地仓库</span></span><br><span class="line">git config user.email<span class="string">&quot;your email@address&quot;</span></span><br><span class="line">git config user.name<span class="string">&quot;your name&quot;</span></span><br><span class="line">git add .</span><br><span class="line">git commit -m<span class="string">&#x27;My first post&#x27;</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure></li><li><p>获取ssh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.ssh</span><br><span class="line">cat id_rsa.pub</span><br></pre></td></tr></table></figure></li><li><p>撤回上一次commit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD~</span><br><span class="line">git reset HEAD@&#123;index&#125;</span><br></pre></td></tr></table></figure></li><li><p>获取已经commit但未push的文件信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git cherry -v</span><br></pre></td></tr></table></figure></li><li><p>当前提交状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure></li><li><p>列出所有操作记录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reflog</span><br></pre></td></tr></table></figure></li><li><p>合并到最后一次提交</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend</span><br></pre></td></tr></table></figure></li><li><p>获取最新提交记录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git cherry-pick master</span><br></pre></td></tr></table></figure></li><li><p>对远程仓库的操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看所有的远程仓库</span></span><br><span class="line">git remote</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看远程仓库对应的地址</span></span><br><span class="line">git remote -v</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置新的远程仓库地址</span></span><br><span class="line">git remote set-url origin [url]</span><br></pre></td></tr></table></figure></li></ol><h3 id="Git-Problem"><a href="#Git-Problem" class="headerlink" title="Git Problem"></a>Git Problem</h3><ol><li><pre><code class="bash">fatal: unable to access &#39;https://github.com/Vilily/python.git/&#39;: SSL certificate problem: self signed certificate in certificate chainsolution：git config --global http.sslVerify false</code></pre></li><li></li></ol>]]></content>
    
    
    <summary type="html">Common Command Lines and the Solutions to sone Errors of GitHub.</summary>
    
    
    
    
    <category term="GitHub" scheme="https://1.15.86.100/tags/GitHub/"/>
    
  </entry>
  
  <entry>
    <title>PCA algorithm</title>
    <link href="https://1.15.86.100/2019/10/28/PCA-algorithm/"/>
    <id>https://1.15.86.100/2019/10/28/PCA-algorithm/</id>
    <published>2019-10-28T13:09:18.000Z</published>
    <updated>2019-11-01T16:12:03.191Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>PCA(Principal Component Analysis)【主成分分析】</strong>，是一种常用的<strong>无监督学习(Unsupervised Learning)<strong>算法，它主要被用在数据降维方面，是机器学习中一种数据进行</strong>预处理</strong>的重要方法</p><h2 id="降维-Dimensionality-Reduction"><a href="#降维-Dimensionality-Reduction" class="headerlink" title="降维(Dimensionality Reduction)"></a>降维(Dimensionality Reduction)</h2><h3 id="啥是降维"><a href="#啥是降维" class="headerlink" title="啥是降维"></a>啥是降维</h3><p><strong>降维</strong>就是将原来维度过高的数据集经过<strong>线性</strong>或<strong>非线性</strong>的变换，变成低维的数据集。比如我有一个观测数据数据 $\vec x={1,2,2.3}$ ，经过某个变换后，剔除了它的<strong>多余</strong>的维度得到 $\vec x={2,2.3}$。这就是一个降维的过程</p><h3 id="为啥要降维"><a href="#为啥要降维" class="headerlink" title="为啥要降维"></a>为啥要降维</h3><p>降维的原因有许多，这里主要讲两个比较重要的原因：</p><ol><li>剔除无用的维度可以减少训练样本的数据量，便于我们的训练学习过程，特别是当样本量特别巨大时【一般有几百G或者几T的数据】。</li><li>降低数据的<strong>多余的无用的</strong>维度可以提高训练出来的模型的<strong>泛化能力</strong>，避免**过拟合(Over Fitting)**。因为数据维度过高时需要训练的模型参数也会增加，这就很容易导致模型过拟合。</li></ol><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>比如我们看这些数据：</p><p><img src="/2019/10/28/PCA-algorithm/Figure_2.png" alt="Figure_1"></p><img src="/2019/10/28/PCA-algorithm/Figure_3.png" alt="Figure_2" style="zoom: 200%;"><p>我们发现，这些数据在y维度上的偏差均匀分布且偏差极小，因此可以对其进行降维处理【删除该维度】，得到下列数据</p><p><img src="/2019/10/28/PCA-algorithm/Figure_1.png" alt="Figure"></p><p>这些数据不仅保留了原数据的<strong>主要特征</strong>，而且剔除了多余的干扰，更利于训练何提高泛化性。</p><h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><h3 id="PCA降维原则"><a href="#PCA降维原则" class="headerlink" title="PCA降维原则"></a>PCA降维原则</h3><p>我们先考虑二维情况，然后再推广到高维。</p><p>考虑下面的数据集：</p><p><img src="/2019/10/28/PCA-algorithm/Figure_5.png" alt="data"></p><p>可以发现，图中不在线上的蓝色数据点基本上沿着一条直线分布。因此我们可以断定，数据集的 x 和 y 之间存在相关关系。因此我们可以进行降维。</p><p>为了对这个数据集进行降维，我们首先要确定什么样的<strong>维</strong>是<strong>好的维</strong>，什么样的<strong>维</strong>是我们要<strong>丢弃的维</strong>。</p><p>为此我们分别沿数据的分布方向和其垂直方向做两条垂直的直线(如图)，根据<strong>线性代数</strong>相关知识，这两条直线代表的构成了该数据空间的<strong>标准正交基</strong>。也就是说以这两条直线为<strong>坐标轴</strong>也可以表示该数据集。</p><p>现在我们将数据点分别<strong>投影</strong>到这两条直线上(如上图)。</p><p>如果进行降维，我们必须要<strong>丢弃一个坐标轴</strong>，也就是所降维后的数据集就是，原始数据集在这两条线<strong>其中一条</strong>上的投影。</p><p>假设我们选择<strong>保留橘黄色</strong>的坐标轴，而丢弃蓝色坐标轴，可以发现：数据的分布十分紧密【<strong>方差较小</strong>】，因此数据对原始数据信息(<strong>数据的分布特征</strong>)保留的也较少。</p><p>而相反，如果我们选择<strong>保留蓝色坐标轴</strong>，这时：数据的分布十分稀疏【<strong>反差较大</strong>】，因此数据对原始数据信息的保留的也较多。</p><p>根据上面的分析，我们得出结论：</p><p>进行降维时我们要保留<strong>使数据在该维度上面投影方差最大</strong>的维度，而丢弃方差最小的维度，这也就是**主成分分析(PCA)**的主要思想。</p><h3 id="PCA推导"><a href="#PCA推导" class="headerlink" title="PCA推导"></a>PCA推导</h3><p>原始数据集 ${\vec x_i},\quad (i=1,2,…,N)$，为<strong>列向量</strong>；</p><p>为了方便计算我们先对数据集进行<strong>中心化</strong>即令 $ \vec x_i=\vec x_i-\vec\mu$ ，$\vec\mu$ 为原始数据集的均值，下面使用的$x_i$ 都指以及<strong>中心化</strong>的数据。</p><p>所以<strong>中心化</strong>后的数据的<strong>平均值</strong> $\vec\mu=0$;</p><p>假设某一<strong>正交基</strong>为 $\vec{u}$ 即投影坐标轴的方向；</p><p>因此 $\vec x_i$ 在 $\vec u$ 方向的投影坐标为 $\hat x_i = \vec x_i^T\cdot\vec u$【根据线性代数知识】；</p><p>而投影后的样本均值</p><p>$$\hat \mu = \frac{\sum_{i=1}^N\hat x_i}{N}=0$$</p><p>因此<strong>PCA</strong>的求解目标就是:</p><p>$$J=\frac{1}{N}\sum_{i=1}^N\Big((\vec x_i^T\cdot\vec u)-\hat\mu\Big)^2$$</p><p>$$s.t.\quad \vec u^T\vec u=1$$</p><p>$$=\frac{1}{N}\sum_{i=1}^N(\vec x_i^T\cdot\vec u)^2$$</p><p>$$=\frac{1}{N}\sum_{i=1}^N\vec u^T\cdot \hat x_i\cdot \hat x_i^T\cdot\vec u$$</p><p>$$=\frac{1}{N}\vec u^T\Big(\sum_{i=1}^N\hat x_i\cdot\hat x_i^T\Big)\vec u$$    <font color="red">(*)</font></p><h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><p>根据统计学相关知识，协方差为：</p><p>$$\Sigma=cov(\vec x,\vec x)=E[(\vec x-\vec\mu)\cdot(\vec x-\vec\mu)^T]$$</p><hr><p>因此，<font color="red">(*)</font>式可以化为：</p><p>$$J=\vec u^T\Sigma\vec u$$</p><p>于是我们最终的优化目标就是：</p><p>$$argmax_u J=argmax_u(\vec u^T\Sigma\vec u)$$</p><p>$$s.t.\quad\vec u^T\vec u=1$$    <font color="red">(1)</font></p><h4 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h4><p>为了求解上面得到的<strong>最优化</strong>问题，我们引入<strong>拉格朗日乘子</strong> $\lambda$</p><p>定义<strong>拉格朗日函数</strong>：</p><p>$$L=\vec u^T\Sigma\vec u-\lambda(\vec u^T\vec u-1)$$</p><p>对该函数求导并令其为0得：</p><p>$$\Sigma\vec u-\lambda\vec u=0$$</p><p>即：$\Sigma\vec u=\lambda\vec u$</p><p>由线性代数知识得到，$\lambda$ 为 $\Sigma$ 得特征值，$\vec u$  对应得特征向量，于是：</p><p>$$J=\vec u^T\Sigma\vec u=\vec u^T\lambda\vec u=\lambda$$</p><p>又因为 $\Sigma$ 的特征向量都是相互<strong>正交</strong>的<strong>单位向量</strong>，且所有的特征向量构成了原数据集的<strong>标准正交基</strong></p><p>所以，对原数据降维只需要：<strong>保留较大特征值对应的特征向量，而丢弃较小特征值对应的特征向量</strong>。</p><p>以上就是<strong>主成分分析</strong>的主要方法。</p><h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>**奇异值分解(PCA,Principle Component Analysis)**，是矩阵论的重要内容。</p><p><strong>奇异值分解基本定理</strong>：若A为一个$m\times n$ 实矩阵，$A\in R^{m\times n}$，则 $A$ 的奇异值分解存在：</p><p>$$\vec A=\vec U\vec \Sigma\vec V^T$$    <font color="red">(#)</font></p><p>其中 $U$ 是m阶正交矩阵，$V$是n阶正交矩阵， $\Sigma$ 是 $m\times n$ 对角矩阵，其对角线元素非负，且按降序排列。</p><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>此处简单进行一下<strong>构造性证明</strong></p><ol><li><p>确定 $\vec V$：</p><p>对于待分解的 $m\times n$ 矩阵 $A$，构造一个 n阶 <strong>实对称矩阵</strong> $A^TA$。由线性代数知识我们得到 $A^TA$ 的 n 个<strong>特征值</strong>，记为 $\lambda_i(i=1,2,…,n)$，每个矩阵对应的<strong>特征向量</strong>记为 $\vec v_i(i=1,2,…,n)$。</p><p>则我们令：</p><p>$$\vec V=[\vec v_1,\vec v_2,…,\vec v_n]$$</p></li></ol><ol start="2"><li><p>确定 $\vec\Sigma$：</p><p>令 $\sigma_i=\sqrt{\lambda_i}(i=1,2,…,n)$</p><p>$$\Sigma_1=\begin{pmatrix}\sigma_1&amp;0&amp;\dots&amp;0\\0&amp;\sigma_2&amp;\dots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\dots&amp;\sigma_n\end{pmatrix}$$</p><p>则：</p><p>$$\Sigma=\begin{pmatrix}\Sigma_1&amp;0\\0&amp;0\end{pmatrix}$$</p></li></ol><ol start="3"><li><p>确定 $\vec U$：</p><p>令 $\vec u_i=\frac{1}{\sigma_i}\vec A\vec v_i$</p><p>令 $\vec U_1=[\vec u_1, \vec u_2,…,\vec u_n]$，$\vec U_2$ 为 $\vec A^T$ 的一组<strong>标准正交基</strong>。</p><p>则最终的：</p><p>$$\vec U=[\vec U_1,\vec U_2]$$</p></li></ol><ol start="4"><li>由线性代数知识知道上面得到的矩阵满足 <font color="red">(#)</font>式，且 $\vec V,\vec U$ 均为正交矩阵，$\vec\Sigma$ 是对角阵。</li></ol><h3 id="用SVD求PCA"><a href="#用SVD求PCA" class="headerlink" title="用SVD求PCA"></a>用SVD求PCA</h3><h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p>对原始得数据矩阵 $\vec X=[\vec x_1,\vec x_2,…,\vec x_n]$【已经进行了<strong>中心化</strong>】进行奇异值分解：</p><p>$$\vec X=\vec U\vec\Sigma\vec V^T$$</p><p>由<font color="red">(1)</font>式得，<strong>主成分</strong>就是原<strong>协方差矩阵</strong>的特征值。</p><p>又由协方差矩阵的定义得 $\vec\Sigma=\vec X^T\vec X=\vec V\vec\Sigma^T\vec U^T\vec U\vec\Sigma\vec V^T$</p><p>因为 $\vec U^T\vec U=\vec I,\vec V^T\vec V=\vec I$</p><p>所以上式继续化为</p><p>$$\Sigma=\vec V\Sigma^T\vec\Sigma\vec V^T$$</p><p>$$=\vec V\vec\Sigma^2\vec V^T$$</p><p>上式也就是协方差矩阵的<strong>特征值分解</strong>，所以 $\vec V$ 矩阵对应的列向量也就是 <strong>主成分</strong></p>]]></content>
    
    
    <summary type="html">A simply inroduction to the PCA algorithm.</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>GMM algorithm</title>
    <link href="https://1.15.86.100/2019/10/25/GMM-algorithm/"/>
    <id>https://1.15.86.100/2019/10/25/GMM-algorithm/</id>
    <published>2019-10-25T14:05:41.000Z</published>
    <updated>2019-10-27T05:39:19.965Z</updated>
    
    <content type="html"><![CDATA[<h1 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>高斯混合模型(Gaussian Mixture Model)<strong>，是前面推导的</strong>EM算法</strong>的一个重要应用。高斯混合模型应用广泛，特别是在**聚类(Cluster)**学习方面，是一种软聚类器。</p><h2 id="模型推导"><a href="#模型推导" class="headerlink" title="模型推导"></a>模型推导</h2><h3 id="高斯分布定义"><a href="#高斯分布定义" class="headerlink" title="高斯分布定义"></a>高斯分布定义</h3><p>我们先回顾一下高斯分布的定义：</p><p>对于n维样本空间 $\chi$ 中的随机变量 $x$，若 $x$ 服从高斯分布，其概率密度函数为：</p><p>$$P(x) =\phi(x) = \frac{1}{2\pi^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}\frac{(x-\mu)^T(x-\mu)}{\Sigma}}$$    <font color="red">(1)</font></p><p>其中 $\vec\mu$ 为n维均值向量，$\vec\Sigma$ 为 $n*n$ 的<strong>协方差矩阵</strong>，由<font color="red">(1)</font>可以看出：高斯分布<strong>完全</strong>由均值向量 $\vec\mu$ 和协方差矩阵 $\vec\Sigma$ 这两个参数确定，为了明确高斯分布与相应参数的依赖关系，将概率密度函数记为$$\phi(\vec x|\vec\mu,\vec\Sigma)$$</p><p>下文中为了方便，<strong>向量不再加箭头</strong>。</p><h3 id="高斯混合模型-1"><a href="#高斯混合模型-1" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h3><h4 id="1-明确隐变量，写出完全数据的对数似然函数"><a href="#1-明确隐变量，写出完全数据的对数似然函数" class="headerlink" title="1.明确隐变量，写出完全数据的对数似然函数"></a>1.明确隐变量，写出完全数据的对数似然函数</h4><p>为了表达方便我们记 $\theta = (\mu,\Sigma)$ 为高斯分布的参数。</p><p>于是我们可以定义<strong>高斯混合模型</strong>为：</p><p>$$P(y|\theta) = \sum_{k=1}^{K}\alpha_k\phi(y|\theta_k)$$    <font color="red">(2)</font></p><p>其中，$\alpha_k$ 是系数，$\alpha_k\ge0$，$\sum_{k=1}^K=1$；$\phi(y|\theta_k)$ 是高斯概率密度函数。</p><p>$$\phi(y|\theta_k)=\frac{1}{\sqrt{2\pi}\sigma_i}exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})$$        <font color="red"> (3)</font></p><p><font color="red">(3)</font>式称为第 $i$ 个分模型。</p><p>设想观测数据 $y_j$ 是这样产生的：首先依概率 $\alpha_i$ 选择 第 $k$ 个高斯分布分模型 $\phi(y|\theta_k)$，然后依第 $k$ 个分模型的概率分布 $\phi(y|\theta_k)$ 生成观测数据 $y_j$。这时，观测数据 $y_j$ 是已知的；反映观测数据 $y_j$ 来自第 $k$ 个分模型的数据是未知的，$k=1,2,…,K$，以隐变量 $\gamma_{jk}$ 表示，其定义如下：</p><p>$$\gamma_{jk}=\begin{cases}1, \quad 第j个观测来自第k个模型\\0,\quad 否则\end{cases}$$</p><p>$$j=1,2,…,N;k=1,2,…,k$$    <font color="red">(4)</font></p><p>$\gamma_{jk}$ 是0-1随机变量【但也可以看成一种加权平均】</p><p>有了观测数据 $y_j$ 以及未观测数据 $\gamma_{jk}$，那么完全数据是：</p><p>$$(y_j,\gamma_{j1},\gamma_{j2},…,\gamma_{jk}),\quad j=1,2,…,N$$</p><p>于是我们可以写出完全数据的似然函数：</p><p>$$P(y,\gamma|\theta)=\prod_{j=1}^NP(y_j,\gamma_{j1},\gamma_{j2},…,\gamma_{jk}|\theta)$$</p><p>$$=\prod_{k=1}^K\prod_{j=1}^N[\alpha_k\phi(y_j|\theta_k)]^{\gamma_{jk}}$$</p><p>$$=\prod_{k=1}^K\alpha_k^{n_k}\prod_{i=1}^N[\phi(y_j|\theta_k)]^{\gamma_{jk}}$$</p><p>$$=\prod_{k=1}^K\alpha_k^{n_k}\prod_{j=1}^N[\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{(y_j-mu_k)^2}{2\sigma_k^2})]^{\gamma_{jk}}$$</p><p>式中，$n_k=\sum_{j=1}^N\gamma_{jk},\sum_{k=1}^Kn_k=N$</p><p>那么，完全数据的对数似然函数为：</p><p>$$logP(y,\gamma|\theta)=\sum_{k=1}^K\Bigg(n_klog\alpha_k+\sum_{j=1}^N\gamma_{jk}\Big[log\big(\frac{1}{\sqrt{2\pi}}-log\sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k\big)^2\Big]\Bigg)$$    <font color="red">(5)</font></p><h4 id="2-EM算法的E步：确定Q函数"><a href="#2-EM算法的E步：确定Q函数" class="headerlink" title="2.EM算法的E步：确定Q函数"></a>2.EM算法的E步：确定Q函数</h4><p>根据Q函数定义得到：</p><p>$$Q(\theta,\theta^{i})=E[logP\Big((y,\gamma|\theta)\Big)|y,\theta^{(i)}]$$</p><p>$$=E\Bigg(\sum_{k=1}^K\Big(n_klog\alpha_k+\sum_{j=1}^N\gamma_{jk}\Big[log\big(\frac{1}{\sqrt{2\pi}}\big)-log\sigma_k^2-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\Big]\Big)\Bigg)$$</p><p>$$=\sum_{k=1}^K\Bigg(\sum_{j=1}^Nlog\alpha_k+\sum_{j=1}^N\big(E(\gamma_{jk})\big)\Big[log\big(\frac{1}{\sqrt{2\pi}}\big)-log\sigma_k^2-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\Big]\Bigg)$$    <font color="red">(6)</font></p><p>这里的 $E(\gamma_{jk})$ 就是 $E(\gamma_{jk}|\theta)$ ，记为 $\hat\gamma_{jk}$</p><p>$$\hat\gamma_{jk}=E(\gamma_{jk}|y_j,\theta)=P(\gamma_{jk}=1|y_j,\theta)$$</p><p>【$\gamma_{kj}$ 为0-1随机变量】</p><p>$$=\Large \frac{P(\gamma_{jk}=1,y_j|\theta_k)}{\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta_k)}$$</p><p>【上式由以下公式推得：$P(\gamma_{jk}=1,y_j|\theta)=\frac{P(\gamma_{jk}=1,y_j,\theta)}{P(\theta)}$，$P(\gamma_{jk}=1|y_j,\theta)=\frac{P(\gamma_{jk}=1,y_j,\theta)}{P(y_j,\theta)}$，$\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta)=\sum_{k=1}^K\frac{P(\gamma_{jk}=1,y_j,\theta)}{P(\theta)}=\frac{P(y_j,\theta)}{P(\theta)}$】</p><p>$$=\Large \frac{P(y_j|\gamma_{jk}=1,\theta_k)P(\gamma_{jk}=1|\theta_k)}{\sum_{k=1}^KP(y_j|\gamma_{jk}=1,\theta_k)P(\gamma_{jk}=1|\theta_k)}$$</p><p>【上式由<strong>条件概率公式</strong>得到】</p><p>$$\Large{\frac{\alpha_k\phi(y_j|\theta_k)}{\sum_{k=1}^K\alpha_k\phi(y_j|\theta_k)}},\quad j=1,2,…,N;k=1,2,…,K$$    <font color="red">(7)</font></p><p>【上式因为 $\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta_k)=P(y_j|\theta_k)=\alpha_k\phi(y_j|\theta_k)$】</p><p>$\hat\gamma_{jk}$ 是在当前模型参数下，第j个观测数据来自第k个分模型的概率，称为分模型k对观测数据 $y_j$ 的<strong>响应度</strong>。</p><p>将 $\hat\gamma_{jk}=E(\gamma_{jk})$ 以及 $n_k=\sum_{j=1}^NE(\gamma_{jk})$ 代入式<font color="red">(6)</font>得：</p><p>$$Q(\theta,\theta^{(i)})=\sum_{k=1}^K\Bigg(n_klog\alpha_k+\sum_{j=1}^N\hat\gamma_{jk}\Big[log\big(\frac{1}{\sqrt{2\pi}}\big)-log\sigma_k^2-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\Big]\Bigg)$$    <font color="red">(8)</font></p><h4 id="3-确定EM算法得M步-求极大值"><a href="#3-确定EM算法得M步-求极大值" class="headerlink" title="3.确定EM算法得M步-求极大值"></a>3.确定EM算法得M步-求极大值</h4><p>EM算法的M步是求函数 $Q(\theta,\theta^{(i)})$ 对 $\theta$ 的极大值，即：</p><p>$$\theta^{(i+1)}=argmax_\theta Q(\theta,\theta^{(i)})$$</p><p>用 $\hat\mu_k$，$\hat\sigma_k^2$，以及 $\hat\alpha_k$ 分别表示 $\theta^{(i+1)}$ 的各个参数。然后将<font color="red">(7)</font>分别对这三个参数求偏导并令其为0得：</p><p>$$\Large\hat\mu_k=\Large \frac{\sum_{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}},\quad k=1,2,…,K$$</p><p>$$\Large\hat\sigma_k^2=\frac{\sum_{j=1}^N\hat\gamma_{jk}(y_j-\mu_k)^T(y_j-\mu_k)}{\sum_{j=1}^N\hat\gamma_{jk}},\quad k=1,2,…,K$$</p><p>$$\Large\hat\alpha_k=\frac{n_k}{N}=\frac{\sum_{j=1}^N\hat\gamma_{jk}}{N},\quad k=1,2,…,K$$</p><p>由此我们得到迭代计算的公式，最终得到<strong>高斯混合模型参数估计的EM算法</strong></p><h2 id="高斯混合模型参数估计算法"><a href="#高斯混合模型参数估计算法" class="headerlink" title="高斯混合模型参数估计算法"></a>高斯混合模型参数估计算法</h2><hr><p>输入：样本集 $D={x_1,x_2,…,x_m}$;</p><p>​            高斯混合成分个数(子高斯分布个数)K</p><p>过程：</p><p>1：初始化高斯混合模型的模型参数 ${(\alpha_k,\mu_k,\Sigma_k)|1\le i\le K}$【这里的 $\Sigma$ 就是前面推导过程中的 $\sigma$ 在多维空间        的推广。</p><p>2：<strong>repeat</strong></p><p>3：    <strong>for</strong>  $j=1,2,…,m$ <strong>do</strong></p><p>4：        根据式<font color="red">(7)</font>计算 $x_j$ 由各混合成分生成的后验概率，即</p><p>5：        $\hat\gamma_{jk}=P(\gamma_{jk}=1|x_j,\theta^{(0)})$</p><p>6：    <strong>end for</strong></p><p>7：    <strong>for</strong> $i=1,2,…,k$ <strong>do</strong></p><p>8：        计算<strong>新均值向量</strong>：$\Large\hat\mu_i=\frac{\sum_{j=1}^m\gamma_{jk}x_j}{\sum_{j=1}^m\gamma_{jk}}$;</p><p>9：        计算<strong>新协方差矩阵</strong>：$\Large\hat\Sigma_i=\frac{\sum_{j=1}^m\gamma_{jk}(x_j-\hat\mu)(x_j-\hat\mu)^T}{\sum_{j=1}^m\gamma_{jk}}$;</p><p>10：        计算<strong>新混合系数</strong>：$\Large\hat\alpha_i=\frac{\sum_{j=1}^m\gamma_{jk}}{m}$;</p><p>11：    <strong>end for</strong></p><p>12：    将模型参数更新为 ${(\hat\alpha_i,\hat\mu_i,\hat\Sigma_i)|1\le i \le K}$</p><p>13：<strong>until</strong> 满足停止条件【参数更新幅度小于某一值或者迭代指定次数】</p><p>14：$C_i=\emptyset\quad (1\le i\le K)$</p><p>15：<strong>for</strong> $j=1,2,…,m$ <strong>do</strong></p><p>16：    根据 $\lambda_j=argmax_{i=1,2,…,K}\gamma_{jk}$ 确定 $x_j$ 簇标记 $\lambda_j$;</p><p>17：    将 $x_j$ 划入相应的簇：$C_{\lambda_{j}}=C_{\lambda_k}\bigcup{x_j}$</p><p>18：<strong>end for</strong></p><p>输出：簇划分 $C={C_1,C_2,…,C_K}$</p><hr><h2 id="Python-代码实现"><a href="#Python-代码实现" class="headerlink" title="Python 代码实现"></a>Python 代码实现</h2><h3 id="高斯混合模型算法代码"><a href="#高斯混合模型算法代码" class="headerlink" title="高斯混合模型算法代码"></a>高斯混合模型算法代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.linalg <span class="keyword">as</span> llg</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GaussianF</span>(<span class="params">mu, sigma, data_Y, K</span>):</span></span><br><span class="line">    Gaussian = []</span><br><span class="line">    sigma_inv = llg.pinv(sigma)</span><br><span class="line">    sigma_value = llg.det(sigma)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        first = <span class="number">1</span>/(((<span class="number">2</span>*np.pi)**(data_Y.shape[<span class="number">0</span>]/<span class="number">2</span>))*np.sqrt(sigma_value[k]))</span><br><span class="line">        Gaussian_k = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_Y.shape[<span class="number">1</span>]):</span><br><span class="line">            second = np.exp(-<span class="number">0.5</span>*np.dot(np.dot((data_Y[:, i] - mu[:, k])[np.newaxis, :], sigma_inv[k,::]), (data_Y[:, i] - mu[:, k])[:np.newaxis]))</span><br><span class="line">            Gaussian_k.append((first*second).tolist())</span><br><span class="line">        Gaussian.append(Gaussian_k)</span><br><span class="line">    Gaussian = np.array(Gaussian)</span><br><span class="line">    Gaussian = np.transpose(Gaussian.reshape((Gaussian.shape[<span class="number">0</span>], Gaussian.shape[<span class="number">1</span>])))</span><br><span class="line">    <span class="keyword">return</span> Gaussian</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalcMu</span>(<span class="params">data_Y, gamma, K</span>):</span></span><br><span class="line">    mu = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        mu_up = <span class="number">0</span></span><br><span class="line">        mu_down = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(data_Y.shape[<span class="number">1</span>]):</span><br><span class="line">            mu_up += gamma[j, i]*data_Y[:, j]</span><br><span class="line">            mu_down += gamma[j, i]</span><br><span class="line">        mu.append(mu_up/mu_down)</span><br><span class="line">    mu = np.transpose(np.array(mu))</span><br><span class="line">    <span class="keyword">return</span> mu</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Covariance</span>(<span class="params">gamma, data_Y, mu, K</span>):</span></span><br><span class="line">    sigma = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        sigma_up = <span class="number">0</span></span><br><span class="line">        sigma_down = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(data_Y.shape[<span class="number">1</span>]):</span><br><span class="line">            y = data_Y[:, j][:, np.newaxis]</span><br><span class="line">            mu_i = mu[:, i][:, np.newaxis]</span><br><span class="line">            sigma_up += gamma[j, i]*np.dot((y - mu_i), np.transpose(y - mu_i))</span><br><span class="line">            sigma_down += gamma[j, i]</span><br><span class="line">        sigma.append(sigma_up/sigma_down)</span><br><span class="line">    sigma = np.array(sigma)</span><br><span class="line">    <span class="keyword">return</span> sigma</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Mixture</span>(<span class="params">gamma, num, K</span>):</span></span><br><span class="line">    alpha = []</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        alpha_j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">            alpha_j += gamma[j, k]</span><br><span class="line">        alpha.append(alpha_j/num)</span><br><span class="line">    <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalcGamma</span>(<span class="params">alpha, mu, sigma, data_Y, K</span>):</span></span><br><span class="line">    Gaussian = GaussianF(mu, sigma, data_Y, K)</span><br><span class="line">    gamma = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(data_Y.shape[<span class="number">1</span>]):</span><br><span class="line">        Gaussian_j = <span class="number">0</span></span><br><span class="line">        gamma_j = []</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">            Gaussian_j += alpha[k]*Gaussian[j, k]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">            gamma_j.append(alpha[k]*Gaussian[j, k]/Gaussian_j)</span><br><span class="line">        gamma.append(gamma_j)</span><br><span class="line">    gamma = np.array(gamma)</span><br><span class="line">    <span class="keyword">return</span> gamma</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GMM</span>(<span class="params">data_Y, K=<span class="number">2</span></span>):</span><span class="comment">#data_Y为列向量</span></span><br><span class="line">    alpha = [<span class="number">1</span>/K]*K</span><br><span class="line">    sigma = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        sigma.append(np.eye(data_Y.shape[<span class="number">0</span>]))</span><br><span class="line">    sigma = np.array(sigma)</span><br><span class="line">    mu = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        mu.append(data_Y[:,i])</span><br><span class="line">    mu = np.transpose(np.array(mu))</span><br><span class="line">    gamma = CalcGamma(alpha, mu, sigma, data_Y, K)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        mu = CalcMu(data_Y, gamma, K)</span><br><span class="line">        sigma = Covariance(gamma, data_Y, mu, K)</span><br><span class="line">        alpha = Mixture(gamma, data_Y.shape[<span class="number">1</span>], K)</span><br><span class="line">        gamma = CalcGamma(alpha, mu, sigma, data_Y, K)</span><br><span class="line">    <span class="keyword">return</span> gamma</span><br></pre></td></tr></table></figure><h3 id="高斯混合模型聚类学习样例"><a href="#高斯混合模型聚类学习样例" class="headerlink" title="高斯混合模型聚类学习样例"></a>高斯混合模型聚类学习样例</h3><p>这里，我们先利用<strong>正态分布随机数发生器</strong>随机生成三组正态分布参数不同的数据，然后将三组数据组合成一组数据作为训练数据。经过高斯混合模型训练后，利用matploylib绘图函数可视化分类结果。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    data_0 = np.random.normal(loc=<span class="number">1</span>, scale=<span class="number">2</span>, size=(<span class="number">100</span>,<span class="number">2</span>))</span><br><span class="line">    data_1 = np.random.normal(loc=<span class="number">10</span>, scale=<span class="number">2</span>, size=(<span class="number">100</span>,<span class="number">2</span>))</span><br><span class="line">    data_2 = np.random.normal(loc=<span class="number">5</span>, scale=<span class="number">1</span>, size=(<span class="number">100</span>,<span class="number">2</span>))</span><br><span class="line">    data_Y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_0.shape[<span class="number">0</span>]):</span><br><span class="line">        data_Y.append(data_0[i, :])</span><br><span class="line">        data_Y.append(data_1[i, :])</span><br><span class="line">        data_Y.append(data_2[i, :])</span><br><span class="line">    data_Y = np.transpose(data_Y)</span><br><span class="line">    K = <span class="number">3</span></span><br><span class="line">    gamma = GMM(data_Y, K)</span><br><span class="line">    index = np.argmax(gamma, axis=<span class="number">1</span>)</span><br><span class="line">    index_ = []</span><br><span class="line">    data_Y_ = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        data_Y_ = data_Y[:,np.where(index == i)]</span><br><span class="line">        plt.scatter(data_Y_[<span class="number">0</span>, :], data_Y_[<span class="number">1</span>, :])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>最后输出的结果图像如下：</p><p><img src="/2019/10/25/GMM-algorithm/Figure.png" alt="GMM"></p><p>可以看出，<strong>高斯混合模型聚类器</strong>很好的将数据进行聚类！</p>]]></content>
    
    
    <summary type="html">What&#39;s GMM and How to use the GMM to do clusterring?</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
    <category term="Cluster" scheme="https://1.15.86.100/tags/Cluster/"/>
    
    <category term="Python" scheme="https://1.15.86.100/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>EM algorithm</title>
    <link href="https://1.15.86.100/2019/10/19/EM-algorithm/"/>
    <id>https://1.15.86.100/2019/10/19/EM-algorithm/</id>
    <published>2019-10-19T04:16:02.000Z</published>
    <updated>2019-10-25T14:03:28.174Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Expectation-Maximization-algorithm"><a href="#Expectation-Maximization-algorithm" class="headerlink" title=" Expectation-Maximization algorithm "></a><center> Expectation-Maximization algorithm </center></h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  <strong>EM</strong>算法（ Expectation-Maximization algorithm ），又称<strong>期望极大算法</strong>，是<strong>机器学习</strong>（Machine Learning）十大算法之一。</p><p>  <strong>EM</strong>算法主要通过迭代的方法来进行<strong>极大似然估计</strong>，最终使似然函数最大，从而获取模型参数进而建立预测模型的算法，<strong>EM</strong>算法通常和<strong>高斯混合分布</strong>算法联合使用来进行<strong>分类学习</strong>。</p><h2 id="对数似然函数-Log-likelihood"><a href="#对数似然函数-Log-likelihood" class="headerlink" title="对数似然函数(Log likelihood)"></a>对数似然函数(Log likelihood)</h2><h3 id="似然函数-Likelihood"><a href="#似然函数-Likelihood" class="headerlink" title="似然函数(Likelihood):"></a>似然函数(Likelihood):</h3><p>  <strong>似然函数</strong>是统计学上计算统计模型参数的函数，给定输出$y$后，关于参数$\theta$的似然函数记为$L(\theta|y)$.</p><p>  其计算方法为：$$L(\theta|Y) = P(Y=y|\theta)$$</p><p>  如果输出参数$y_i$有$N$个时，其<strong>似然函数</strong>为：$$L(\theta|Y) = \prod_{i=1}^{N}P(y_i|\theta)$$</p><hr><h3 id="极大似然估计-MLE"><a href="#极大似然估计-MLE" class="headerlink" title="极大似然估计(MLE):"></a>极大似然估计(MLE):</h3><p>为了求解到最优的统计模型参数，我们需要极大化<strong>似然函数</strong>，并取此时的$\theta$为我们的模型参数，即：</p><p>$$\theta = argmax_{\theta}L(\theta|Y)$$</p><hr><h3 id="对数似然函数-LLD-："><a href="#对数似然函数-LLD-：" class="headerlink" title="对数似然函数(LLD)："></a>对数似然函数(LLD)：</h3><p>  对于上面的普通的<strong>似然函数</strong>虽然很容易理解，但却不方便计算(特别是对于计算机来说)，因为当已知数据$y_i$数量很大时，$P(y_i|\theta) &lt; 1$ 从而使得 $L(\theta|Y) &lt;&lt; 1$，甚至会突破计算机对浮点数的存储极限。因此，我们将<strong>似然函数</strong>取对数，从而得到<strong>对数似然函数</strong>，即：</p><p>$$LL(\theta|Y) = log(L(\theta|Y))$$</p><p><strong>对数似然函数</strong>不仅满足了计算机的存储问题，而且对数函数是<strong>单调增函数</strong>，所以<strong>似然函数</strong>的数学性质不会改变，也满足我们的计算需求。</p><hr><h2 id="EM算法的导出"><a href="#EM算法的导出" class="headerlink" title="EM算法的导出"></a>EM算法的导出</h2><h3 id="隐变量"><a href="#隐变量" class="headerlink" title="隐变量"></a>隐变量</h3><p>**隐变量(latent variable)**即潜在变量，是模型中无法被观测到却又在理论上存在的中间变量。例如，有以下模型：假设有三枚硬币，分别记作A、B、C。这些硬币正面出现的概率分别为 $\pi、p、q$。进行如下试验：先掷硬币A，如果A正面向上，选B硬币；如果A反面向上，选C硬币；然后记录B或C硬币投掷的结果 $y$。</p><p>在上述试验中：$y$ 为观测变量， $\pi、p、q$ 为模型参数，<strong>投掷A硬币的结果即为隐变量</strong>。</p><h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><p>对于含有<strong>隐变量</strong>且参数未知的概率模型，我们的优化目标就是极大化观测数（不完全数据）$Y$关于参数$\theta$的<strong>对数似然函数</strong>，即极大化：$$L(\theta) = logP(Y|\theta) = log\sum_ZP(Y,Z|\theta)$$</p><p>$$=log\Big(\sum_ZP(Y|Z,\theta)P(Z|\theta)\Big)$$</p><p>对上式求极大化的困难主要在于，上式存在未观测变量$Z$</p><p>而<strong>EM算法</strong>并没有硬求为观测变量，它是通过迭代计算的方法逐步极大化$L(\theta)$。因此，我们假设第$i$次迭代后概率模型参数的估计值为$\theta^{(i)}$。我们希望新的估计值 $\theta$ 能使$L(\theta)$ 增加，即 $L(\theta) &gt; L(\theta^{(i)})$ ，为此，我们考虑两者之差：</p><p>$$L(\theta) - L(\theta^{(i)}) = log(\sum_zP(Y|Z,\theta)P(Z|\theta)) - logP(Y|\theta^{(i)})$$</p><p>然后利用<strong>Jensen 不等式</strong>($log\sum_j\lambda_jy_j\ge\sum_j\lambda_jlogy_j , \lambda_j\ge0,\sum_j\lambda_j=1$，这样我们得到下面的放缩：</p><p>$$L(\theta) - L(\theta^{(i)})=log\sum_{\small{Z}}P(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})})-logP(Y|\theta^{(i)})$$</p><p>$$\ge\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-logP(Y|\theta^{(i)})$$</p><p>$$ = \sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$$</p><p>令$$B(\theta,\theta^{(i)})=L(\theta,\theta^{(i)})+\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$$</p><p>则有：</p><p>$$L(\theta)\ge B(\theta,\theta^{(i)})$$</p><p>即说明函数 $B(\theta,\theta^{(i)})$ 是函数 $L(\theta)$ 的一个下界，而且由上式可得：</p><p>$$L(\theta^{(i)})=B(\theta^{(i)},\theta^{(i)})$$</p><p>因此，任何可以使 $B(\theta,\theta^{(i)})$ 增大的 $\theta$ ，都可以使 $L(\theta)$ 增大。为了使 $L(\theta)$ 有尽可能大的增长，我们选择 $\theta^{(i+1)}$,即：</p><p>$$L(\theta^{(i+1)})=argmax_{\small{\theta}}B(\theta,\theta^{(i)})$$</p><p>但现在 $\theta^{(i+1)}$  是未知的，所以我们需要推导出 $\theta^{(i+1)}$ 的表达式，如下：</p><p>$$\large{\theta^{(i+1)}}=argmax_{\small{\theta}}\Big(L(\theta^{(i)})+\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\Big)$$</p><p>[因为$L(\theta^{(i)}) , P(Z|Y,\theta^{(i)}),  P(Y|\theta^{(i)})$ , 与自变量 $\theta$ 无关，故删除]，得到：</p><p>$$=argmax_\theta\bigg(\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\Big(P(Y|Z,\theta)P(Z|\theta)\Big)\bigg)$$</p><p>$$=argmax_\theta\Big(\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log(P(Y,Z|\theta))\Big)$$</p><p>[令其为Q函数]，得到：</p><p>$$=argmax_{\theta}Q(\theta,\theta^{(i)})$$</p><h3 id="Q函数"><a href="#Q函数" class="headerlink" title="Q函数"></a>Q函数</h3><hr><p>完全数据的对数似然函数 $logP(Y,Z|\theta)$ 关于在给定观测数据 $Y$ 和当前参数 $\theta^{(i)}$ 下对未知观测数据 $Z$ 的条件概率分布 $P(Z|Y,\theta^{(i)})$ 的期望，称为<strong>Q函数</strong>：</p><p>$$Q(\theta,\theta^{(i)})=E_{\small{Z}}[logP(Y,Z|\theta)|Y,\theta^{(i)}]$$</p><p>$$=\sum_{\small{Z}}log\big(P(Y,Z|\theta)\big)P(Z|Y,\theta^{(i)})$$</p><hr><p>由于 $P(Y,Z|\theta)$ 我们可以通过原始数据求的，因此我们便得出最终的优化目标，即<strong>最大化Q函数</strong>，下面是<strong>EM算法</strong>的步骤：</p><h3 id="EM算法步骤"><a href="#EM算法步骤" class="headerlink" title="EM算法步骤"></a>EM算法步骤</h3><p>输入：观测变量数据<strong>Y</strong>，隐变量数据<strong>Z</strong>，联合分布**$P(Y,Z|\theta)$**，条件分布$P(Z|Y,\theta)$</p><p>输出：模型参数 $\theta$</p><ol><li><p>选择参数的初值 $\theta^{(0)}$ ，开始迭代；</p></li><li><p><strong>E步</strong>：记 $\theta^{(i)}$，为第 i 次迭代的参数 $\theta$ 的估计值，在第 i+1 次迭代的E步，计算：</p><p>$$Q(\theta,\theta^{(i)})=\sum_{\small{Z}}log\big(P(Y,Z|\theta)\big)P(Z|Y,\theta^{(i)})$$</p><p>【这里，$P(Z|Y,\theta^{(i)})$ 是在给定观测数据 $Y$ 和当前的参数估计 $\theta^{(i)}$ 下隐变量 $Z$ 的条件概率分布】</p></li><li><p><strong>M步</strong>：求使 $Q(\theta,\theta^{(i)})$ 极大化的 $\theta$，确定第 i+1 次迭代的参数估计值 $\theta^{(i+1)}$：</p><p>$$\theta^{(i+1)}=argmax_\theta Q(\theta,\theta^{(i)})$$</p></li><li><p>重复 2、3 步，直到收敛。</p></li></ol><h2 id="EM算法的收敛性证明"><a href="#EM算法的收敛性证明" class="headerlink" title="EM算法的收敛性证明"></a>EM算法的收敛性证明</h2><p><strong>定理 1</strong>：设 $P(Y|\theta)$ 为观测数据的似然函数，$\theta^{(i)}(i=1,2,…)$ 为<strong>EM算法</strong>得到的<strong>参数估计序列</strong>，$P(Y|\theta^{(i)})(i=1,2,…)$ 为对应的似然函数序列，则 $P(Y|\theta^{(i)})$ 是单调递增的，即</p><p>$$P(Y|\theta^{(i+1)})\ge P(Y|\theta^{(i)})$$     <font color="red">(*) </font></p><p><strong>证明</strong>：</p><p>由于【根据概率公式】：</p><p>$$P(Y|\theta)=\frac{P(Y,Z|\theta)}{P(Z|Y,\theta)}$$</p><p>两边取对数得：</p><p>$$logP(Y|\theta)=logP(Y,Z|\theta)-logP(Z|Y,\theta)$$</p><p>由Q函数定义，即：</p><p>$$Q(\theta,\theta^{(i)})=\sum_{\small{Z}}logP(Y,Z|\theta)P(Z|Y,\theta^{(i)})$$</p><p>令：</p><p>$$H(\theta,\theta^{(i)})=\sum_{\small{Z}}logP(Z|Y,\theta)P(Z|Y,\theta^{(i)})$$    <font color="red">(1)</font></p><p>于是上面得<strong>对数似然函数</strong>可以写成：</p><p>$$logP(Y|\theta)=Q(\theta,\theta^{(i)})-H(\theta,\theta^{(i)})$$</p><p>上式中的 $\theta$ 分别取 $\theta^{(i)}$ 和 $\theta^{(i+1)}$ 并相减，得到：</p><p>$$log(P(Y|\theta^{(i+1)}))-log(P(Y|\theta^{(i)}))$$</p><p>$$= [Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)}),\theta^{(i)}]-[H(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i)},\theta^{(i)})]$$    <font color="red">(*&#39;)</font></p><p>为了证明<font color="red">(*)</font>式，只需证明上式的右边是非负的。<em>上式右边第一项</em>，因为 $\theta^{(i+1)}$ 使 $Q(\theta,\theta^{(i)})$ 达到极大，所以有：</p><p>$$Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})\ge0$$    <font color="red">(2)</font></p><p><em>上式右边第二项</em>，由 <font color="red">(1)</font> 式得：</p><p>$$H(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i)},\theta^{(i)})$$</p><p>$$=\sum_{\small{Z}}\Big(log\frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})}\Big)P(Z|Y,\theta^{(i)})$$</p><p>下面由<strong>Jensen不等式</strong>得：</p><p>$$\le log\Big(\sum_{\small{Z}}\frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})}P(Z|Y,\theta^{(i)})\Big)$$</p><p>$$=log\Big(\sum_{\small{Z}}P(Z|Y,\theta^{(i+1)})\Big)=0$$    <font color="red">(3)</font></p><p>由<font color="red">(*&#39;)  (2) (3) </font>得，<font color="red">(*&#39;)</font>的左式是非负的，所以 <font color="red">(*)</font> 式得证。</p><hr><p><strong>定理 2</strong>：设 $L(\vec\theta)=logP(Y|\vec\theta)$ 为观测数据的对数似然函数，$\vec\theta^{(i)}(i=1,2,…)$ 为<strong>EM算法</strong>得到的<strong>参数估计序列</strong>，$L(\vec\theta^{(i)})=logP(Y|\vec\theta^{(i)})$ 为对应的对数似然函数序列。</p><ol><li>如果 $P(Y|\vec\theta)$ 有上界，则 $L(\vec\theta^{(i)})=logP(Y|\vec\theta^{(i)})$ 收敛到某一值 $L^*$ ;</li><li>在函数 $Q(\theta,\theta^{‘})$ 与 $L(\theta)$ 满足一定条件下，由<strong>EM</strong>算法得到的参数估计序列 $\vec\theta^{(i)}$ 的收敛值 $\vec\theta^{*}$ 是 $L(\vec\theta)$ 的稳定点</li></ol><p><strong>证明</strong>：</p><ol><li>由 $L(\vec\theta)=logP(Y|\vec\theta^{(i)})$ 的单调性及 $P(Y|\vec\theta)$ 的有界性得到，【<strong>单调有界原理</strong>】</li><li>证明<strong>略</strong>.</li></ol>]]></content>
    
    
    <summary type="html">An introduction to the Expectation-Maximization algorithm</summary>
    
    
    
    
    <category term="Machine Learning" scheme="https://1.15.86.100/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>markdown-doc</title>
    <link href="https://1.15.86.100/2019/10/17/markdown-doc/"/>
    <id>https://1.15.86.100/2019/10/17/markdown-doc/</id>
    <published>2019-10-17T15:34:13.000Z</published>
    <updated>2019-10-24T15:37:38.163Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MarkDown"><a href="#MarkDown" class="headerlink" title="MarkDown"></a>MarkDown</h1><h2 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h2><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><table><thead><tr><th align="center"><center>字母</center></th><th align="center"><center>写法(首字母大写即对应大写)</center></th></tr></thead><tbody><tr><td align="center"><center>$\alpha$</center></td><td align="center"><center>\alpha</center></td></tr><tr><td align="center"><center>$\beta$</center></td><td align="center"><center>\beta</center></td></tr><tr><td align="center"><center>$\gamma$</center></td><td align="center"><center>\gamma</center></td></tr><tr><td align="center"><center>$\delta$</center></td><td align="center"><center>\delta</center></td></tr><tr><td align="center"><center>$\epsilon$</center></td><td align="center"><center>\epsilon</center></td></tr><tr><td align="center"><center>$\varepsilon$</center></td><td align="center"><center>\varepsilon</center></td></tr><tr><td align="center"><center>$\zeta$</center></td><td align="center"><center>\zeta</center></td></tr><tr><td align="center"><center>$\eta$</center></td><td align="center"><center>\eta</center></td></tr><tr><td align="center"><center>$\theta$</center></td><td align="center"><center>\theta</center></td></tr><tr><td align="center"><center>$\iota$</center></td><td align="center"><center>\iota</center></td></tr><tr><td align="center"><center>$\kappa$</center></td><td align="center"><center>\kappa</center></td></tr><tr><td align="center"><center>$\lambda$</center></td><td align="center"><center>\lambda</center></td></tr><tr><td align="center"><center>$\mu$</center></td><td align="center"><center>\mu</center></td></tr><tr><td align="center"><center>$\nu$</center></td><td align="center"><center>\nu</center></td></tr><tr><td align="center"><center>$\xi$</center></td><td align="center"><center>\xi</center></td></tr><tr><td align="center"><center>$\omicron$</center></td><td align="center"><center>\omicron</center></td></tr><tr><td align="center"><center>$\pi$</center></td><td align="center"><center>\pi</center></td></tr><tr><td align="center"><center>$\rho$</center></td><td align="center"><center>\rho</center></td></tr><tr><td align="center"><center>$\sigma$</center></td><td align="center"><center>\sigma</center></td></tr><tr><td align="center"><center>$\tau$</center></td><td align="center"><center>\tau</center></td></tr><tr><td align="center"><center>$\upsilon$</center></td><td align="center"><center>\upsilon</center></td></tr><tr><td align="center"><center>$\phi$</center></td><td align="center"><center>\phi</center></td></tr><tr><td align="center"><center>$\varphi$</center></td><td align="center"><center>\varphi</center></td></tr><tr><td align="center"><center>$\chi$</center></td><td align="center"><center>\chi</center></td></tr><tr><td align="center"><center>$\psi$</center></td><td align="center"><center>\psi</center></td></tr></tbody></table><h3 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h3><ol><li>行内公式：\$xyz\$ , 显示：$xyz$</li><li>独行公式：\$\$xyz\$\$ , 显示：$$xyz$$</li></ol><h3 id="上标和下标以及组合"><a href="#上标和下标以及组合" class="headerlink" title="上标和下标以及组合"></a>上标和下标以及组合</h3><ol><li>上标：^ , $x^2$ , 显示：$x^3$</li><li>下标：_ , $x_i$ , 显示：$x_i$</li><li>组合：{} , ${16}<em>{18}{2+}</em>{2}$ , 显示：${16}<em>{18}{2+}</em>{2}$</li></ol><h3 id="字母大小"><a href="#字母大小" class="headerlink" title="字母大小"></a>字母大小</h3><ol><li>\tiny : $\tiny{abc}$</li><li>\small : $\small{abc}$</li><li>\normalsize : $\normalsize{abc}$</li><li>\large : $\large{abc}$</li><li>\Large : $\Large{abc}$</li><li>\LARGE : $\LARGE{abc}$</li></ol><h3 id="分式"><a href="#分式" class="headerlink" title="分式"></a>分式</h3><ol><li>\dfrac{}{} : 表示该分式是以 displaystyle 设置的，例如： $\dfrac{abc}{xyz}$</li><li>\tfrac{}{} : 表示该分式是以 textstyle 设置的，例如：$\tfrac{abc}{xyz}$</li><li>\frac{}{} : 表示该分式根据环境设置样式，例如：$\frac{abc}{xyz}$</li><li>{}\over{} : 分式的另一种形式，例如：${abc}\over{xyz}$</li></ol><h3 id="根式"><a href="#根式" class="headerlink" title="根式"></a>根式</h3><ol><li>\sqrt{} : 二次根式，例如：$\sqrt{abc}$</li><li>\sqrt[n]{} : n次根式，例如：$\sqrt[abc]{xyz}$</li><li>\mathstrut : 使连根式变得整齐，例如：$\sqrt{\mathstrut a} + \sqrt{\mathstrut b} + \sqrt{\mathstrut c}$</li></ol><h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><ol><li>\vec{} : 矢量箭头，例如：$\vec{xyz}$</li><li>\overrightarrow{} : 右箭头，例如：$\overrightarrow{xyx}$</li><li>\overleft arrow{} : 左箭头，例如：$\overleftarrow{xyz}$</li><li>\overleftrightarrow{} : 双箭头，例如：$\overleftrightarrow{xyz}$</li></ol><h3 id="括号"><a href="#括号" class="headerlink" title="括号"></a>括号</h3><ol><li>\big() : 小括号，例如：$\big(xyz)$</li><li>\Big() : 大一点的小括号，例如：$\Big(xyz)$</li></ol><h2 id="文本"><a href="#文本" class="headerlink" title="文本"></a>文本</h2><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ol><li># 一级标题</li><li>## 二级标题</li><li>### 三级标题</li><li>#### 四级标题</li><li>##### 五级标题</li><li>###### 六级标题</li></ol><h3 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h3><ol><li>加粗：**加粗**，<strong>加粗</strong></li><li>斜体：*斜体*，<em>斜体</em></li><li>斜体加粗：***斜体加粗***，<em><strong>斜体加粗</strong></em></li><li>删除线：~~删除线~~，<del>删除线</del></li></ol><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ol><li>一层引用：&gt;</li><li>二层引用：&gt;&gt;</li><li>多层引用：&gt;&gt;&gt;&gt;</li></ol><blockquote><blockquote><blockquote><blockquote><p>引用</p></blockquote></blockquote></blockquote></blockquote><h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p>三个以上---、三个以上***</p><hr><hr><h3 id="序号"><a href="#序号" class="headerlink" title="序号"></a>序号</h3><h4 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h4><p>使用-，+，* 任意一种都可以，可以嵌套：</p><ol><li>- 列表内容</li><li>+ 列表内容</li><li>* 列表内容</li></ol><p>效果：</p><ul><li>列表内容<ul><li>列表内容<ul><li>列表内容<ul><li>列表内容</li></ul></li></ul></li></ul></li></ul><h4 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h4><p>数字加点(英文)：</p><p>例如：1.</p><p>效果：</p><ol><li>效果</li><li>效果</li></ol><h2 id="插入资源"><a href="#插入资源" class="headerlink" title="插入资源"></a>插入资源</h2><h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><p>![图片名称](图片地址)</p><p><img src="/2019/10/17/markdown-doc/picture.png" alt="picture"></p><h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p>[超链接名](超链接地址)</p><p>例如：<a href="https://vilily.github.io/">myblog</a></p>]]></content>
    
    
    <summary type="html">An introduction to markdown and it&#39;s methods</summary>
    
    
    
    
    <category term="markdown" scheme="https://1.15.86.100/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>Hexo-Learn</title>
    <link href="https://1.15.86.100/2019/10/16/Hexo-Learn/"/>
    <id>https://1.15.86.100/2019/10/16/Hexo-Learn/</id>
    <published>2019-10-16T15:35:05.000Z</published>
    <updated>2019-10-25T14:52:12.423Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hexo-Learning"><a href="#Hexo-Learning" class="headerlink" title="Hexo Learning"></a><center>Hexo Learning</center></h1><h2 id="Change-Theme"><a href="#Change-Theme" class="headerlink" title="Change Theme"></a>Change Theme</h2><ol><li><p>Get open open source <a href="https://hexo.io/themes/">theme</a>.</p></li><li><p>Clone the files of the themes in <a href="https://github.com/">Github</a> to the theme folder.</p></li><li><p>Open the (_config.yml)file which is a file to store the information of sets.</p><p><img src="/2019/10/16/Hexo-Learn/test.png" alt="config.yml"></p></li><li><p>run code [hexo generate]</p></li><li><p>run code [hexo deploy]</p></li></ol><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p><h2 id="Add-Tags"><a href="#Add-Tags" class="headerlink" title="Add Tags"></a>Add Tags</h2><p>tags: [tags1, tags 2, …]</p>]]></content>
    
    
    <summary type="html">My first post, something about hexo.</summary>
    
    
    
    
    <category term="Hexo" scheme="https://1.15.86.100/tags/Hexo/"/>
    
  </entry>
  
</feed>
