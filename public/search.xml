<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>组合数学-排列组合</title>
      <link href="/2021/08/11/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6-%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88/"/>
      <url>/2021/08/11/%E7%BB%84%E5%90%88%E6%95%B0%E5%AD%A6-%E6%8E%92%E5%88%97%E7%BB%84%E5%90%88/</url>
      
        <content type="html"><![CDATA[<h1 id="排列组合"><a href="#排列组合" class="headerlink" title="排列组合"></a>排列组合</h1><h2 id="多重排列"><a href="#多重排列" class="headerlink" title="多重排列"></a>多重排列</h2><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>对于若干个元素，<script type="math/tex">r_1</script>个1，<script type="math/tex">r_2</script>个2，……<script type="math/tex">r_t</script>个t，元素个数之和为<strong>n</strong>，即<script type="math/tex">\sum_{i=1}^{t}r_i = n</script>​，那么其**全排列记为：<script type="math/tex">P(n;r_1,r_2,\cdots,r_t)</script>​。​</p><h4 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h4><script type="math/tex; mode=display">P(n;r_1,r_2,\cdots,r_t)=\frac{n!}{r_1!r_2!\cdots r_t!}</script><h2 id="无重组合"><a href="#无重组合" class="headerlink" title="无重组合"></a>无重组合</h2><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>从 <strong>n</strong> 个不同的元素中，取 <strong>r</strong> 个不重复的元素，组成一个子集，不考虑顺序，记为：<script type="math/tex">C(n,r)</script>​</p><h4 id="计算公式-1"><a href="#计算公式-1" class="headerlink" title="计算公式"></a>计算公式</h4><script type="math/tex; mode=display">C(n,r)=\frac{n!}{(n-r)!r!}</script><h2 id="无重排列"><a href="#无重排列" class="headerlink" title="无重排列"></a>无重排列</h2><h4 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h4><p>从 <strong>n</strong> 个不同的元素中，取 <strong>r</strong> 个不重复的元素，按次序排列，记为：<script type="math/tex">P(n,r)</script></p><h4 id="计算公式-2"><a href="#计算公式-2" class="headerlink" title="计算公式"></a>计算公式</h4><script type="math/tex; mode=display">P(n,r)=\frac{n!}{(n-r)!}</script><h2 id="可重组合"><a href="#可重组合" class="headerlink" title="可重组合"></a>可重组合</h2><h4 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h4><p>从n个不同的元素中，取r个元素，可以重复选取，记为：<script type="math/tex">\bar{C}(n,r)</script>​</p><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>取 <script type="math/tex">r</script>​ 个无标志的球，放入 <script type="math/tex">n</script>​​ 个有区别的盒子中，可以放0个或多余1个。</p><h4 id="计算公式-3"><a href="#计算公式-3" class="headerlink" title="计算公式"></a>计算公式</h4><script type="math/tex; mode=display">\bar{C}(n,r)=C(n+r-1, r)</script><h2 id="可重排列"><a href="#可重排列" class="headerlink" title="可重排列"></a>可重排列</h2><h4 id="定义-4"><a href="#定义-4" class="headerlink" title="定义"></a>定义</h4><p>从n个不同的元素中，取r个进行排列，可以重复选取，记为：<script type="math/tex">P(n,r)</script></p><h4 id="计算公式-4"><a href="#计算公式-4" class="headerlink" title="计算公式"></a>计算公式</h4><script type="math/tex; mode=display">P(n,r)=n^r</script><h2 id="不相邻组合"><a href="#不相邻组合" class="headerlink" title="不相邻组合"></a>不相邻组合</h2><h4 id="定义-5"><a href="#定义-5" class="headerlink" title="定义"></a>定义</h4><p>从<script type="math/tex">A=\{1,2,3,\cdots,n\}</script>中取<strong>r</strong>个不相邻的数进行组合，即不存在相邻的两个数<script type="math/tex">j,j+1</script>​的组合。</p><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><script type="math/tex; mode=display">C(n-r+1, r)</script><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>设<script type="math/tex">B=\{b_1,b_2,\cdots,b_r\}</script>是一组不相邻的组合；</p><p>让<strong>B</strong>序列元素分别减去其序号，构造<strong>C</strong>序列：<script type="math/tex">C=\{b_1-0,b_2-1,\cdots,b_r-(r-1)\}</script>​;</p><p>因为，<strong>B</strong>是不相邻的组合，所以：</p><script type="math/tex; mode=display">b_i<b_{i+1} - 1\to b_i-i+1<b_{i+1}-i+2</script><p>所以：</p><script type="math/tex; mode=display">c_1<c_2<\cdots c_r</script><p>又因为：<script type="math/tex">c_1=b_1-0\ge 1,c_r=b_r-r+1\le n-r+1</script></p><p>所以，组合数为：<script type="math/tex">C(n-r+1, r)</script>​</p><h4 id="REF"><a href="#REF" class="headerlink" title="REF"></a>REF</h4><p><a href="https://www.bilibili.com/video/BV1vZ4y1j7gf?p=22&amp;spm_id_from=pageDriver">【学堂在线】组合数学_哔哩哔哩_bilibili</a></p>]]></content>
      
      
      <categories>
          
          <category> 组合数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 排列组合 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SG函数</title>
      <link href="/2021/07/14/SG%E5%87%BD%E6%95%B0/"/>
      <url>/2021/07/14/SG%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="SG-Sprague–Grundy-函数"><a href="#SG-Sprague–Grundy-函数" class="headerlink" title="SG(Sprague–Grundy)函数"></a>SG(Sprague–Grundy)函数</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在<strong>博弈论</strong>中，任何一个<strong>公平组合游戏</strong>都可以通过把每个<strong>局面（状态）</strong>看成一个<strong>顶点</strong>，对每个局面和它的子局面连一条有向边来抽象成一个<strong>有向图</strong>，游戏过程就是状态沿着顶点移动的过程，而<strong>SG函数</strong>就是定义在该<strong>有向图</strong>上的函数。</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p><strong>组合游戏</strong>需要以下条件：</p><ul><li><p>游戏有且只有<strong>两个玩家</strong>；</p></li><li><p>游戏由两个玩家<strong>交替执行步骤</strong>，并且两位玩家都掌握<strong>完全信息</strong>。</p></li><li><p>游戏满足<strong>结束条件（ending condition）</strong>：</p><ul><li>所有的状态最终都会达到结束；</li><li>所有转移的步骤是有限的；</li></ul></li><li><p>游戏满足<strong>普通条件（normal play condition）</strong>：</p><p>  <strong>不能移动</strong>的玩家<strong>失败</strong>；</p></li></ul><h4 id="必胜点（P点）"><a href="#必胜点（P点）" class="headerlink" title="必胜点（P点）"></a>必胜点（P点）</h4><p>任何处于该点的玩家最终必胜；</p><h4 id="必败点（N点）"><a href="#必败点（N点）" class="headerlink" title="必败点（N点）"></a>必败点（N点）</h4><p>任何处于该点的玩家最终必败；</p><h4 id="相关性质"><a href="#相关性质" class="headerlink" title="相关性质"></a>相关性质</h4><ol><li>所有终结点是<strong>必败点</strong>；</li><li>从任何<strong>必胜点</strong>操作，至少有一种方式可以进入<strong>必败点</strong>；</li><li>无论如何操作，<strong>必败点</strong>只能进入<strong>必胜点</strong>；</li></ol><h3 id="SG函数"><a href="#SG函数" class="headerlink" title="SG函数"></a>SG函数</h3><h4 id="mex-minimal-excludant-运算"><a href="#mex-minimal-excludant-运算" class="headerlink" title="mex(minimal excludant)运算"></a>mex(minimal excludant)运算</h4><p>在集合上的一个运算，表示<strong>最小</strong>的<strong>不属于</strong>这个集合的<strong>非负整数</strong>。</p><p>例如：</p><ul><li>mex{0, 1, 2, 4} = 3；</li><li>mex{2, 4} = 0；</li><li>mex{} = 0；</li></ul><h4 id="SG-x-函数"><a href="#SG-x-函数" class="headerlink" title="SG(x)函数"></a>SG(x)函数</h4><p>对于任意状态<strong>x</strong>，<strong>SG(x) = mex(S)</strong>。</p><p>其中，<strong>S</strong>表示<strong>x</strong>后继状态的<strong>SG</strong>函数值的集合。</p><p>当<strong>SG函数值=0</strong>时，当前先手玩家必败，否则先手玩家必胜。</p><h4 id="取石子问题"><a href="#取石子问题" class="headerlink" title="取石子问题"></a>取石子问题</h4><blockquote><p>现在你有 <strong>n</strong> 个石子，每次可以取走 <script type="math/tex">a(a\in\{1,2\})</script>​​ 个，两个人轮流取，谁取完了谁就获胜，问先手是否必胜。</p></blockquote><p>首先，0个石子肯定是<strong>必败点</strong>，所以有：<script type="math/tex">SG(0)=0</script>；</p><p>有1个石子时，后继状态只有0，所以有：<script type="math/tex">SG(1) = mex\{SG(0)\}=1</script>；</p><p>有2个石子时，后继状态有0，1，所以有：<script type="math/tex">SG(2)=mex\{SG(1),SG(0)\}=2</script>​</p><p>有3个石子时，后继状态有1，2，所以有：<script type="math/tex">SG(3)=mex\{SG(1),SG(2)\}=0</script></p><p>有4个石子时，后继状态有2，3，所以有：<script type="math/tex">SG(4)=mex\{SG(2),SG(3)\}=1</script>​</p><p>有5个石子时，后继状态有3，4，所以有：<script type="math/tex">SG(5)=mex\{SG(3),SG(4)\}=2</script></p><p>有6个石子时，后继状态有4，5，所以有：<script type="math/tex">SG(6)=mex\{SG(4),SG(5)\}=0</script></p><p>……</p><p>容易发现：<script type="math/tex">SG(x)>0,if\ x>0</script>​，所以<strong>不一定先手必胜</strong>。</p><h3 id="证明（SG-0必败，否则必胜的证明）"><a href="#证明（SG-0必败，否则必胜的证明）" class="headerlink" title="证明（SG=0必败，否则必胜的证明）"></a>证明（SG=0必败，否则必胜的证明）</h3><p>首先，对于<strong>终结点</strong> <script type="math/tex">x</script>，此使令<script type="math/tex">SG(x)=0</script>，且必定属于<strong>必败点</strong>；</p><p>对于 <script type="math/tex">SG(x)= 0</script>​ 的<strong>非终结点</strong> <script type="math/tex">x</script>​：</p><ul><li>因为 <script type="math/tex">SG(x) =0</script>，所以，其<strong>子状态</strong>的<strong>SG值</strong>都不为0，即 <script type="math/tex">SG(y)\neq 0,if\ y\in\ sub(x)</script>；</li><li>而，对于<script type="math/tex">SG(y)\neq 0</script> 的状态 <script type="math/tex">y</script>​，其<strong>子状态集合</strong>中必包含<strong>SG值</strong>为0的状态，即 <script type="math/tex">\exist z,SG(z)=0,if\ z\in\ sub(y)</script>；</li><li>所以，处于 <script type="math/tex">x</script>​ 状态时，先手任意选择，后手选择<strong>到达 <script type="math/tex">z</script>状态</strong>的操作即可到达另一使先手<strong>SG值</strong>为0的状态；</li><li>又，该<strong>游戏图</strong>是<strong>有向无环图</strong>，且<strong>叶结点（终结点）</strong>都为必败点，所以<strong>SG=0</strong>的点必为必败点；</li></ul><p>对于 <script type="math/tex">SG(x) \neq 0</script>的点 <script type="math/tex">x</script>：</p><ul><li>其<strong>子状态集合</strong>中必包含<strong>SG值</strong>为0的状态，即 <script type="math/tex">\exist y,SG(y)=0,if\ y\in\ sub(x)</script>​​；</li><li>选择可以到达<strong>y</strong>状态的路径即可；</li></ul><h3 id="SG定理"><a href="#SG定理" class="headerlink" title="SG定理"></a>SG定理</h3><blockquote><p>游戏和的<strong>SG函数</strong>等于各个游戏SG函数值的<strong>Nim和</strong>。这样就可以将每一个子游戏<strong>分而治之</strong>，从而简化了问题。</p></blockquote><h4 id="Nim和"><a href="#Nim和" class="headerlink" title="Nim和"></a>Nim和</h4><p>将各个数<strong>异或</strong>的结果。</p><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><ul><li>设 <strong>#S</strong> 是所有子游戏的<strong>Nim和</strong>；</li><li>假设 <strong>#S</strong> 不为0，且最高位为1（假设为第 <strong>k</strong>位）的对应的其中一个子游戏的<strong>SG值</strong>为 <strong>#s</strong>；</li><li>令 <strong>#A</strong> 表示除去 <strong>#s</strong> 后，剩下的所有子游戏的 <strong>Nim和</strong>，则有：<strong>#S = #s ^ #A</strong>；</li><li>令 <strong>#a = #s ^ #S</strong>，既该轮的先手将一个子游戏的<strong>SG值</strong>由<strong>#s</strong>变为<strong>#a</strong>，因为<strong>#S，#s</strong>第 <strong>k</strong>位都为1，所以异或后为0，所以<strong>#a &lt; #s</strong>；</li><li>则有：<strong>#A ^ #a = #A ^ #s ^ #S = #A ^ #s ^ #s ^#A = 0</strong>；</li><li>综上如果 <strong>#S</strong> 不为0，先手总可以使 <strong>#S</strong>变为0，并且各堆的<strong>SG值</strong>是单调递减的，所以最终后手局时各堆 <strong>SG值</strong>都变为0，先手必胜；</li><li>同上如果 <strong>#S</strong>为0，先手必输；</li></ul><p><strong><a href="http://1.15.86.100:9000/Dilworth.pdf">REF</a></strong></p>]]></content>
      
      
      <categories>
          
          <category> 组合数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正则表达式</title>
      <link href="/2021/06/05/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
      <url>/2021/06/05/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><h2 id="字符"><a href="#字符" class="headerlink" title="字符"></a>字符</h2><div class="table-container"><table><thead><tr><th>写法</th><th>描述</th></tr></thead><tbody><tr><td>[0-9]</td><td>匹配任何数字</td></tr><tr><td></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>卡特兰数</title>
      <link href="/2021/05/26/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/"/>
      <url>/2021/05/26/%E5%8D%A1%E7%89%B9%E5%85%B0%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="卡特兰数"><a href="#卡特兰数" class="headerlink" title="卡特兰数"></a>卡特兰数</h1><h2 id="引入"><a href="#引入" class="headerlink" title="引入"></a>引入</h2><p>将<script type="math/tex">1,2,\cdots,N</script>，<strong>N</strong>个数依次进行入栈出栈操作，求输出的<strong>不同排列顺序的序列</strong>的个数。</p><p>假设<script type="math/tex">f(x)</script>表示<strong>x</strong>个数的序列个数，则对于<strong>N</strong>个数的数列，设<strong>i</strong>是最后出栈的数字，则比<strong>i</strong>小的数有<strong>i-1</strong>个，比<strong>i</strong>大的数有<strong>N-i</strong>个，所以<strong>递归公式</strong>为：</p><script type="math/tex; mode=display">f(N)=\sum_{i=1}^{N}f(i-1)\cdot f(N-i) \tag{*}</script><h2 id="递推公式"><a href="#递推公式" class="headerlink" title="递推公式"></a>递推公式</h2><script type="math/tex; mode=display">f(N)=\frac{4N-2}{n+1}f(N-1)</script><h2 id="通项公式"><a href="#通项公式" class="headerlink" title="通项公式"></a>通项公式</h2><script type="math/tex; mode=display">f(N)=\frac{C_{2N}^N}{N+1}</script>]]></content>
      
      
      <categories>
          
          <category> 组合数学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>RSA</title>
      <link href="/2021/05/25/RSA/"/>
      <url>/2021/05/25/RSA/</url>
      
        <content type="html"><![CDATA[<h1 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h1><p><strong>RSA</strong>是一种<strong>非对称加密</strong>方法。1977年由<em>Ron Rivest</em>、<em>Adi Shamir</em>和<em>Leonard Adleman</em>一起提出的。当时他们三人都在。RSA就是他们三人姓氏开头字母拼在一起组成的。</p><h3 id="欧拉函数-phi​"><a href="#欧拉函数-phi​" class="headerlink" title="欧拉函数\phi​"></a>欧拉函数<script type="math/tex">\phi</script>​</h3><p>欧拉函数<script type="math/tex">\phi(n)</script>表示从<script type="math/tex">1</script>到<script type="math/tex">n</script>中和<script type="math/tex">n</script>​互质的自然数的个数。</p><p>根据<strong>质数性质</strong>，有以下结论：</p><ul><li><script type="math/tex">\phi(1)=1</script>，因为<script type="math/tex">1</script>和<strong>任何数</strong>互质；</li><li><script type="math/tex">\phi(n)=n-1</script>，<script type="math/tex">n</script>为质数时，<script type="math/tex">n</script>和<script type="math/tex">1\cdots n-1</script>互质；</li></ul><h3 id="欧拉定理"><a href="#欧拉定理" class="headerlink" title="欧拉定理"></a>欧拉定理</h3><p>如果两个正整数 <script type="math/tex">a</script> 和 <script type="math/tex">n</script> <strong>互质</strong>，则 <script type="math/tex">n</script> 的<strong>欧拉函数</strong> <script type="math/tex">\phi(n)</script> 可以让下面的等式成立：</p><script type="math/tex; mode=display">a^{\phi(n)}=1(\mod{n})</script><h2 id="RSA算法"><a href="#RSA算法" class="headerlink" title="RSA算法"></a>RSA算法</h2><h3 id="密钥生成"><a href="#密钥生成" class="headerlink" title="密钥生成"></a>密钥生成</h3><ul><li>随机选择两个<strong>不等</strong>的<strong>质数</strong> <script type="math/tex">p</script> 和 <script type="math/tex">q</script>；</li><li>计算 <script type="math/tex">p,q</script> 的<strong>乘积</strong> <script type="math/tex">n</script>​；【<strong>n</strong>做为<strong>公钥</strong>】</li><li>根据公式计算 <script type="math/tex">\phi(n)=LCM((p-1)(q-1))</script>​​，<strong>LCM</strong>：最小公倍数；</li><li>选择一个人整数<strong>e</strong>，满足：<script type="math/tex">1<e<\phi(n)\ and \ gcd(e,\phi(n))=1</script>​，即在<script type="math/tex">1</script>和<script type="math/tex">\phi(n)</script>之间和<script type="math/tex">\phi(n)</script>​互质的数；【<strong>e</strong>做为<strong>公钥</strong>】</li><li>令<script type="math/tex">d=e^{-1}(mod\ \phi(n))</script>；【<strong>d</strong>做为<strong>私钥</strong>】；</li></ul><h3 id="加密"><a href="#加密" class="headerlink" title="加密"></a>加密</h3><p>待加密数据：<script type="math/tex">m</script>；</p><p>计算<strong>密文c</strong>：<script type="math/tex">c(\mod n)=m^e</script></p><h3 id="解密"><a href="#解密" class="headerlink" title="解密"></a>解密</h3><p>密文：<script type="math/tex">c</script>；</p><p>计算<strong>原文m</strong>：<script type="math/tex">m(\mod n)=c^d</script></p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><ol><li>选择两个<strong>质数</strong>：<script type="math/tex">p=61,q=53</script>；</li><li>计算<strong>n</strong>：<script type="math/tex">n=p\cdot q=3233</script>；</li><li>计算欧拉函数：<script type="math/tex">\phi(n)=LCM((p-1)\cdot (q-1))=780</script>​；</li><li>随机取<strong>公钥</strong>（质数）：<script type="math/tex">e=17</script>；</li><li>计算<strong>私钥</strong>：<script type="math/tex">d=413,1=(17\times 413)(\mod780)</script>；</li><li>加密函数：<script type="math/tex">c(m)=m^{17}(\mod 3233)</script></li><li>解密函数：<script type="math/tex">m(c)=c^{413}(\mod 3233)</script></li></ol><h2 id="算法证明"><a href="#算法证明" class="headerlink" title="算法证明"></a>算法证明</h2><h3 id="基于欧拉定理"><a href="#基于欧拉定理" class="headerlink" title="基于欧拉定理"></a>基于欧拉定理</h3><p>对于和 <script type="math/tex">n</script> 互质的<strong>原文</strong>：<strong>m</strong>，需要证明：<script type="math/tex">m^{ed}=m(\mod n)</script></p><p>证明：</p><p>因为 <script type="math/tex">ed=1(\mod \phi(n))</script>​</p><p>有<script type="math/tex">ed=1+h\phi(n)</script></p><p>所以，<script type="math/tex">m^{ed}=m^{1+h\phi(n)}=m(m^{\phi(n)})^h</script>​​</p><p>根据<strong>欧拉定理</strong>：<script type="math/tex">m^{\phi(n)}=1(\mod n)</script></p><p>上式变为：<script type="math/tex">m(1)^h=m</script></p><p>故，原式得证。</p>]]></content>
      
      
      <categories>
          
          <category> 密码学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 加密算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dilworth定理</title>
      <link href="/2021/05/24/Dilworth%E5%AE%9A%E7%90%86/"/>
      <url>/2021/05/24/Dilworth%E5%AE%9A%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="Dilworth定理"><a href="#Dilworth定理" class="headerlink" title="Dilworth定理"></a>Dilworth定理</h1><h2 id="概念定义"><a href="#概念定义" class="headerlink" title="概念定义"></a>概念定义</h2><ul><li><strong>自反性</strong>：对于集合<strong>A</strong>上的<strong>二元关系R</strong>，<script type="math/tex">xRx,\forall x\in A</script>；</li><li><strong>反对称性</strong>：对于集合<strong>A</strong>上的<strong>二元关系R</strong>，对于 <script type="math/tex">xRy\ \ and\ \ yRx,\forall x,y\in A</script>，有<script type="math/tex">x=y</script>；</li><li><strong>传递性</strong>：对于集合<strong>A</strong>上的<strong>二元关系R</strong>，对于 <script type="math/tex">xRy\ \ and\ \ yRz,\forall x,y,z\in A</script>，有<script type="math/tex">xRz</script>；</li><li><strong>偏序关系</strong>：满足<strong>自反性</strong>、<strong>反对称性</strong>、<strong>传递性</strong>的二元关系；</li><li><strong>偏序集</strong>：<script type="math/tex">(S,\precsim)</script>，集合<strong>S</strong>中任意两个元素之间都存在偏序关系；</li><li><strong>链</strong>：对于一个有序序列<script type="math/tex">(x_1,x_2,\cdots,x_n)</script>，如果<script type="math/tex">x_i\precsim x_j \forall i \le j</script>，则称该序列为<strong>链</strong>；</li><li><strong>反链</strong>：对于一个有序序列<script type="math/tex">(x_1,x_2,\cdots,x_n)</script>，如果<script type="math/tex">\forall i \le j,x_1,x_j</script>都不满足偏序关系<script type="math/tex">\precsim</script>（即不可比），则称该序列为<strong>反链</strong>；</li></ul><h2 id="定理"><a href="#定理" class="headerlink" title="定理"></a>定理</h2><p>偏序集的<strong>最少</strong>的链的划分数等于其<strong>最长</strong>反链长度。</p><p>【把一个数列划分成最少的<strong>最长不升子序列</strong>的数目就等于这个数列的<strong>最长上升子序列</strong>的长度（LIS)】</p><h2 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h2><p><strong>数学归纳法</strong>：</p><p>设<strong>m</strong>表示<strong>偏序集</strong><script type="math/tex">(S,\precsim)</script>中元素个数。</p><ul><li><p>对于<strong>m=0，1</strong>，命题成立；</p></li><li><p>假设对于<script type="math/tex">m<n(n\in\mathbb{N}^+)</script>时命题成立，下面讨论<strong>m=n</strong>情况：</p><p>  设 <script type="math/tex">a</script> 为 <script type="math/tex">S</script> 中的一个<strong>极大元</strong>，构建偏序集 <script type="math/tex">S':=S-\{a\}</script>，由<strong>假设</strong>，<script type="math/tex">(S’,\precsim)</script> 满足该定理；</p><p>  假设 <script type="math/tex">S'</script> <strong>最小的链划分数</strong>以及<strong>最长反链长度</strong>为 <script type="math/tex">k</script>，划分为 <script type="math/tex">C_1,C_2\cdots,C_k</script> 这些不相交的链，长度为 <script type="math/tex">k</script> 的<strong>反链</strong>为 <script type="math/tex">a_1,a_2,\cdots,a_k</script>。</p><p>  记 <script type="math/tex">b_i</script> 为 <script type="math/tex">C_i</script> 中所有属于长为 <script type="math/tex">k</script> 的<strong>反链</strong>的元素中的<strong>最大元</strong>，构造<script type="math/tex">B:=\{b_1,b_2,\cdots,b_k\}</script>。在 <script type="math/tex">B</script> 中，易知 <script type="math/tex">B</script> 是<strong>反链</strong>，且 <script type="math/tex">C_i</script> 中的不可能存在多余一个元素都在<strong>反链</strong>上，也不会少于1个。</p><p>  下面考虑偏序集 <script type="math/tex">S</script>：</p><ul><li>因为 <script type="math/tex">a</script> 为 <script type="math/tex">S</script> 的<strong>极大元</strong>，那么 <script type="math/tex">\{a,b_1,b_2\cdots,b_k\}</script> 是一条长为 <script type="math/tex">k+1</script> 的<strong>反链</strong>。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 组合数学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>以太网帧</title>
      <link href="/2021/05/21/%E4%BB%A5%E5%A4%AA%E7%BD%91%E5%B8%A7/"/>
      <url>/2021/05/21/%E4%BB%A5%E5%A4%AA%E7%BD%91%E5%B8%A7/</url>
      
        <content type="html"><![CDATA[<h1 id="以太帧"><a href="#以太帧" class="headerlink" title="以太帧"></a>以太帧</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在以太网<strong>链路层</strong>上的<strong>数据包</strong>称作<strong>以太帧</strong>。</p><p><strong>以太帧</strong>起始部分由<strong>前导码</strong>和<strong>帧开始符</strong>【802.3协议】组成。后面紧跟着一个以太网报头，以<strong>MAC地址</strong>说明目的地址和源地址。帧的中部是该帧负载的包含<strong>其他协议报头</strong>的数据包（例如IP协议）。以太帧由一个32位<strong>冗余校验码</strong>结尾。它用于检验数据传输是否出现损坏。</p><p><strong>前导码</strong>用于引起接受节点的注意，实质是告诉接收方准备接受新帧。</p><p>相关的有<strong>Ethernet II</strong>、<strong>802.2 LLC</strong>等协议。</p><h2 id="Ethernet-II"><a href="#Ethernet-II" class="headerlink" title="Ethernet II"></a>Ethernet II</h2><p><img src="/2021/05/21/%E4%BB%A5%E5%A4%AA%E7%BD%91%E5%B8%A7/700px-Ethernet_Type_II_Frame_format.svg.png" alt="img"></p><ul><li><strong>目的硬件地址（Destination MAC Address）</strong>：占 48 bit；</li><li><strong>源硬件地址（Source MAC Address）</strong>：占 48 bit；</li><li><strong>以太类型（Type）</strong>：占 16 bit；</li><li><strong>数据（Data）</strong>：上一层传递的数据；标识封装于<strong>以太网帧</strong>中的上层协议，常见值为16进制：<ul><li>0x800：IPv4；</li><li>0x86DD：IPv6；</li><li>0x806：ARP；</li></ul></li><li><strong>循环校验和（CRC Checksum）</strong>：占 32 bit，进行数据校验；</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ethernet II </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TCP/IP</title>
      <link href="/2021/05/21/TCP-IP/"/>
      <url>/2021/05/21/TCP-IP/</url>
      
        <content type="html"><![CDATA[<h1 id="TCP-IP"><a href="#TCP-IP" class="headerlink" title="TCP/IP"></a>TCP/IP</h1><h2 id="总体结构"><a href="#总体结构" class="headerlink" title="总体结构"></a>总体结构</h2><h2 id="TCP协议"><a href="#TCP协议" class="headerlink" title="TCP协议"></a>TCP协议</h2><p><img src="/2021/05/21/TCP-IP/6-19110Q62344I5.gif" alt="img"></p><h3 id="字段含义"><a href="#字段含义" class="headerlink" title="字段含义"></a>字段含义</h3><ul><li><strong>源端口（Source Port）</strong>：源计算机上的应用程序的端口号，占 16 位；</li><li><strong>目的端口（Destination Port）</strong>：目标计算机的应用程序端口号，占 16 位；</li><li><strong>序列号字段（Sequence Number）</strong>：占 32 位。它表示本报文段所发送数据的第一个字节的编号。在 TCP 连接中，所传送的字节流的每一个字节都会按顺序编号；</li><li><strong>确认号字段（Acknowledgement Number）</strong>：占 32 位。它表示接收方期望收到发送方下一个报文段的第一个字节数据的编号。其值是接收计算机即将接收到的下一个序列号，也就是下一个接收到的字节的序列号加1；</li><li><strong>数据偏移字段（Header Length）</strong>：占 4 位，数据段中的<strong>数据</strong>部分起始处，距离 TCP 数据段起始处的字节偏移量；</li><li><strong>保留字段（Resv）</strong>：占 4 位，为 TCP 将来的发展预留空间，目前必须全部为 0；</li><li><strong>标志字段</strong>：占 8 位，<ul><li><strong>CWR（Congestion Window Reduce）</strong>：拥塞窗口减少标志，用来表明它接收到了设置 ECE 标志的 TCP 包。并且，发送方收到消息之后，通过减小发送窗口的大小来降低发送速率；</li><li><strong>ECE（ECN Echo）</strong>：用来在 TCP 三次握手时表明一个 TCP 端是具备 ECN 功能的。在数据传输过程中，它也用来表明接收到的 TCP 包的 IP 头部的 ECN 被设置为 11，即网络线路拥堵；</li><li><strong>URG（Urgent）</strong>：表示本报文段中发送的数据是否包含紧急数据。URG=1 时表示有紧急数据。当 URG=1 时，后面的紧急指针字段才有效；</li><li><strong>ACK</strong>：表示前面的<strong>确认号</strong>字段是否有效。ACK=1 时表示有效。只有当 ACK=1 时，前面的确认号字段才有效。TCP 规定，连接建立后，ACK 必须为 1。</li><li><strong>PSH（Push）</strong>：告诉对方收到该报文段后是否立即把数据推送给上层。如果值为 1，表示应当立即把数据提交给上层，而不是缓存起来；</li><li><strong>RST</strong>：表示是否重置连接。如果 RST=1，说明 TCP 连接出现了严重错误（如主机崩溃），必须释放连接，然后再重新建立连接；</li><li><strong>SYN</strong>：在建立连接时使用，用来同步序号。当 <strong>SYN=1，ACK=0</strong> 时，表示这是一个请求建立连接的报文段；当 <strong>SYN=1，ACK=1</strong> 时，表示对方同意建立连接。SYN=1 时，说明这是一个请求建立连接或同意建立连接的报文。只有在前两次握手中 SYN 才为 1；</li><li><strong>FIN</strong>：标记数据是否发送完毕。如果 FIN=1，表示数据已经发送完成，可以释放连接；</li></ul></li><li><strong>窗口大小字段（Window Size）</strong>：占 16 位，它表示从<strong>确认号</strong>开始还可以接收多少字节的数据量，也表示当前接收端的接收窗口还有多少剩余空间，该字段可以用于 TCP 的流量控制；</li><li><strong>TCP 校验和字段（TCP Checksum）</strong>：占 16 位，它用于确认传输的数据是否有损坏。发送端基于数据内容校验生成一个数值，接收端根据接收的数据校验生成一个值。两个值必须相同，才能证明数据是有效的。如果两个值不同，则丢掉这个数据包。Checksum 是根据伪头 + TCP 头 + TCP 数据三部分进行计算的；</li><li><strong>紧急指针字段（Urgent Pointer）</strong>：占16位，仅当前面的 URG 控制位为 1 时才有意义。它指出本数据段中为紧急数据的字节数，占 16 位。当所有紧急数据处理完后，TCP 就会告诉应用程序恢复到正常操作。即使当前窗口大小为 0，也是可以发送紧急数据的，因为紧急数据无须缓存；</li><li><strong>可选字段（Options）</strong>：长度不定，但长度必须是 32bits 的整数倍。</li></ul><h2 id="IP协议"><a href="#IP协议" class="headerlink" title="IP协议"></a>IP协议</h2><h3 id="ipv4"><a href="#ipv4" class="headerlink" title="ipv4"></a>ipv4</h3><p><img src="/2021/05/21/TCP-IP/p1.png" alt="ipv4" style="zoom: 60%;"></p><ul><li><strong>版本（Version）</strong>：包含一个 4 位二进制值 0100，用于标识这是 IPv4 数据包。</li></ul><ul><li><p><strong>区分服务（DS）</strong>：以前称为服务类型 (ToS) 字段，DS 字段是一个用于确定每个数据包优先级的 8 位字段。DiffServ 字段的六个最高有效位是区分服务代码点 (DSCP)，而后两位是显式拥塞通知 (ECN) 位。</p></li><li><p><strong>生存时间 （TTL）</strong>：包含用于限制数据包寿命的一个 8 位二进制值。数据包发送方设置初始 TTL 值，数据包每经过一次路由器处理，数值就减少一。如果 TTL 字段的值减为零，则路由器将丢弃该数据包并向源 IP 地址发送 Internet 控制消息协议 (ICMP) 超时消息。</p></li><li><p><strong>协议（Protocol）</strong>：字段用于确定下一级协议。此 8 位二进制值表示数据包包含的数据负载类型，使网络层将数据传送到相应的上层协议。常用的值包括 <strong>ICMP (1)</strong>、<strong>TCP (6)</strong> 和 <strong>UDP (17)</strong>。</p></li><li><p><strong>源 IPv4 地址（Source Address）</strong>：包含表示数据包源 IPv4 地址的 32 位二进制值。源 IPv4 地址始终为单播地址。</p></li><li><p><strong>目的 IPv4 地址（Destination Address）</strong>：包含表示数据包目的 IPv4 地址的 32 位二进制值。目的 IPv4 地址为单播、组播或广播地址。</p></li></ul><ul><li><strong>互联网报头长度 (IHL)</strong>、<strong>总长度</strong>和<strong>报头校验和</strong>字段用于识别和验证数据包。</li></ul><h3 id="ipv6"><a href="#ipv6" class="headerlink" title="ipv6"></a>ipv6</h3><p><img src="/2021/05/21/TCP-IP/p2.png" alt="ipv6" style="zoom:80%;"></p><ul><li><p><strong>版本（Version）</strong>：此字段包含一个 4 位二进制值 0110，用于标识这是 IPv6 数据包；</p></li><li><p><strong>流量类别（Traffic Class）</strong>：此 8 位字段相当于<strong>IPv4 区分服务 (DS)</strong>字段；</p></li><li><p><strong>流量号（Flow Label）</strong>：此 20 位字段建议带有相同流量标签的所有数据包收到路由器的相同处理；</p></li><li><p><strong>有效负载长度（Payload Length）</strong>：此 16 位字段表示 IPv6 数据包的数据部分或负载的长度；</p></li><li><p><strong>下一首部（Next Header）</strong>：此 8 位字段相当于<strong>IPv4 协议</strong>字段。它表示数据包传送的数据负载类型，使网络层将数据传送到相应的上层协议；</p></li><li><p><strong>跳数限制（Hop Limit）</strong>：此 8 位字段取代 IPv4 的 TTL 字段。每个转发数据包的路由器均会使此数值减一。当跳数达到 0 时，会丢弃此数据包，并且会向发送主机转发 ICMPv6 超时消息来指明数据包未到达目的地，因为超过跳数限制。</p></li><li><p><strong>源 IPv6 地址 </strong>：此 128 位字段用于确定发送主机的 IPv6 地址；</p></li><li><p><strong>目的 IPv6 地址</strong>：此 128 位字段用于确定接收主机的 IPv6 地址；</p></li></ul><h3 id="ARP协议"><a href="#ARP协议" class="headerlink" title="ARP协议"></a>ARP协议</h3><p><strong>地址解析协议（Address Resolution Protocol）</strong>，其基本功能为通过目标设备的<strong>IP地址</strong>，查询目标设备的<strong>MAC地址</strong>，以保证通信的顺利进行。</p><h4 id="协议包结构："><a href="#协议包结构：" class="headerlink" title="协议包结构："></a>协议包结构：</h4><p><img src="/2021/05/21/TCP-IP/blog\blog\source\_posts\TCP-IP\p3.png" style="zoom:67%;"></p><ul><li><strong>硬件类型（Hardware type,HTYPE）</strong>：【16bits】指定网络连接协议类型：<ul><li>1：Ethernet；</li></ul></li><li><strong>协议类型（Protocol type,PTYPE）</strong>：【16bits】指定了<strong>ARP请求</strong>的互联网协议：<ul><li>0x0800：ipv4；</li></ul></li><li><strong>硬件地址长度（Hardware length,HLEN）</strong>：【8bits】硬件地址长度，以太网是6字节；</li><li><strong>协议地址长度（Protocol length,PLEN）</strong>：【8bits】网络地址长度，ipv4是4字节；</li><li><strong>操作（Operation）</strong>：【16bits】指定报文的目的：<ul><li>1：请求；</li><li>2：回复；</li></ul></li><li><strong>发送方地址（Sender hardware address,SHA）</strong>:【48bits】发送方<strong>MAC</strong>地址；</li><li><strong>发送方协议地址（Sender protocol address,SPA）</strong>：【32bits】发送方网络地址；</li><li><strong>目地方硬件地址（Target hardware address,THA）</strong>：【48bits】目的方<strong>MAC</strong>地址；</li><li><strong>目的方协议地址（Target protocol address,TPA）</strong>：【32bits】目的方网络地址；</li></ul><h2 id="网络字节顺序"><a href="#网络字节顺序" class="headerlink" title="网络字节顺序"></a>网络字节顺序</h2><p><strong>TCP/IP</strong>定义了标准的用于<strong>协议头</strong>中的二进制整数表示：<strong>网络字节顺序（Network Byte Order）</strong>。</p><h3 id="转换函数"><a href="#转换函数" class="headerlink" title="转换函数"></a>转换函数</h3><h4 id="htons"><a href="#htons" class="headerlink" title="htons"></a>htons</h4><ul><li>本地字节顺序 <script type="math/tex">\rightarrow</script> 网络字节顺序 【16bits】</li></ul><h4 id="ntohs"><a href="#ntohs" class="headerlink" title="ntohs"></a>ntohs</h4><ul><li>网络字节顺序 <script type="math/tex">\rightarrow</script> 本地字节顺序 【16bits】</li></ul><h4 id="htonl"><a href="#htonl" class="headerlink" title="htonl"></a>htonl</h4><ul><li>本地字节顺序 <script type="math/tex">\rightarrow</script> 网络字节顺序 【32bits】</li></ul><h4 id="ntohl"><a href="#ntohl" class="headerlink" title="ntohl"></a>ntohl</h4><ul><li>网络字节顺序 <script type="math/tex">\rightarrow</script> 本地字节顺序 【32bits】</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TCP </tag>
            
            <tag> IP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>WireShark</title>
      <link href="/2021/05/21/WireShark/"/>
      <url>/2021/05/21/WireShark/</url>
      
        <content type="html"><![CDATA[<h1 id="WireShark"><a href="#WireShark" class="headerlink" title="WireShark"></a>WireShark</h1><h2 id="WireShark抓包内容分析"><a href="#WireShark抓包内容分析" class="headerlink" title="WireShark抓包内容分析"></a>WireShark抓包内容分析</h2><h3 id="链路层封装"><a href="#链路层封装" class="headerlink" title="链路层封装"></a>链路层封装</h3><p>共占<strong>8*14=112</strong> bit。</p><p><img src="/2021/05/21/WireShark/p1.png" alt="p1" style="zoom: 80%;"></p><ul><li><strong>目的硬件地址（Destination Hardware Address）</strong>：占 48 bit；</li><li><strong>源硬件地址（Source Hardware Address）</strong>：占 48 bit；</li><li><strong>协议类别（Type）</strong>：占 16 bit；</li></ul><h3 id="网络层封装"><a href="#网络层封装" class="headerlink" title="网络层封装"></a>网络层封装</h3><p>共占<strong>8*20=160</strong> bit。</p><p><img src="/2021/05/21/WireShark/p2.png" alt="network" style="zoom:80%;"></p><ul><li><strong>版本号（Version）</strong>：占 4 bit，<strong>ipv4</strong>版本号；</li><li><strong>报文头长度（Header length）</strong>：占 4 bit，表示报文头长度【单位：字节（byte）】；</li><li><strong>区分服务域（Differentiated Services Field）</strong>：占 8 bit，<ul><li><strong>区分服务代码点（Differentiated Services Codepoints）</strong>：占 6 bit，DS标记值；</li><li><strong>显示拥塞通知（Explicit Congestion Notification）</strong>：占 2 bit；</li></ul></li><li><strong>总长度（Total Length）</strong>：占 16 bit；</li><li><strong>标识符（Identification）</strong>：占 16 bit；</li><li><strong>标志位（Flags）</strong>：占 3 bit；</li><li><strong>片位移（Fragment Offset）</strong>：占 13 bit；</li><li><strong>TTL（Time to Live）</strong>：占 8 bit；</li><li><strong>内层封装协议（Protocol）</strong>：占 8 bit；</li><li><strong>头部校验和（Header Checksum）</strong>：占 16 bit；</li><li><strong>源地址（Source Address）</strong>：占 32 bit；</li><li><strong>目标地址（Destination Address）</strong>：占 32 bit，即<strong>ip地址</strong>；</li></ul><h3 id="传输层封装（TCP）"><a href="#传输层封装（TCP）" class="headerlink" title="传输层封装（TCP）"></a>传输层封装（TCP）</h3><p>共占 <strong>8*32=256</strong> bit。</p><p><img src="/2021/05/21/WireShark/p3.png" alt="TCP"></p><ul><li><p><strong>源端口号（Source Port）</strong>：占 16 bit；</p></li><li><p><strong>目的端口号（Destination Port）</strong>：占 16 bit；</p></li><li><p><strong>序列号字段（Sequence Number）</strong>：占 32 bt；</p></li><li><p><strong>确认号字段（Acknowledgement Number）</strong>：占 32 bit；</p></li><li><p><strong>数据偏移字段（Header Length）</strong>：占 4 bit；</p></li><li><strong>标志字段（Flags）</strong>：占 12 bit；</li><li><strong>窗口大小字段（Window）</strong>：占 16 bit；</li><li><strong>校验和字段（Checksum）</strong>：占16 bit；</li><li><strong>紧急指针字段（Urgent Pointer）</strong>：占16 bit；</li><li><strong>可选字段（Option）</strong>：长度不定；</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Network </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Computer Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Socket</title>
      <link href="/2021/05/20/Socket/"/>
      <url>/2021/05/20/Socket/</url>
      
        <content type="html"><![CDATA[<h1 id="Socket"><a href="#Socket" class="headerlink" title="Socket"></a>Socket</h1><h2 id="Socket-API"><a href="#Socket-API" class="headerlink" title="Socket API"></a>Socket API</h2><h4 id="WSAStartup"><a href="#WSAStartup" class="headerlink" title="WSAStartup"></a>WSAStartup</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">WSAStartup</span><span class="params">(WORD wVersionRequested, LPWSADATA IpWSAData)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li><p>使用<strong>Socket</strong>应用程序之前，必须调用<strong>WSAStartup</strong>；</p><ul><li>仅<strong>WinSock</strong>环境下；</li></ul></li><li><p>参数：</p><ul><li><p><strong>WORD wVersionRequested</strong>，指明程序请求使用的<strong>WinSock</strong>版本，<strong>高位字节</strong>指明<strong>副版本</strong>，<strong>低位字节</strong>指明<strong>主版本</strong>；</p><p>  例如：<strong>0x102</strong>表示<strong>2.1</strong>版；</p></li><li><p><strong>LPWSADATA IpWSAData</strong>，指向<strong>WSADATA</strong>的指针，保存返回的实际的<strong>WinSock</strong>的版本信息；</p></li></ul></li></ul><h4 id="WSACleanup"><a href="#WSACleanup" class="headerlink" title="WSACleanup"></a>WSACleanup</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">WSACleanup</span><span class="params">(<span class="keyword">void</span>)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li>应用程序在完成<strong>Socket</strong>使用后调用；<ul><li>仅<strong>WinSock</strong>环境下；</li></ul></li><li>解除与<strong>Socket</strong>库的绑定；</li><li>释放<strong>Socket</strong>系统资源；</li></ul><h4 id="socket"><a href="#socket" class="headerlink" title="socket"></a>socket</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">socket</span><span class="params">(<span class="keyword">int</span> protofamily, <span class="keyword">int</span> type, <span class="keyword">int</span> protocol)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li><p>创建<strong>套接字</strong>；</p></li><li><p>返回<strong>套接字</strong>描述符；</p></li><li><p>参数：</p><ul><li><p><strong>int protofamily</strong>：指明<strong>协议族</strong>，</p><p>  <strong>PF_INET</strong>——<strong>TCP/IP</strong></p></li><li><p><strong>int type</strong>：指明套接字类型，</p><p>  <strong>SOCK_STREAM</strong>：应用层到传输层（TCP）；</p><p>  <strong>SOCK_DGRAM</strong>：应用层到传输层（UDP）；</p><p>  <strong>SOCK_RAM</strong>：应用层到网络层；</p></li><li><p><strong>int protocol</strong>：指明协议号，0为默认；</p></li></ul></li></ul><h4 id="closesocket"><a href="#closesocket" class="headerlink" title="closesocket"></a>closesocket</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">closesocket</span><span class="params">(SOCKET sd)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li>关闭一个<strong>套接字</strong>；</li><li>如果多个进程共享一个套接字，调用<strong>closesocket</strong>将套接字<strong>引用计数</strong>减1，减至0才关闭该套接字；</li><li>一个进程中的多个线程对一个套接字的使用没有<strong>计数</strong>功能；</li><li>参数：<ul><li><strong>SOCKET sd</strong>：套接字描述符；</li></ul></li><li>返回：0——成功，<strong>SOCKET_ERROR</strong>——失败；</li></ul><h4 id="bind"><a href="#bind" class="headerlink" title="bind"></a>bind</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">bind</span><span class="params">(<span class="keyword">int</span> sockfd, <span class="keyword">const</span> struct sockaddr *addr, <span class="keyword">socklen_t</span> addrlen)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li><p>绑定<strong>套接字</strong>本地<strong>端点地址</strong>；</p></li><li><p>客户端一般不用调用，服务器端需要；</p></li><li><p>参数：</p><ul><li><strong>int sockfd</strong>：套接字描述符；</li><li><strong>const struct sockaddr *addr</strong>：本地端口地址（IP地址+端口号），地址通配符<strong>INADDR_ANY</strong>可以绑定所有端口；</li><li><strong>socklen_t addrlen</strong>：地址大小；</li></ul></li></ul><h4 id="listen"><a href="#listen" class="headerlink" title="listen"></a>listen</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">listen</span><span class="params">(<span class="keyword">int</span> sockfd, <span class="keyword">int</span> queuesize)</span></span>;</span><br></pre></td></tr></tbody></table></figure><ul><li>将<strong>服务器端</strong>的<strong>流套接字</strong>置于<strong>监听状态</strong>；<ul><li>仅用于<strong>服务器端</strong>；</li><li>仅用于<strong>面向连接的流套接字</strong>；</li></ul></li><li>参数：<ul><li><strong>int sockfd</strong>：套接字描述符；</li><li><strong>int queuesize</strong>：请求队列的大小；</li></ul></li></ul><h4 id="connect"><a href="#connect" class="headerlink" title="connect"></a>connect</h4><figure class="highlight c++"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">connect</span>(<span class="keyword">int</span> sockfd, <span class="keyword">const</span> struct sockaddr *saddr, <span class="keyword">int</span> saddrlen)</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>客户</strong>程序调用<strong>connect</strong>使<strong>客户端套接字sockfd</strong>，与特点计算机的特定端口<strong>saddr</strong>的套接字进行连接；<ul><li>仅用于<strong>客户端</strong>；</li><li>可用于<strong>TCP、UDP</strong>；</li></ul></li></ul><h4 id="accept"><a href="#accept" class="headerlink" title="accept"></a>accept</h4><h4 id="send"><a href="#send" class="headerlink" title="send"></a>send</h4><h4 id="sendto"><a href="#sendto" class="headerlink" title="sendto"></a>sendto</h4><h4 id="recv"><a href="#recv" class="headerlink" title="recv"></a>recv</h4><h4 id="recvfrom"><a href="#recvfrom" class="headerlink" title="recvfrom"></a>recvfrom</h4><h4 id="setsockopt"><a href="#setsockopt" class="headerlink" title="setsockopt"></a>setsockopt</h4><h4 id="getsockopt"><a href="#getsockopt" class="headerlink" title="getsockopt"></a>getsockopt</h4>]]></content>
      
      
      <categories>
          
          <category> Computer Network </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>A Frustratingly Easy Approach for Joint Entity and Relation Extraction</title>
      <link href="/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/"/>
      <url>/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/</url>
      
        <content type="html"><![CDATA[<h1 id="A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction1"><a href="#A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction1" class="headerlink" title="A Frustratingly Easy Approach for Joint Entity and Relation Extraction1"></a>A Frustratingly Easy Approach for Joint Entity and Relation Extraction<a href="#refer-anchor-1"><sup>1</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>传统的<strong>命名实体识别（NER）</strong>和<strong>关系抽取（ER）</strong>任务是相互独立的；过去几年，开始出现把这两种任务进行<strong>联合训练</strong>的方法。</p><p>现有的<strong>关系抽取</strong>方法可以分为两种：</p><ul><li><strong>结构化预测（structured prediction）</strong>：将两个任务合并成一个任务；</li><li><strong>多任务学习（multi-task learning）</strong>：为两个任务构建两个分离的模型，然后通过<strong>参数共享（parameter  sharing）</strong>的方式进行联合训练和优化；</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><h4 id="NER（Named-Entity-Recognition）"><a href="#NER（Named-Entity-Recognition）" class="headerlink" title="NER（Named Entity Recognition）"></a>NER（Named Entity Recognition）</h4><p>设<script type="math/tex">\Epsilon</script>表示<strong>预先定义的实体类型集合</strong>，该<strong>NER</strong>任务是：</p><p>对于每个<strong>词组（span）</strong><script type="math/tex">s_i\in S</script>，需要预测一个实体<script type="math/tex">y_e(s_i)\in \Epsilon \cup \{\epsilon\}</script>，其中：<script type="math/tex">\epsilon</script>表示该<strong>词组</strong>不是<strong>实体</strong>。</p><p>输出为：<script type="math/tex">Y_e=\{(s_i,e),s_i\in S,e\in \Epsilon\cup \{\epsilon\}\}</script></p><h4 id="ER（Relation-Extraction）"><a href="#ER（Relation-Extraction）" class="headerlink" title="ER（Relation Extraction）"></a>ER（Relation Extraction）</h4><p>设<script type="math/tex">\mathcal{R}</script>表示预先定义的<strong>关系类型</strong>的集合，该<strong>ER</strong>任务为：</p><p>对于每对<strong>词组（span）</strong><script type="math/tex">s_i\in S,s_j\in S</script>，需要预测一个关系种类<script type="math/tex">y_r(s_i,s_j)\in \mathcal{R}\cup\{\epsilon\}</script>，<script type="math/tex">\epsilon</script>表示不存在关系。</p><p>输出为：<script type="math/tex">Y_r=\{(s_i,s_j,r),s_i,s_j\in S,r\in \mathcal{R}\}</script></p><h3 id="NER模型"><a href="#NER模型" class="headerlink" title="NER模型"></a>NER模型</h3><ul><li><p>使用<strong>预训练模型</strong>（例如：BERT）获取<script type="math/tex">x_t的</script><strong>上下文表征（contextualized representation）</strong><script type="math/tex">\mathrm{x}_t</script>；</p></li><li><p>获得<strong>词组表征（span representation）</strong>：</p><p>  对于词组<script type="math/tex">s_i\in S</script>，其<strong>词组表征</strong>为：</p><script type="math/tex; mode=display">\bold{h}_e(s_i)=[\mathrm{x}_{START(i)};\mathrm{x}_{END(i)};\phi(s_i)]</script><p>  其中，<script type="math/tex">\phi(s_i)\in\mathbb{R}^{d_W}</script>表示<strong>可学习的</strong>对于<strong>词组长度（span width）</strong>的表征；<script type="math/tex">\mathrm{x}_{START(i)}</script>表示<strong>词组</strong>的首单词，<script type="math/tex">\mathrm{x}_{END(i)}</script>表示词组的尾单词。</p></li><li><p>使用两层带<strong>ReLU激活函数</strong>的<strong>前向神经网络</strong>，将<strong>词组表征</strong>投影到<strong>分类空间</strong><script type="math/tex">\Epsilon</script>：</p><script type="math/tex; mode=display">P_e(e|s_i)=\mathrm{softmax}(\mathrm{W_eFFNN}(\bold{h_e(s_i)}))</script></li></ul><h3 id="ER模型"><a href="#ER模型" class="headerlink" title="ER模型"></a>ER模型</h3><p>过去的模型直接使用<strong>NER模型</strong>的<script type="math/tex">\textbf{h}_e(s_i),\textbf{h}(s_j)</script>来进行<strong>关系</strong>预测。但是，该文章认为：</p><ul><li><strong>NER</strong>这些表征只获取了每个<strong>实体</strong>附近的<strong>上下文信息（contextual information）</strong>，可能无法获取两个<strong>词组（span）</strong>之间的联系（因为这需要获取整个句子信息）。</li><li>在不同种类的<strong>词组对（pair of span）</strong>的表征之间共享参数可能不太好；</li></ul><p>于是该文章提出下面模型：</p><ul><li><p>将<strong>NER</strong>获得的实体标签插入到句子中：</p><p>  对于一对词组，<script type="math/tex">s_i,s_j</script>，其对应的<strong>实体</strong>为<script type="math/tex">e_i,e_j</script>，这里定义<strong>文本标记（text marker）</strong><script type="math/tex">\langle S:e_i \rangle,\langle /S:e_i\rangle,\langle O:e_j\rangle,\langle /O:e_j\rangle</script>，然后将他们插入原句子得到：</p><script type="math/tex; mode=display">\hat{X}=\cdots \langle S:e_i\rangle,x_{START(i)},\cdots,x_{END(i)},\langle /S:e_i\rangle,</script><script type="math/tex; mode=display">\cdots\langle O:e_j\rangle,x_{START(j)},\cdots,x_{END(j)},\langle/O:e_j\rangle,\cdots</script></li><li><p>使用另一个<strong>预训练模型</strong>获得<script type="math/tex">\hat{X}</script>的表征：</p><script type="math/tex; mode=display">\textbf{h}_r(s_i,s_j)=[\hat{\mathrm{x}}_\hat{START(i)};\hat{\mathrm{x}}_\hat{START(j)}]</script><p>  其中，<script type="math/tex">\hat{START(i)},\hat{START(j)}</script>分别是<script type="math/tex">\hat{X}</script>中<script type="math/tex">\langle S:e_i\rangle,\langle O:e_j\rangle</script>的<strong>序号</strong>；</p></li><li><p>计算分类结果：</p><script type="math/tex; mode=display">P_r(r|s_i,s_j)=\mathrm{softmax}(\textbf{W}_r\textbf{h}_r(s_i,s_j))</script></li></ul><h3 id="跨句文本表示（Cross-sentence-context）"><a href="#跨句文本表示（Cross-sentence-context）" class="headerlink" title="跨句文本表示（Cross-sentence context）"></a>跨句文本表示（Cross-sentence context）</h3><p>某个句子的前后文可能会对<strong>NER</strong>和<strong>ER</strong>结果产生影响。</p><p>这里将<strong>NER</strong>和<strong>ER</strong>输入句子长度<strong>固定为W</strong>，对于每个长度为<strong>n</strong>的句子，从该句子的上下文中分别补充<script type="math/tex">(W-n)/2</script>个单词。</p><h3 id="训练和推断"><a href="#训练和推断" class="headerlink" title="训练和推断"></a>训练和推断</h3><h4 id="训练loss"><a href="#训练loss" class="headerlink" title="训练loss"></a>训练loss</h4><p>都使用<strong>交叉熵</strong>损失。</p><p><strong>NER</strong>：</p><script type="math/tex; mode=display">\mathcal{L}_e=-\sum_{s_i\in S}\log P_e(e_i^*|s_i)</script><p><strong>ER</strong>：</p><script type="math/tex; mode=display">\mathcal{L}_r=-\sum_{s_i,s_j\in S_G,s_i\ne s_j}\log P_r(r_{i,j}^*|s_i,s_j)</script><p>对于<strong>ER</strong>训练，只考虑存在关系的实体对（gold entities）</p><h3 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h3><p><strong>NER</strong>：</p><script type="math/tex; mode=display">y_e(s_i)=\mathrm{argmax}_{e\in \Epsilon}P_e(e|s_i)</script><p><strong>ER</strong>：</p><p>枚举所有的<strong>实体对</strong>，并计算：<script type="math/tex">P_r(r|s_i,s_j)</script></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/image-20210517201212314.png" alt="image-20210517201212314"></p><p><strong>效果很好</strong></p><h3 id="验证构建文本标记的重要性"><a href="#验证构建文本标记的重要性" class="headerlink" title="验证构建文本标记的重要性"></a>验证构建文本标记的重要性</h3><p><img src="/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/image-20210517201423138.png" alt="image-20210517201423138" style="zoom:67%;"></p><ul><li><strong>TEXT</strong>：直接使用，<script type="math/tex">[\textbf{h}_e(s_i), \textbf{h}_e(s_j),\textbf{h}_e(s_i) \odot \textbf{h}_e(s_j)]</script>；</li><li><strong>TEXTETYPE</strong>：在<strong>TEXT</strong>基础上加入<strong>长度表征</strong>，<script type="math/tex">[\textbf{h}_e(s_i), \textbf{h}_e(s_j),\textbf{h}_e(s_i) \odot \textbf{h}_e(s_j),\phi(e_i)]</script>；</li><li><strong>MARKER</strong>：文本标记改为：<script type="math/tex">\langle S \rangle,\langle /S\rangle,\langle O\rangle,\langle /O\rangle</script>；</li><li><strong>MAKERETYPE</strong>：在<strong>MAKER</strong>基础上加入<strong>长度表征</strong>；</li><li><strong>MAKKERELOSS</strong>：在<strong>MARKER</strong>基础上使用<strong>FFNN</strong>进行预测；</li><li><strong>TYPEDMARKERS</strong>：模型提出的方法；</li></ul><p><strong>发现</strong>：使用<strong>标记</strong>更好，加上实体信息更好，加上长度信息更好。</p><h3 id="NER和ER之间关系"><a href="#NER和ER之间关系" class="headerlink" title="NER和ER之间关系"></a>NER和ER之间关系</h3><h4 id="NER和ER共享encoder效果更差"><a href="#NER和ER共享encoder效果更差" class="headerlink" title="NER和ER共享encoder效果更差"></a>NER和ER共享encoder效果更差</h4><p><img src="/2021/05/16/A-Frustratingly-Easy-Approach-for-Joint-Entity-and-Relation-Extraction/image-20210517202950372.png" alt="image-20210517202950372" style="zoom:50%;"></p><h4 id="联合训练效果基本没有提高"><a href="#联合训练效果基本没有提高" class="headerlink" title="联合训练效果基本没有提高"></a>联合训练效果基本没有提高</h4><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/2010.12812">A Frustratingly Easy Approach for Joint Entity and Relation Extraction</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> NER </tag>
            
            <tag> RE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Naive Bayes</title>
      <link href="/2021/05/15/Naive-Bayes/"/>
      <url>/2021/05/15/Naive-Bayes/</url>
      
        <content type="html"><![CDATA[<h1 id="朴素贝叶斯（Naive-Bayes）"><a href="#朴素贝叶斯（Naive-Bayes）" class="headerlink" title="朴素贝叶斯（Naive Bayes）"></a>朴素贝叶斯（Naive Bayes）</h1><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>利用<strong>条件独立性假设</strong>和<strong>贝叶斯公式</strong>，将对<strong>测试集</strong>的<strong>概率预测</strong>变为可计算的，再利用<strong>训练集</strong>求得。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>根据给定数据<script type="math/tex">X^*\in\mathbb{R}^d</script>，预测出其标签值<script type="math/tex">y^*\in\{0,1,\cdots,n\}</script>。</p><p>易得，预测结果为<script type="math/tex">y'</script>的概率为：</p><script type="math/tex; mode=display">P_\theta(y=y'|X=X^*)</script><p>由<strong>贝叶斯定理</strong>：</p><script type="math/tex; mode=display">p(a|b)=\frac{p(b|a)\cdot p(a)}{p(b)}</script><p>得：</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{p(X^*|y')\cdot p(y')}{p(X^*)}</script><p>由<strong>条件独立性假设</strong>，得：</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{p(x_1=x_1^*|y')\cdot p(x_2=x_2^*|y')\cdots p(x_d=x_d^*|y')\cdot p(y')}{p(X^*)}</script><p>其中，<script type="math/tex">p(x_i=x_i^*|y'),p(x_i=x_i^*)</script>可以由<strong>训练集</strong>求得<strong>后验概率</strong>。</p><h3 id="条件独立性假设"><a href="#条件独立性假设" class="headerlink" title="条件独立性假设"></a>条件独立性假设</h3><p>朴素贝叶斯法对条件概率分布做了 <strong>条件独立性假设</strong>，该<strong>条件独立性假设</strong>不等于<strong>独立性假设</strong>。</p><p>它是说 【<strong>用于分类的特征</strong>】在【<strong>类确定</strong>】的条件下都 是【<strong>条件独立</strong>】的，即：在给定<script type="math/tex">y</script>的条件下<script type="math/tex">x_i,x_j</script>是<strong>条件独立</strong>的。</p><h3 id="高斯模型"><a href="#高斯模型" class="headerlink" title="高斯模型"></a>高斯模型</h3><p>假设原始数据服从<strong>高斯分布</strong>，有以下概率公式：</p><script type="math/tex; mode=display">p(x_i=x|y=y')=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}</script><p>其中，<script type="math/tex">\mu,\sigma^2</script>是<script type="math/tex">y=y'</script>的第<script type="math/tex">i</script>个属性的数据分布的<strong>均值</strong>和<strong>方差</strong>。</p><h3 id="多项式模型"><a href="#多项式模型" class="headerlink" title="多项式模型"></a>多项式模型</h3><p>若特征值不是代表着简单的<strong>离散类型</strong>（如男性、女性），而是一种<strong>计数形式</strong>的特殊类型（身高，薪水等级），可以使用<strong>多项式模型</strong>。</p><p>该模型一般用于<strong>文本分类</strong>，<strong>属性</strong>是<strong>单词</strong>，<strong>属性值</strong>是<strong>单词出现次数</strong>。</p><script type="math/tex; mode=display">p(x_i|y=y')=\frac{N_{yi}+\alpha}{N_y+\alpha n}</script><p>其中，<script type="math/tex">N_{yi}=\sum_{x\in T}x_i</script>是训练集<script type="math/tex">D</script>中，标签为<script type="math/tex">y'</script>且第<script type="math/tex">i</script>个属性值求和（即单词<script type="math/tex">i</script>出现次数），<script type="math/tex">N_y=\sum_{i=1}^dN_{yi}</script>（所有单词出现总次数）。</p><script type="math/tex; mode=display">P_\theta(y'|X^*)=\frac{m!}{x_1!\cdot x_2!\cdots x_d!}\prod_{i=1}^d(p^{x_i=x}(x_i|y=y'))</script><h3 id="伯努利模型"><a href="#伯努利模型" class="headerlink" title="伯努利模型"></a>伯努利模型</h3>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Naive Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KNN</title>
      <link href="/2021/05/15/KNN/"/>
      <url>/2021/05/15/KNN/</url>
      
        <content type="html"><![CDATA[<h1 id="K近邻（KNN）"><a href="#K近邻（KNN）" class="headerlink" title="K近邻（KNN）"></a>K近邻（KNN）</h1><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>给定<strong>训练集</strong><script type="math/tex">D</script>，当对<strong>测试样本</strong>进行<strong>分类</strong>时，找到<strong>训练集</strong>中与该<strong>样本</strong>最相似的<script type="math/tex">k</script>个<strong>样本（k近邻）</strong>，根据<script type="math/tex">k</script>个<strong>样本</strong>的标签确定测试样本的标签。</p><p><img src="/2021/05/15/KNN/image-20210515162556039.png" alt="image-20210515162556039"></p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><ul><li>输入训练集<script type="math/tex">D</script>；</li><li>确定<script type="math/tex">k</script>的大小和距离计算方法<script type="math/tex">d(\cdot)</script>；</li><li>从<strong>训练集</strong>中得到前<script type="math/tex">k</script>个与测试样本最近的样本（<script type="math/tex">k</script>近邻）；</li><li>根据<script type="math/tex">k</script>个最相似的训练样本的类别，通过<strong>投票</strong>的方式来确定测试样本的类别；</li></ul><h3 id="常用距离度量方法"><a href="#常用距离度量方法" class="headerlink" title="常用距离度量方法"></a>常用距离度量方法</h3><h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sqrt{\sum_{i=1}^d(x_{1i}-x_{2i})^2}</script><h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sum_{i=1}^d|x_{1i}-x_{2i}|</script><h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\sqrt{(x_1-x_2)^T\Sigma^{-1}(x_1-x_2)}</script><h4 id="余弦距离"><a href="#余弦距离" class="headerlink" title="余弦距离"></a>余弦距离</h4><script type="math/tex; mode=display">d(x_1,x_2)=\frac{x_1^Tx_2}{||x_1||_2||x_2||_2}</script><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul><li>对<strong>异常数据</strong>不敏感，具有较好的<strong>抗噪性</strong>；</li><li><strong>KNN</strong>算法的计算<strong>效率低</strong>；</li><li>当<strong>训练集较小</strong>的时候，<strong>KNN</strong>算法易导致<strong>过拟合</strong>；</li></ul><h2 id="Kd树"><a href="#Kd树" class="headerlink" title="Kd树"></a>Kd树</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> KNN </tag>
            
            <tag> Supervised Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decision Tree</title>
      <link href="/2021/05/15/Decision-Tree/"/>
      <url>/2021/05/15/Decision-Tree/</url>
      
        <content type="html"><![CDATA[<h1 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h1><h3 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h3><p><strong>决策树</strong>是一种描述对<em>实例</em>分类的树状结构，由结点（node）和有向边（directed edge）组成。在进行分类时，对每个实例，从<strong>根节点</strong>（root）开始，根据<em>该实例</em>是否满足结点的调节，选择不同的分支进入下个结点，最终到达<strong>叶节点</strong>（leaf）得到分类结果。</p><h4 id="信息增益："><a href="#信息增益：" class="headerlink" title="信息增益："></a>信息增益：</h4><p><strong>熵（entropy）</strong>表示随机变量<strong>不确定性</strong>的度量，设$X$是一个取有限个值的离散随机变量，其概率分布</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Decision Tree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K-means</title>
      <link href="/2021/05/15/K-means/"/>
      <url>/2021/05/15/K-means/</url>
      
        <content type="html"><![CDATA[<h1 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h1><h2 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>K-means</strong>聚类算法是一种简单的<strong>无监督</strong>聚类方法。</p><h3 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h3><p><strong>输入</strong>：类数<strong>K</strong>，训练数据<strong>D</strong>。</p><ul><li>选择（一般随机）<strong>K</strong>个点作为初始<strong>簇中心</strong>；</li><li>将每个点指派到<strong>最近的簇中心</strong>形成<strong>K个簇</strong>；</li><li>重新计算每个<strong>簇</strong>的簇中心；</li><li>重复上述步骤直到<strong>簇中心不发生变化</strong>；</li></ul><p><strong>算法特点</strong>：</p><ul><li>简单、直观；</li><li>聚类结果依赖于<strong>𝐾</strong>个初始簇中心的选择；</li><li>容易陷入局部最优，不易处理非簇状数据；</li><li>聚类结果容易受离群值影响；</li></ul><p><strong>时间复杂度</strong>：</p><script type="math/tex; mode=display">O(mnkt)</script><p>其中，<script type="math/tex">m</script>：数据维度；<script type="math/tex">n</script>：样本个数；<script type="math/tex">k</script>：类别数；<script type="math/tex">t</script>：迭代次数。</p><p><img src="/2021/05/15/K-means/sphx_glr_plot_kmeans_assumptions_001.png" alt="Incorrect Number of Blobs, Anisotropicly Distributed Blobs, Unequal Variance, Unevenly Sized Blobs"></p><h2 id="改进模型"><a href="#改进模型" class="headerlink" title="改进模型"></a>改进模型</h2><h3 id="K-medoids"><a href="#K-medoids" class="headerlink" title="K-medoids"></a>K-medoids</h3><p><strong>K-medoids</strong>改进方法：</p><ul><li>选取每个簇中到<strong>簇内其他点</strong>的<strong>距离和最小</strong>的样本作为簇的中心。</li></ul><h3 id="二分K-means"><a href="#二分K-means" class="headerlink" title="二分K-means"></a>二分K-means</h3><p><strong>Bi K-means</strong>改进方法：</p><ul><li>每次从所有簇中选择能最大限度降低<strong>损失函数</strong>的簇，然后将该簇<strong>划分为两个簇</strong>，重复该过程，直至簇集合中含有<strong>𝐾</strong>个簇；</li></ul><h3 id="K-median"><a href="#K-median" class="headerlink" title="K-median"></a>K-median</h3><p><strong>K-median</strong>改进方法：</p><ul><li>距离度量使用<strong>曼哈顿距离</strong>；</li><li>每次<strong>簇中心</strong>使用簇的中位数；</li></ul><h2 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h2><h3 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h3><ul><li><strong>紧凑度</strong>：衡量簇内样本之间是否足够紧凑；</li><li><strong>分离度</strong>：衡量不同簇之间距离是否足够远；</li></ul><h3 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h3><ul><li>将<strong>聚类</strong>结果与其他模型进行比较；</li></ul><h3 id="轮廓系数"><a href="#轮廓系数" class="headerlink" title="轮廓系数"></a>轮廓系数</h3><p><strong>轮廓系数</strong>是聚类效果好坏的一种评价方式。它结合<strong>紧凑度</strong>和<strong>分离度</strong>两种因素。可以用来在相同原始数据的基础上用来评价不同算法，或者算法不同运行方式对聚类结果所产生的影响。</p><h4 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h4><ul><li><p><strong>簇内不相似度</strong>：计算样本<script type="math/tex">x_i</script>到同簇其他样本的平均距离<script type="math/tex">a_i</script>，称为样本<script type="math/tex">x_i</script>的<strong>簇内不相似度</strong>。</p><script type="math/tex; mode=display">a_i=\frac{1}{N-1}\sum_{j \ne i}d(x_i,x_j)</script><p>  <script type="math/tex">a_i</script>越小，说明样本<script type="math/tex">x_i</script>越应该被聚类到该簇。</p></li><li><p><strong>簇不相似度</strong>：簇<strong>C</strong>中所有样本的<strong>簇内不相似度</strong>的<strong>均值</strong>。</p></li><li><p><strong>不相似度</strong>：样本<script type="math/tex">x_i</script>到其他某簇<script type="math/tex">C_j</script>的<strong>所有样本</strong>的<strong>平均距离</strong><script type="math/tex">b_{iC_j}</script>，称为样本<script type="math/tex">x_i</script>与簇<script type="math/tex">C_j</script>的<strong>不相似度</strong>。</p><script type="math/tex; mode=display">b_{iC_j}=\frac{1}{N_{C_j}}\sum_{x\in C_j}d(x_i,x)</script></li><li><p><strong>簇间不相似度</strong>：样本<script type="math/tex">x_i</script>与其他簇的<strong>不相似度</strong>的<strong>最小值</strong><script type="math/tex">b_i</script>。</p><script type="math/tex; mode=display">b_i=\min(b_1,\cdots,b_N)</script><p>  <script type="math/tex">b_i</script>越大，说明样本<script type="math/tex">x_i</script>越不属于其他簇。</p></li><li><p><strong>轮廓系数</strong>：</p><script type="math/tex; mode=display">s(x_i)=\frac{b_i-a_i}{\max(a_i,b_i)}</script></li></ul><h4 id="判断方法"><a href="#判断方法" class="headerlink" title="判断方法"></a>判断方法</h4><ul><li><script type="math/tex">s(x_i)</script>越接近<strong>1</strong>，则说明样本<script type="math/tex">x_i</script>聚类越合理；</li><li><script type="math/tex">s(x_i)</script>越接近<strong>-1</strong>，则说明样本<script type="math/tex">x_i</script>越应该分类到另外的簇；</li><li><script type="math/tex">s(x_i)</script>近似为<strong>0</strong>，则说明样本<script type="math/tex">x_i</script>在两个簇的<strong>边界</strong>上。</li></ul><h3 id="兰德指数"><a href="#兰德指数" class="headerlink" title="兰德指数"></a>兰德指数</h3><p><strong>兰德指数（Rand index）</strong>需要给定<strong>实际标签C</strong>，假设<script type="math/tex">y_i,y_j</script>是<script type="math/tex">x_i,x_j</script>聚类结果，<script type="math/tex">y_i^*,y_j^*</script>是实际类别。则，<script type="math/tex">a</script>表示<script type="math/tex">y_i^*=y_j^*\and y_i=y_j</script>的对数，<script type="math/tex">b</script>表示<script type="math/tex">y_i^*\ne y_j^*\and y_i\ne y_j</script>的对数：</p><script type="math/tex; mode=display">\mathrm{RI}=\frac{a + b}{C_{|C|}^2}</script><h4 id="判断方法-1"><a href="#判断方法-1" class="headerlink" title="判断方法"></a>判断方法</h4><ul><li><strong>兰德指数</strong>取值范围为<strong>[0, 1]</strong>，值越大意味着聚类结果与真实情况越吻合；</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Cluser </tag>
            
            <tag> K-means </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Discretization</title>
      <link href="/2021/05/15/Discretization/"/>
      <url>/2021/05/15/Discretization/</url>
      
        <content type="html"><![CDATA[<h1 id="数据离散化"><a href="#数据离散化" class="headerlink" title="数据离散化"></a>数据离散化</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>将<strong>连续性特征</strong>转换成为<strong>离散型特征</strong>的过程称为<strong>特征离散化（data discretization）</strong>。</p><h2 id="无监督离散化"><a href="#无监督离散化" class="headerlink" title="无监督离散化"></a>无监督离散化</h2><h3 id="等距离散化"><a href="#等距离散化" class="headerlink" title="等距离散化"></a>等距离散化</h3><p>该方法根据<strong>连续特征</strong>的取值，将其均匀地划分成<script type="math/tex">k</script>个区间，每个区间的<strong>区间宽度<script type="math/tex">w</script></strong>相等：</p><script type="math/tex; mode=display">w=\frac{x_{max}-x_{min}}{k}</script><p><strong>特点</strong>：</p><ul><li>对数据要求高；</li><li>对离群值敏感；</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cut()</span><br></pre></td></tr></tbody></table></figure><h3 id="等频离散化"><a href="#等频离散化" class="headerlink" title="等频离散化"></a>等频离散化</h3><p>根据<strong>连续性特征</strong>取值的总数是<script type="math/tex">N</script>，仍然将其划分为<script type="math/tex">k</script>个区段，每个区段包含的数据个数<script type="math/tex">n</script>为:</p><script type="math/tex; mode=display">n=\frac{N}{k}</script><p><strong>特点</strong>：</p><ul><li>保证了每个区间段有相同的样本数；</li><li>取值相近的样本会被划分到不同区间；</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="聚类离散化"><a href="#聚类离散化" class="headerlink" title="聚类离散化"></a>聚类离散化</h3><p>离散化<strong>连续性特征</strong>时，如果<strong>相似的样本</strong>能落到<strong>相同的区间段</strong>内，则这样的划分可以更好地代表原始数据的信息，因此可以考虑利用<strong>聚类</strong>对连续性特征进行离散化处理。</p><h2 id="有监督离散化"><a href="#有监督离散化" class="headerlink" title="有监督离散化"></a>有监督离散化</h2><h3 id="信息增益离散化"><a href="#信息增益离散化" class="headerlink" title="信息增益离散化"></a>信息增益离散化</h3><p>该方法源自于<strong>决策树</strong>模型，在建立决策树时，遍历每一个特征，选择熵最小也就是<strong>信息增益最大</strong>的特征作为正式分裂节点。</p><h4 id="算法步骤："><a href="#算法步骤：" class="headerlink" title="算法步骤："></a>算法步骤：</h4><ul><li>对连续型特征进行排序；</li><li>把特征的每一个取值作为候选<strong>切分点</strong>，计算出相应的<strong>熵</strong>，选择熵最小的取值作为正式的切分点，将原来的区间一分为二；</li><li>递归处理第二步中得到的两个新区间段，直到每个区间段内特征的类别一样为止；</li><li>合并相邻的，<strong>类的熵值为0且特征类别相同</strong>的区段，重新计算新区间段类的熵值；</li><li>重复第四步到满足终止条件（决策树的深度或叶子数）；</li></ul><h3 id="卡方离散化"><a href="#卡方离散化" class="headerlink" title="卡方离散化"></a>卡方离散化</h3><p>基于<strong>卡方</strong>的离散化方法是采用<strong>自底向上</strong>的合并策略，首先将特征的取值看作单独的区间，然后逐一递归进行区间合并。</p><h4 id="ChiMerge方法步骤："><a href="#ChiMerge方法步骤：" class="headerlink" title="ChiMerge方法步骤："></a>ChiMerge方法步骤：</h4><ul><li>将连续型特征的每一个取值看作是一个单独的区间段，并进行排序；</li><li>针对每对相邻的区间段，计算卡方统计量。卡方值最小或者低于设定阈值的相邻区间段合并：</li><li>对于新的区间段，递归进行第1,2步，只到满足终止条件；</li></ul><h3 id="类别属性依赖最大化（CAIM）离散化"><a href="#类别属性依赖最大化（CAIM）离散化" class="headerlink" title="类别属性依赖最大化（CAIM）离散化"></a>类别属性依赖最大化（CAIM）离散化</h3><p>它采取<strong>自顶向下</strong>的策略。通过选择切分点<script type="math/tex">p</script>，把特征的取值空间划分为<script type="math/tex">x\le p</script>和<script type="math/tex">x \gt p</script>两个子区间段, 用来衡量切分点选择优劣的度量方法是类别属性的<strong>相互依赖程度</strong>。</p><p>假设某个<strong>连续型特征</strong>有<script type="math/tex">n</script>个取值，<script type="math/tex">C</script>个类别. 假设我们把特征划分为<script type="math/tex">k</script>个<strong>子区间段</strong>，子区间段集合记为：</p><script type="math/tex; mode=display">D=\{[x_0,x_1],(x_1,x_2],\cdots, (x_{k-1},x_k]\}</script><p>其中，<script type="math/tex">x_0,x_k</script>分别为特征的<strong>最小值</strong>和<strong>最大值</strong>。</p><p>设，<script type="math/tex">n_{i\cdot}</script>表示<strong>属于类别i</strong>的样本个数，<script type="math/tex">n_{\cdot j}</script>表示落在<strong>区间段</strong><script type="math/tex">(x_{j-1},x_j]</script>的样本个数，<script type="math/tex">n_{ij}</script>表示在区间内<script type="math/tex">(x_{j-1},x_j)</script>的且属于类别<script type="math/tex">\textbf{i}</script>的样本个数. 我们可以得到一个由<strong>类别特征</strong>和<strong>离散化特征取值</strong>所构成的二维表：</p><div class="table-container"><table><thead><tr><th>类别</th><th style="text-align:center"><script type="math/tex">[x_0,x_1]</script></th><th style="text-align:center"><script type="math/tex">(x_1,x_2]</script></th><th style="text-align:center">……</th><th style="text-align:center"><script type="math/tex">(x_{k-1},x_k]</script></th><th style="text-align:center">类别样本数</th></tr></thead><tbody><tr><td><strong>1</strong></td><td style="text-align:center"><script type="math/tex">n_{11}</script></td><td style="text-align:center"><script type="math/tex">n_{12}</script></td><td style="text-align:center">……</td><td style="text-align:center"><script type="math/tex">n_{1k}</script></td><td style="text-align:center"><script type="math/tex">n_{1\cdot}</script></td></tr><tr><td><strong>2</strong></td><td style="text-align:center"><script type="math/tex">n_{21}</script></td><td style="text-align:center"><script type="math/tex">n_{22}</script></td><td style="text-align:center">……</td><td style="text-align:center"><script type="math/tex">n_{2k}</script></td><td style="text-align:center"><script type="math/tex">n_{2\cdot}</script></td></tr><tr><td><script type="math/tex">\vdots</script></td><td style="text-align:center"><script type="math/tex">\vdots</script></td><td style="text-align:center"><script type="math/tex">\vdots</script></td><td style="text-align:center"><script type="math/tex">\ddots</script></td><td style="text-align:center"><script type="math/tex">\vdots</script></td><td style="text-align:center"><script type="math/tex">\vdots</script></td></tr><tr><td><strong>C</strong></td><td style="text-align:center"><script type="math/tex">n_{C1}</script></td><td style="text-align:center"><script type="math/tex">n_{C2}</script></td><td style="text-align:center">……</td><td style="text-align:center"><script type="math/tex">n_{Ck}</script></td><td style="text-align:center"><script type="math/tex">n_{C\cdot}</script></td></tr><tr><td><strong>区间样本数</strong></td><td style="text-align:center"><script type="math/tex">n_{\cdot 1}</script></td><td style="text-align:center"><script type="math/tex">n_{\cdot 2}</script></td><td style="text-align:center">……</td><td style="text-align:center"><script type="math/tex">n_{\cdot k}</script></td><td style="text-align:center"><script type="math/tex">n_{\cdot}</script></td></tr></tbody></table></div><p>则：</p><script type="math/tex; mode=display">\mathrm{CAIM}=\frac{1}{N}\sum_{j=1}^k\frac{M_j^2}{n_{\cdot j}},M_j=\max(\{n_{1j},n_{2j},\cdots,n_{Cj}\})</script><p><strong>CAIM</strong>的值<strong>越大</strong>，说明类和离散区间的<strong>相互依赖程度越大</strong>，也就说明了<strong>离散化效果越好</strong>。</p><h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><ul><li>对进行离散化的<strong>特征</strong>进行<strong>升序排列</strong>，确定取值区间的<strong>最小值</strong><script type="math/tex">x_0</script>和<strong>最大值</strong><script type="math/tex">x_k</script>，初始化划分策略<script type="math/tex">D=\{[d_0,d_k]\}</script>；</li><li>把区间内的每个值当作<strong>候选切分点</strong>，计算把<strong>区间二分</strong>后的<strong>CAIM</strong>值，并选取<strong>CAIM值最高</strong>的点作为<strong>切分点</strong>，并更新<script type="math/tex">D</script>；</li><li>对于<script type="math/tex">D</script>中的每个<strong>区间段</strong>，重复第二步的计算，直到满足终止条件；</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> Feature Engineering </tag>
            
            <tag> Discretization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models</title>
      <link href="/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/"/>
      <url>/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/</url>
      
        <content type="html"><![CDATA[<h1 id="Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models1"><a href="#Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models1" class="headerlink" title="Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models1"></a>Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models<a href="#refer-anchor-1"><sup>1</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>目前最好的方式是将<strong>对话系统</strong>看成一个<strong>部分可观测的马尔可夫决策过程（Partially Observable Markov Decision Process，POMDP）</strong>。</p><p>但是大部分部署的<strong>对话系统</strong>都是基于<strong>人工指定的关于状态和行为的特征</strong>。</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><p>将一个<strong>对话（dialogue）</strong>看成由两个<strong>对话者（interlocutor）</strong>发出的<script type="math/tex">M</script>个<strong>话语（utterance）</strong><script type="math/tex">D={U_1,\cdots,U_M}</script>构成的<strong>序列（sequence）</strong>。其中每个<strong>话语</strong><script type="math/tex">U_m=\{w_{m,1},\cdots,w_{m,n}\}</script>，其中<script type="math/tex">w_{m,n}</script>表示第<script type="math/tex">m</script>句话的第<script type="math/tex">n</script>个位置的<strong>token（包括单词和说话动作speech act）</strong>。</p><p>因此，一段对话<script type="math/tex">D</script>可以被分解为：</p><script type="math/tex; mode=display">P_\theta(U_1,\cdots,U_M)=\prod_{m=1}^MP_\theta(U_m|U_{<m})</script><script type="math/tex; mode=display">=\prod_{m=1}^M\prod_{n=1}^NP_\theta(w_{m,n}|w_{m,<n},U_{<m})</script><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><script type="math/tex; mode=display">h_n=f(h_{n-1}, w_n)=\tanh(Hh_{n-1}+I_{w_n})</script><p>其中，<script type="math/tex">h_n\in\mathbb{R}^{d_h}</script>是<strong>隐状态</strong>，<script type="math/tex">I</script>是词向量，是对从1到n的语句片段的表征。</p><p>对下一个单词的预测：</p><script type="math/tex; mode=display">P_\theta(w_{n+1}=v|w\le n)=\frac{\exp(g(h_n,v))}{\sum_{v'}\exp(g(h_n,v'))}</script><script type="math/tex; mode=display">g(h_n,v)=O^T_{w_n}h_n</script><p>其中，<script type="math/tex">O</script>是词向量。</p><h3 id="Hierarchical-Recurrent-Encoder-Decoder"><a href="#Hierarchical-Recurrent-Encoder-Decoder" class="headerlink" title="Hierarchical Recurrent Encoder-Decoder"></a>Hierarchical Recurrent Encoder-Decoder</h3><p>每个对话看成由<strong>语句</strong>组成，而每个<strong>语句</strong>又由<strong>字词</strong>组成。所以可以利用<strong>RNN</strong>在两个层次建模。<img src="/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/image-20210509200101676.png" alt="image-20210509200101676"></p><ul><li><strong>encoder RNN</strong>：将每个<strong>语句</strong>编码成一个<strong>语句向量（utterance vector）</strong>【最后的hidden state】；</li><li><strong>context RNN</strong>：记录经过的每个语句；</li><li><strong>decoder RNN</strong>：进行下一个语句的预测。</li></ul><h3 id="Bi-HRED"><a href="#Bi-HRED" class="headerlink" title="Bi-HRED"></a>Bi-HRED</h3><p>将上面的<strong>encoder RNN</strong>变成双向的。最终的<strong>语句向量</strong>，有两种方法取的：</p><ul><li>直接将双向RNN最后的<strong>hidden state</strong>拼接起来；</li><li>采用<script type="math/tex">L_2</script><strong>池化</strong>：<script type="math/tex">\sqrt{\frac{\sum_{n=1}^{N_m}h_n^2}{N_m}}</script>，<script type="math/tex">h_n</script>是位置n的<strong>hidden state</strong>，然后再拼接；</li></ul><h3 id="Bootstrapping-from-Word-Embeddings-and-Subtitles-Q-A"><a href="#Bootstrapping-from-Word-Embeddings-and-Subtitles-Q-A" class="headerlink" title="Bootstrapping from Word Embeddings and Subtitles Q-A"></a>Bootstrapping from Word Embeddings and Subtitles Q-A</h3><ul><li>使用在包括1000亿个单词的<strong>Google News</strong>数据集上训练的<strong>Word2Vec</strong>词向量，引入常识知识；</li><li>在<strong>非对话语料库</strong>上进行<strong>预训练</strong>；</li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Evaluation-Metrics"><a href="#Evaluation-Metrics" class="headerlink" title="Evaluation Metrics"></a>Evaluation Metrics</h3><p>定义<strong>单词困惑度（word perplexity）</strong>：</p><script type="math/tex; mode=display">\exp{(-\frac{1}{N_w}\sum_{n=1}^N\log P_\theta(U_1^n,U_2^n,U_3^n))}</script><p>其中，<script type="math/tex">\theta</script>是参数，<script type="math/tex">\{U_1^n,U_2^n,U_3^n\}_{n=1}^N</script>是三元组的数据集，<script type="math/tex">N_w</script>是整个数据集的单词个数。</p><p><strong>困惑度越低模型越好</strong>。</p><h3 id="MAP-Output"><a href="#MAP-Output" class="headerlink" title="MAP Output"></a>MAP Output</h3><p>利用<strong>集束搜索（beam search）</strong>得到预测结果：</p><p><img src="/2021/05/09/Building-End-to-End-Dialogue-Systems-Using-Generative-Hierarchical-Neural-Network-Models/image-20210509203744611.png" alt="image-20210509203744611"></p><p>发现：结果比较合理，但大部分结果都是<strong>客套话（generic）</strong>，比如<em>I don’t know,I’m sorry</em>。</p><p>原因：</p><ul><li><strong>数据不够（data scarcity）</strong>：该模型比较大，可能需要更多数据；</li><li><strong>大部分token是标点符号代词（pronouns）</strong>：训练时，每个token都是地位相同的，导致模型训练不够好；</li><li><strong>三元组太短了</strong>：模型需要更多的信息。</li></ul><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1507.04808v2">Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> Dialogue </tag>
            
            <tag> RNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sklearn Naive Bayes</title>
      <link href="/2021/04/28/Sklearn-Naive-Bayes/"/>
      <url>/2021/04/28/Sklearn-Naive-Bayes/</url>
      
        <content type="html"><![CDATA[<h1 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h1>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Sklearn </tag>
            
            <tag> Naive Bayes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Consistency by Agreement in Zero-shot Neural Machine Translation</title>
      <link href="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/"/>
      <url>/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/</url>
      
        <content type="html"><![CDATA[<h1 id="Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation"><a href="#Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation" class="headerlink" title="Consistency by Agreement in Zero-shot Neural Machine Translation[*]"></a>Consistency by Agreement in Zero-shot Neural Machine Translation<a href="#refer-*"><sup>[*]</sup></a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>一般对于多语言间（例如<strong>k</strong>中语言）间的互相翻译，需要训练<strong>k*k</strong>个<strong>NMT</strong>。</p><p>后来有人提出<strong>zero-shot translation</strong>，即给定三种语言<strong>German（De）、English（En）、French（Fr）</strong>，然后在<strong>平行文本（De，En）、（En，Fr）</strong>，同时使得该模型能够学得<strong>（De，Fr）</strong>翻译的方法。</p><ul><li><strong>零射学习（zero-shot learning）</strong>：对学习目标不提供任何样本。</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="定义【1】（Expected-Zero-shot-Consistency）"><a href="#定义【1】（Expected-Zero-shot-Consistency）" class="headerlink" title="定义【1】（Expected Zero-shot Consistency）"></a>定义【1】（Expected Zero-shot Consistency）</h3><p>令<script type="math/tex">\varepsilon_s\ \varepsilon_0</script>分别表示<strong>监督（supervised）</strong>和<strong>零射（zero-shot）</strong>任务，<script type="math/tex">\mathcal{l(\cdot)}</script>表示非负的<strong>损失函数</strong>，<script type="math/tex">\mathcal{M}</script>是使用<strong>最大期望监督损失（maximum expected supervised loss）</strong>的模型，并满足下列约束：</p><script type="math/tex; mode=display">\mathrm{max}_{(i,j)\in \varepsilon_s}\mathbb{E}_{x_i,x_j}[l(\mathcal{M})]<\epsilon</script><p>当对于<script type="math/tex">k(\epsilon)>0</script>满足：</p><script type="math/tex; mode=display">\max_{(i,j)\in \varepsilon_0}\mathbb{E}_{x_i,x_j}[l(M)]<k(\epsilon)</script><p>且当<script type="math/tex">\epsilon\to0</script>时，<script type="math/tex">k(\epsilon)\to 0</script></p><p>直观上说明，就是如果<strong>监督任务上的低loss会使零射任务上的loss降低</strong>则称<strong>zero-shot consistent</strong>。</p><h3 id="新方法"><a href="#新方法" class="headerlink" title="新方法"></a>新方法</h3><p>考虑对于四种语言（En，Es，Fr，Ru）的任务：</p><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426201028430.png" alt="image-20210426201028430" style="zoom:67%;"></p><p>考虑对<strong>En-Fr</strong>的联合似然：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}^{ind}(\theta)=\log[\mathbb{P}_\theta(\mathrm{x}_{Fr}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{x}_{En}|\mathrm{x}_{Fr})]</script><script type="math/tex; mode=display">=\log[\sum_{z'_{Es},z'_{Ru}}\mathbb{P}_\theta(\mathrm{x}_{Fr},\mathrm{z}'_{Es},\mathrm{z}'_{Ru}|\mathrm{x}_{En})]\times</script><script type="math/tex; mode=display">\sum_{z''_{Es},z''_{Ru}}\mathbb{P}_\theta(\mathrm{x}_{En},\mathrm{z}''_{Es},\mathrm{z}''_{Ru}|\mathrm{x}_{Fr})</script><p>其中<strong>Es，Ru</strong>是<strong>隐译文（latent translations）</strong>，同时这里假设<script type="math/tex">En\to Fr</script>和<script type="math/tex">Fr\to En</script>是<strong>相互独立</strong>的。</p><p>为了简便起见，令<script type="math/tex">\mathrm{z}:=(\mathrm{z}_{Es},\mathrm{z}_{Ru})</script></p><p>于是得到下面简化的公式：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}^{agree}(\theta)=\log\sum_\mathrm{z}[\mathbb{P}_\theta(\mathrm{x}_{Fr},\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{x}_{En},\mathrm{z}|\mathrm{x}_{Fr})] \tag{2}</script><p>接着利用下列公式：</p><script type="math/tex; mode=display">\mathbb{P}(x,z|y)=\mathbb{P}(x|z,y)\mathbb{P}(z|y)</script><script type="math/tex; mode=display">\mathbb{P}(x_{Fr}|z,x_{En})\approx \mathbb{P}(x_{Fr}|x_{En})</script><p>将<font color="red">(2)</font>式变形为：</p><script type="math/tex; mode=display">\mathcal{L}_{EnFr}(\theta)\approx \tag{3}</script><script type="math/tex; mode=display">\log \mathbb{P}_\theta(\mathrm{x}_{Fr}|\mathrm{x}_{En})+\log\mathbb{P}_\theta(\mathrm{x}_{En}|\mathrm{x}_{Fr})+\  (composite\ likelihood\ terms)</script><script type="math/tex; mode=display">\log\sum_\mathrm{z}\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{Fr})\ (agreement\ term)</script><h4 id="Lower-bound"><a href="#Lower-bound" class="headerlink" title="Lower bound"></a>Lower bound</h4><p>对上式的<strong>(agreement term)</strong>利用<strong>Jensen不等式</strong>得到下面不等式：</p><script type="math/tex; mode=display">\log\sum_\mathrm{z}\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{En})\mathbb{P}_\theta(\mathrm{z}|\mathrm{x}_{Fr})</script><script type="math/tex; mode=display">\ge \mathbb{E}_{\mathrm{z}_{Es}|\mathrm{x}_{En}}[\log \mathbb{P}_\theta(\mathrm{z}_{Es}|\mathrm{x}_{Fr})]+</script><script type="math/tex; mode=display">\mathbb{E}_{\mathrm{z}_{Ru}|\mathrm{x}_{En}}[\log\mathbb{P}_\theta(\mathrm{z}_{Ru}|\mathrm{x}_{Fr})]</script><h3 id="定理【2】（Agreement-Zero-shot-Consistency）"><a href="#定理【2】（Agreement-Zero-shot-Consistency）" class="headerlink" title="定理【2】（Agreement Zero-shot Consistency）"></a>定理【2】（Agreement Zero-shot Consistency）</h3><script type="math/tex; mode=display">L_1,L_2,L_3$$是三种语言，其中$$L_1 \leftrightarrow L_2,L_2\leftrightarrow L_3$$是监督学习，$$L_1\leftrightarrow L_3$$是零射学习的内容。如果$$\mathbb{E}_{\mathrm{x_1,x_2,x_3}}[\mathcal{L}_{12}^{agree}(\theta)+\mathcal{L}_{23}^{agree}{(\theta)}]$$存在边界$$\epsilon>0$$，则：$$\mathbb{E}_{\mathrm{x_1,x_3}}[-\log\mathbb{P}_\theta(\mathrm{x_3}|\mathrm{x_1})]\le k(\epsilon)\ where \ k(\epsilon)\to 0\ as \ \epsilon\to 0</script><h3 id="模型总体结构"><a href="#模型总体结构" class="headerlink" title="模型总体结构"></a>模型总体结构</h3><p>给定平行句子<script type="math/tex">\mathrm{x_{En},x_{Fr}}</script>，以及辅助语言<script type="math/tex">\mathrm{x_{Es}}</script>。</p><h4 id="agreement-term"><a href="#agreement-term" class="headerlink" title="agreement term"></a>agreement term</h4><p><font color="red">(3)</font>式的<strong>agreement term</strong>按下列步骤计算：</p><p>首先将<strong>Es标签</strong>连接（concatenate）到<script type="math/tex">\mathrm{x_{En},x_{Fr}}</script>句子上，然后<strong>编码（encode）</strong>这些句子，使之翻译成<strong>Es</strong>语言</p><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426233520953.png" alt="image-20210426233520953" style="zoom:67%;"></p><p>然后，对编码的句子进行<strong>解码（decode）</strong>，得到辅助语言的翻译内容<script type="math/tex">\mathrm{z_{Es}(x_{En}),z_{Es}(x_{Fr})}</script>。</p><p>接着，用<script type="math/tex">\mathrm{(x_{Fr},z_{Es}(x_{En})),(x_{En},z_{Es}(x_{Fr}))}</script>分别作为两个<strong>平行语料</strong>训练<script type="math/tex">\mathrm{En\to Es,Fr\to Es}</script>两个翻译器，可以计算出：</p><script type="math/tex; mode=display">\log\mathbb{P}_\theta(\mathrm{z_{Es}(x_{Fr})|x_{En}})</script><script type="math/tex; mode=display">\log\mathbb{P}_\theta(\mathrm{z_{Es}(x_{En})|x_{Fr}})</script><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426234435972.png" alt="image-20210426234435972" style="zoom:67%;"></p><h4 id="算法流程如下："><a href="#算法流程如下：" class="headerlink" title="算法流程如下："></a>算法流程如下：</h4><p><img src="/2021/04/24/Consistency-by-Agreement-in-Zero-shot-Neural-Machine-Translation/image-20210426234553607.png" alt="image-20210426234553607" style="zoom:67%;"></p><p>在每一轮迭代中，我们希望：</p><ol><li>提高梯度下降对<strong>zero-shot</strong>的影响；</li><li>降低梯度下降对<strong>监督学习</strong>的影响；</li></ol><p>为此分别使用：</p><ol><li><strong>连续贪心解码（greedy continuous decoding）</strong></li><li><strong>停止梯度下降（stop-gradient）</strong></li></ol><p><strong>REF</strong></p><p></p><div id="#refer-*"></div>[*]<a href="https://www.aclweb.org/anthology/N19-1121/">Consistency by Agreement in Zero-shot Neural Machine Translation</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Neural Machine Translation </tag>
            
            <tag> Zero-shot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Triangular Architecture for Rare Language Translation</title>
      <link href="/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/"/>
      <url>/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/</url>
      
        <content type="html"><![CDATA[<h1 id="Triangular-Architecture-for-Rare-Language-Translation"><a href="#Triangular-Architecture-for-Rare-Language-Translation" class="headerlink" title="Triangular Architecture for Rare Language Translation[*]"></a>Triangular Architecture for Rare Language Translation<a href="#refer-*"><sup>[*]</sup></a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>作者提出了一种引入<strong>（Y，Z），（X，Y）</strong>平行文本，来增强<strong>（X，Z）</strong>语言机器翻译的<strong>三角形结构</strong>模型<strong>TA-NMT</strong>，。</p><p><img src="/2021/04/23/Triangular-Architecture-for-Rare-Language-Translation/image-20210423161225412.png" alt="image-20210423161225412" style="zoom:67%;"></p><p><strong>实线</strong>代表丰富的文本，<strong>虚线</strong>代表少量的文本。</p><p>该论文主要思路是将<script type="math/tex">X\to Y</script>分解成训练两个少量文本的模型<script type="math/tex">X\to Z</script>和<script type="math/tex">Y\to Z</script>。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ul><li><p><strong>NMT</strong>首先将输入句子<strong>encode</strong>成<strong>vector</strong>，然后<strong>decoder</strong>在其基础上生成目标句子；</p></li><li><p>为了解决<strong>数据少（data sparsity）</strong>的问题，提出了以下几种方法：</p><ul><li><strong>单语言文本（monolingual data）</strong>；</li><li><strong>反向翻译（back translation）</strong>；</li><li><strong>联合训练（joint training）</strong>；</li><li><strong>对偶学习（dual learning）</strong>；</li></ul></li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="X-to-Y的loss"><a href="#X-to-Y的loss" class="headerlink" title="X \to Y的loss"></a><script type="math/tex">X \to Y</script>的loss</h3><script type="math/tex; mode=display">L(\Theta;D)=\sum_{(x,y)\in D}\mathrm{log}p(y|x)</script><script type="math/tex; mode=display">=\sum_{(x,y)\in D}\mathrm{log}\sum_z p(z|x)p(y|z)</script><script type="math/tex; mode=display">=\sum_{(x,y)\in D}\mathrm{log}\sum_zQ(z)\frac{p(z|x)p(y|z)}{Q(z)}\tag{1}</script><script type="math/tex; mode=display">\ge \sum_{(x,y)\in D}\sum_zQ(z)\mathrm{log}\frac{p(z|x)p(y|z)}{Q(z)}\ (Jensen's\ Inequation)</script><script type="math/tex; mode=display">=\mathcal{L}(Q)</script><p><strong>其中</strong>：</p><ul><li><script type="math/tex">Q(z)</script>是<script type="math/tex">z</script>的<strong>后验分布</strong>，满足<script type="math/tex">\sum_zQ(z)=1</script>；</li><li><script type="math/tex">p(y|x,z) \approx p(y|z)</script>；</li><li><script type="math/tex">D</script>：整个训练集；</li></ul><h3 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h3><p>用<strong>EM算法</strong>来最大化<strong>下界</strong><script type="math/tex">\mathcal{L}(Q)</script>。</p><p>这里令<script type="math/tex">Q(z)=p(z|x)</script>来近似：</p><h4 id="M步"><a href="#M步" class="headerlink" title="M步"></a>M步</h4><script type="math/tex; mode=display">\Theta_{y|z}=\mathrm{argmax}_{\Theta_{y|x}}\mathcal{L}(Q)</script><script type="math/tex; mode=display">=\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}\sum_z p(z|x)\mathrm{log}p(y|z)</script><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}E_{z\sim p(z|x)\mathrm{log}p(y|z)} \tag{2}</script><h4 id="E步"><a href="#E步" class="headerlink" title="E步"></a>E步</h4><p>因为：<script type="math/tex">L(\Theta;D)-\mathcal{L}(Q)=\sum_zQ(z)\mathrm{log}\frac{Q(z)}{p(z|y)}</script></p><script type="math/tex; mode=display">=KL(Q(z)||p(z|y))=KL(p(z|x)||p(z|y))</script><p>所以在<strong>E步</strong>，需要最小化上式：</p><script type="math/tex; mode=display">\Theta_{z|x}=\mathrm{argmin}_{\Theta_{z|x}}KL(p(z|x)||p(z|y)) \tag{4}</script><h3 id="双向训练"><a href="#双向训练" class="headerlink" title="双向训练"></a>双向训练</h3><h4 id="X-to-Y方向"><a href="#X-to-Y方向" class="headerlink" title="X\to Y方向"></a><script type="math/tex">X\to Y</script>方向</h4><h5 id="E：最优化-Theta-z-x"><a href="#E：最优化-Theta-z-x" class="headerlink" title="E：最优化\Theta_{z|x}"></a>E：最优化<script type="math/tex">\Theta_{z|x}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{z|x}}KL(p(z|x)||p(z|y)) \tag{5}</script><h5 id="M：最优化-Theta-y-z"><a href="#M：最优化-Theta-y-z" class="headerlink" title="M：最优化\Theta_{y|z}"></a>M：最优化<script type="math/tex">\Theta_{y|z}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{y|z}}\sum_{(x,y)\in D}E_{z\sim p(z|x)}\mathrm{log}p(y|z) \tag{6}</script><h4 id="Y-to-X方向"><a href="#Y-to-X方向" class="headerlink" title="Y\to X方向"></a><script type="math/tex">Y\to X</script>方向</h4><h5 id="E：最优化-Theta-z-y"><a href="#E：最优化-Theta-z-y" class="headerlink" title="E：最优化\Theta_{z|y}"></a>E：最优化<script type="math/tex">\Theta_{z|y}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{z|y}}KL(p(z|y)||p(z|x)) \tag{7}</script><h5 id="M：最优化-Theta-x-z"><a href="#M：最优化-Theta-x-z" class="headerlink" title="M：最优化\Theta_{x|z}"></a>M：最优化<script type="math/tex">\Theta_{x|z}</script></h5><script type="math/tex; mode=display">\mathrm{argmax}_{\Theta_{x|z}}\sum_{(x,y)\in D}E_{z\sim p(z|y)}\mathrm{log}p(x|z) \tag{8}</script><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>训练过程的难点在于<strong>分解候选词的搜索空间（exponential search space of the translation candidates）</strong>，这里使用<strong>采样</strong>方法。</p><p>将<font color="red">(5)(7)</font>式改写成如下形式：</p><script type="math/tex; mode=display">\bigtriangledown_{\Theta_{z|x}}KL(p(z|x)||p(z|y))</script><script type="math/tex; mode=display">=E_{z\sim p(z|x)}\mathrm{log}\frac{p(z|x)}{p(z|y)}\bigtriangledown_{\Theta_{z|x}}\mathrm{log}p(z|x)\tag{9}</script><script type="math/tex; mode=display">\bigtriangledown_{\Theta_{z|y}}KL(p(z|y)||p(z|x))</script><script type="math/tex; mode=display">=E_{z\sim p(z|y)}\mathrm{log}\frac{p(z|y)}{p(z|x)}\bigtriangledown_{\Theta_{z|y}}\mathrm{log}p(z|y)\tag{10}</script><p>因为数据的<strong>target</strong>是由模型生成的，效果不好，使用不使用<strong>BLEU</strong>，而是使用<strong>IBM</strong>loss函数。</p><p><strong>REF</strong>:</p><p></p><div id="#refer-*"></div>[*]<a href="https://arxiv.org/abs/1805.04813">Triangular Architecture for Rare Language Translation</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Monolingual Data </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KL-Divergence</title>
      <link href="/2021/04/20/KL-Divergence/"/>
      <url>/2021/04/20/KL-Divergence/</url>
      
        <content type="html"><![CDATA[<h1 id="KL-Divergence"><a href="#KL-Divergence" class="headerlink" title="KL Divergence"></a>KL Divergence</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h2 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h2><h3 id="连续形式"><a href="#连续形式" class="headerlink" title="连续形式"></a>连续形式</h3><script type="math/tex; mode=display">\mathrm{KL}(P||Q)=\int_x P(x)\mathrm{log}\frac{P(x)}{Q(x)}</script><h3 id="离散形式"><a href="#离散形式" class="headerlink" title="离散形式"></a>离散形式</h3><script type="math/tex; mode=display">\mathrm{KL}(P||Q)=\sum_{i=1}^N P(x_i)\mathrm{log}\frac{P(x_i)}{Q(x_i)}</script><h2 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h2><p>只有在<script type="math/tex">P(x)=Q(x)</script>时等于0，其他任何时候都大于0。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural Machine Translation by Jointly Learning to Align and Translate</title>
      <link href="/2021/04/20/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate/"/>
      <url>/2021/04/20/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate/</url>
      
        <content type="html"><![CDATA[<h1 id="Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate"><a href="#Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate" class="headerlink" title="Neural Machine Translation by Jointly Learning to Align and Translate[*]"></a>Neural Machine Translation by Jointly Learning to Align and Translate<a href="#refer-*"><sup>[*]</sup></a></h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>本文是<strong>Attention</strong>的开山之作，主要是为了解决<strong>RNN</strong>对长句子表现不佳而发表的，论文作者之一是鼎鼎大名的<strong>图灵奖</strong>得主<strong>Bengio</strong>。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p></p><div id="#refer-*"></div>[*]<a href="https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5?p2df">Neural Machine Translation by Jointly Learning to Align and Translate</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Neural Machine Translation </tag>
            
            <tag> Jointly Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Back-Translation at Scale</title>
      <link href="/2021/04/20/Understanding-Back-Translation-at-Scale/"/>
      <url>/2021/04/20/Understanding-Back-Translation-at-Scale/</url>
      
        <content type="html"><![CDATA[<h1 id="Understanding-Back-Translation-at-Scale"><a href="#Understanding-Back-Translation-at-Scale" class="headerlink" title="Understanding Back-Translation at Scale[*]"></a>Understanding Back-Translation at Scale<a href="#refer-anchor-*"><sup>[*]</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="单语言文本"><a href="#单语言文本" class="headerlink" title="单语言文本"></a>单语言文本</h3><p>单语言文本可以用来提高<strong>统计机器翻译（statistical machine translation）</strong>的流畅度。</p><p>常用的方法有：</p><ul><li><strong>模型融合（language model fusion）</strong><a href="#refer-anchor-1"><sup>[1]</sup></a><a href="#refer-anchor-2"><sup>[2]</sup></a>；</li><li><strong>反向翻译（back translation）</strong><a href="#refer-anchor-3"><sup>[3]</sup></a>；</li><li><strong>结对学习（dual learning）</strong><a href="#refer-anchor-4"><sup>[4]</sup></a>；</li></ul><h3 id="反向翻译"><a href="#反向翻译" class="headerlink" title="反向翻译"></a>反向翻译</h3><p><strong>反向翻译</strong>需要训练一个<strong>目标语言到源语言（target-to-source）</strong>的系统，利用这个系统从<strong>目标语言</strong>的单语言文本来生成额外的<strong>平行语料（synthetic parallel data）</strong>。然后利用这些生成的语料来补充<strong>人工语料（human bitext）</strong>来训练<strong>源语言到目标语言（source-to-target）</strong>的系统【真正需要的】。</p><h2 id="Generating-synthetic-sources"><a href="#Generating-synthetic-sources" class="headerlink" title="Generating synthetic sources"></a>Generating synthetic sources</h2><h3 id="最大后验估计（MAP）"><a href="#最大后验估计（MAP）" class="headerlink" title="最大后验估计（MAP）"></a>最大后验估计（MAP）</h3><p>传统的<strong>MAP</strong>方法可能导致译文的<strong>丰富度（less rich）</strong>降低。</p><p><strong>集束搜索（beam）</strong>和<strong>贪心算法（greedy）</strong>主要关心模型分布的头部样本，这样就会导致生成的语句分布出现偏差。</p><h3 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h3><p>该文章提出了两种采样方法以及添加一种噪声的方法。</p><ul><li><strong>非限制性采样（unrestricted sampling）</strong>，从文本中随机采样；</li><li><strong>限制性采样（restricted sampling）</strong>，每步会挑选出可能性最大的<strong>k</strong>个词（token），然后从这些词中随机采样一个作为结果；</li><li><strong>集束搜索+噪声</strong>，向集束搜索得到的句子添加噪声<ul><li>0.1的概率删除一个单词；</li><li>0.1的概率将一个单词替换成<strong>填充单词（filler token）</strong>；</li><li>按均匀分布随机调换两个相隔不超过三个位置的单词；</li></ul></li></ul><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment[#]"></a>Experiment<a href="#refer-anchor-#"><sup>[#]</sup></a></h2><p>使用<strong>Transformer</strong>模型进行实验。</p><h2 id="Analysis"><a href="#Analysis" class="headerlink" title="Analysis"></a>Analysis</h2><h3 id="合成数据生成方法"><a href="#合成数据生成方法" class="headerlink" title="合成数据生成方法"></a>合成数据生成方法</h3><p>该使用用5种方法分别进行生成<strong>合成数据（synthetic data）</strong>：</p><ul><li><strong>贪心算法</strong>【greedy】；</li><li>k=5的<strong>集束搜索</strong>【beam】；</li><li><strong>非限制采样</strong>【sampling】；</li><li>从k=10个最高可能性中随机采样【top10】；</li><li>在集束搜索上添加噪声【beam+nosing】；</li></ul><p><img src="/2021/04/20/Understanding-Back-Translation-at-Scale/image-20210420162836335.png" alt="image-20210420162836335" style="zoom:67%;"></p><p>结果显示<strong>sampling</strong>、<strong>beam+nosing</strong>更好。</p><h4 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h4><p><strong>集束搜索</strong>关注可能性非常大的输出，这就导致文本的<strong>丰富性（richness）</strong>和<strong>多样性（diversity）</strong>降低。</p><p><strong>采样</strong>和<strong>添加噪声</strong>可以比卖你这些问题。</p><h3 id="语料的领域对结果影响"><a href="#语料的领域对结果影响" class="headerlink" title="语料的领域对结果影响"></a>语料的领域对结果影响</h3><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><p>分别将下列<strong>sampling</strong>生成的语料添加到 原来<strong>640k</strong>平行语料中，构成不同的训练集来训练相同的翻译器：</p><ul><li>剩下的平行数据【bitext】</li><li>将剩下的平行数据<strong>反向翻译</strong>得到的数据【BT-bitext】</li><li>将<strong>newscrawl</strong>经过<strong>反向翻译</strong>得到的数据【BT-news】</li></ul><p>将训练得到的翻译器在不同验证集实验，实验结果如下：</p><p><img src="/2021/04/20/Understanding-Back-Translation-at-Scale/image-20210420170028156.png" alt="image-20210420170028156"></p><p><strong>反向翻译</strong>的<strong>生成数据</strong>对模型有很大提升，特别是当数据领域相同时。</p><h3 id="平行语料缺乏情况"><a href="#平行语料缺乏情况" class="headerlink" title="平行语料缺乏情况"></a>平行语料缺乏情况</h3><h4 id="实验设置-1"><a href="#实验设置-1" class="headerlink" title="实验设置"></a>实验设置</h4><p>从训练数据中随机采样<strong>80K</strong>和<strong>640K</strong>句子对，然后分别添加由<strong>sampling</strong>和<strong>beam</strong>得到的<strong>合成文本</strong>【添加数量根据下图横坐标】。结果如下：</p><p><img src="/2021/04/20/Understanding-Back-Translation-at-Scale/image-20210420164558902.png" alt="image-20210420164558902" style="zoom:67%;"></p><h4 id="结果分析-1"><a href="#结果分析-1" class="headerlink" title="结果分析"></a>结果分析</h4><p>发现<strong>平行文本</strong>为<strong>640k</strong>和<strong>5M</strong>时，<strong>sampling</strong>都比<strong>beam</strong>好，但当<strong>平行文本</strong>是<strong>80k</strong>时相反。</p><p>原因是：</p><p><strong>80k</strong>时，模型训练效果较差，<strong>反向翻译</strong>的噪声会极大影响模型的效果。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p><strong>随机采样</strong>和<strong>集束搜索+噪声</strong>相比<strong>贪心算法</strong>和<strong>集束搜索</strong>效果更好。</p><p><strong>REF</strong>:</p><p></p><div id="refer-anchor-*"></div> [*] <a href="https://www.researchgate.net/publication/327280445_Understanding_Back-Translation_at_Scale">Understanding Back-Translation at Scale</a><p></p><p></p><div id="refer-anchor-#"></div> [#] <a href="https://github.com/pytorch/fairseq">Code</a><p></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1503.03535">On Using Monolingual Corpora in Neural Machine Translation</a><p></p><p></p><div id="refer-anchor-2"></div> [2] <a href="https://dl.acm.org/doi/10.1016/j.csl.2017.01.014">On integrating a language model into neural machine translation</a><p></p><p></p><div id="refer-anchor-3"></div> [3] <a href="https://www.aclweb.org/anthology/P16-1009/">Improving Neural Machine Translation Models with Monolingual Data</a><p></p><p></p><div id="refer-anchor-4"></div> [4] <a href="https://www.researchgate.net/publication/335400882_Semi-supervised_Learning_for_Neural_Machine_Translation">Semi-supervised Learning for Neural Machine Translation</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Back Translation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Joint Training for Neural Machine Translation Models with Monolingual Data</title>
      <link href="/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/"/>
      <url>/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/</url>
      
        <content type="html"><![CDATA[<h1 id="Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data1"><a href="#Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data1" class="headerlink" title="Joint Training for Neural Machine Translation Models with Monolingual Data1"></a>Joint Training for Neural Machine Translation Models with Monolingual Data<a href="#refer-anchor-1"><sup>1</sup></a></h1><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="背景关键词"><a href="#背景关键词" class="headerlink" title="背景关键词"></a>背景关键词</h3><ul><li><strong>encoder-decoder</strong></li><li><strong>NMT</strong></li></ul><h3 id="传统NMT面临的问题"><a href="#传统NMT面临的问题" class="headerlink" title="传统NMT面临的问题"></a>传统NMT面临的问题</h3><ul><li>传统<strong>NMT</strong>所需要的<strong>平行文本（parallel data）</strong>缺乏；</li><li>传统<strong>NMT</strong>高度依赖高质量的<strong>平行文本</strong>，并且在<strong>平行文本</strong>缺乏和<strong>特定领域（domain-specific）</strong>的任务上表现很差；</li><li>但<strong>单语言（Monolingual Data）</strong>【即该语言文本没有对应的译文数据】文本丰富，易收集且多样；</li></ul><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>本模型采用<strong>联合训练（Joint Training）</strong>和<strong>反向翻译（Back Translation）</strong>的方式，提出一种新的<strong>联合训练</strong>方法。</p><p>该模型使用两个<strong>NMT</strong>模型来<strong>联合训练</strong>：</p><p><strong>A-model</strong>：从<strong>源语言</strong>到<strong>目标语言（source-to-target）</strong>的<strong>NMT</strong>模型；</p><p><strong>B-model</strong>：从<strong>目标语言</strong>到<strong>源语言（target-to-source）</strong>的<strong>NMT</strong>模型；</p><h3 id="联合训练方法"><a href="#联合训练方法" class="headerlink" title="联合训练方法"></a>联合训练方法</h3><p>在模型开始时，利用少量的<strong>平行文本D</strong>训练出两个翻译器<strong>A-model</strong>和<strong>B-model</strong>。</p><p>然后，该模型的<strong>联合训练</strong>是采用<strong>迭代训练（iterative）</strong>的过程【如下图】。在每个迭代中：</p><ul><li><strong>B-model</strong>利用<strong>目标语言</strong>的单文本得到对应的<strong>源语言</strong>文本，该生成的平行文本再加上平行文本D作为<strong>A-model</strong>的<strong>伪训练数据（pseudo-training data）</strong>。即<strong>A-model</strong>利用<strong>B-model</strong>生成的<strong>源语言</strong>文本作为输入，而原来的单文本作为目标，进行训练；</li><li>同理，<strong>A-model</strong>利用<strong>源语言</strong>的单文本得到对应的<strong>目标语言</strong>文本，该生成的平行文本再加上平行文本D作为<strong>B-model</strong>的<strong>伪训练数据</strong>。</li></ul><p><img src="/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/image-20210419233220188.png" alt="image-20210419233220188"></p><p><strong>图中</strong>：</p><p>在第一轮迭代之前：<script type="math/tex">M^0_{x\to y},M^0_{y\to x}</script>，都是利用<strong>平行文本</strong><script type="math/tex">D=\{x^n,y^n\}</script>预训练得到；</p><p>在每一轮迭代中：</p><ul><li><script type="math/tex">M^i_{x\to y}</script>利用<script type="math/tex">X={x^{(s)}}</script>，得到翻译后的文本<script type="math/tex">Y'_i={y_i^{(s)}}</script>；</li><li><script type="math/tex">M^i_{y\to x}</script>利用<script type="math/tex">Y={y^{(s)}}</script>，得到翻译后的文本<script type="math/tex">X'_i={x_i^{(s)}}</script>；</li><li>利用<script type="math/tex">\{X_i',Y\}\cup D</script>训练模型<script type="math/tex">M^i_{x\to y}</script>；</li><li>利用<script type="math/tex">\{Y_i',X\}\cup D</script>训练模型<script type="math/tex">M^i_{y\to x}</script>；</li></ul><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><img src="/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/image-20210419234535538.png" alt="image-20210419234535538"></p><h2 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h2><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><ul><li><strong>平行语料</strong>：<script type="math/tex">D=\{(x^{(n)}, y^{(n)})\}_{n=1}^N</script></li><li><strong>目标语言的单语言语料</strong>：<script type="math/tex">Y=\{y^{(t)}\}_{t=1}^T</script></li></ul><h3 id="优化过程"><a href="#优化过程" class="headerlink" title="优化过程"></a>优化过程</h3><p>优化目标如下：</p><script type="math/tex; mode=display">L^*(\theta_{x\to y})=\sum_{n=1}^N\mathrm{log}p(y^{(n)}|x^{(n)})+\sum_{t=1}^T\mathrm{log}p(y^{(t)}) \tag{1}</script><p>右边第一项是<strong>平行语料</strong>训练的<strong>似然</strong>，第二项是单语言中<strong>目标语言一边</strong>的<strong>似然</strong>。</p><p>接着，我们引入另一模型将<strong>目标语言</strong>文本 翻译成的<strong>源语言</strong>文本 作为<strong>目标语言</strong>的<strong>隐状态（hidden state）</strong>。</p><p>然后，将<script type="math/tex">\mathrm{log}p(y^{(t)})</script>进行分解，得到下列公式：</p><script type="math/tex; mode=display">\mathrm{log}p(y^{(t)})=\mathrm{log}\sum_xp(x,y^{(t)})=\mathrm{log}\sum_xQ(x)\frac{p(x,y^{(t)})}{Q(x)}</script><script type="math/tex; mode=display">\ge \sum_xQ(x)\mathrm{log}\frac{p(x, y^{(t)})}{Q(x)}\ \  (\bold{Jensen's\ inequality})</script><script type="math/tex; mode=display">= \sum_x[Q(x)\mathrm{log}p(y^{(t)}|x)-KL(Q(x)||p(x))] \tag{2}</script><p>其中：</p><ul><li><script type="math/tex">x</script>：目标文本对应的源文本；</li><li><script type="math/tex">Q(x)</script>：<script type="math/tex">x</script>的分布的估计；</li><li><script type="math/tex">p(x)</script>：<script type="math/tex">x</script>的<strong>边缘分布（marginal distribution）</strong>；</li><li><script type="math/tex">KL(\cdot|\cdot)</script>：<a href="http://1.15.86.100/2021/04/20/KL-Divergence/"><strong>KL散度（Kullback-Leibler Divergence）</strong></a></li></ul><p>为了使得式<font color="red">(2)</font>的等号成立，<script type="math/tex">Q(x)</script>必须满足下列条件：</p><script type="math/tex; mode=display">\frac{p(x,y^{(t)})}{Q(x)}=c\ (constant) \tag{3}</script><p>同时，对上式变形得到：</p><script type="math/tex; mode=display">Q(x)=\frac{p(x,y^{(t)})}{c}=\frac{p(x,y^{(t)})}{\sum_xp(x,y^{(t)})}=p^*(x|y^{(t)})</script><p>其中<script type="math/tex">p^*(x|y^{(t)})</script>表示真实的<strong>目标语言到源语言（target-to-source）</strong>的概率，但实际上难以求出，因此使用<strong>机器翻译模型</strong>计算的<script type="math/tex">p(x|y^{(t)})</script>作为替代。</p><p>同时，<strong>KL散度</strong>可以忽略，所以有：</p><script type="math/tex; mode=display">L(\theta_{x\to y})=\sum_{n=1}^N\mathrm{log}p(y^{(n)}|x^{(n)})+\sum_{t=1}^T\sum_xp(x|y^{(t)})\mathrm{log}p(y^{(t)}|x) \tag{4}</script><p>其中第一部分是<strong>极大似然估计（MLE）</strong>，第二部分可以用<strong>EM算法</strong>估计。</p><p>综上：</p><p>对于<script type="math/tex">M_{x\to y}</script>，优化目标为：</p><script type="math/tex; mode=display">L(\theta_{x\to y})=\sum_{n=1}^N\mathrm{log}p(y^{(n)|}|x^{(n)})+\sum_{t=1}^T\mathrm{log}p(y^{(t)}|M_{y\to x}(y^{t}))</script><p>对于<script type="math/tex">M_{y\to x}</script>，优化目标为：</p><script type="math/tex; mode=display">L(\theta_{y\to x})=\sum_{n=1}^N\mathrm{log}p(x^{(n)|}|y^{(n)})+\sum_{t=1}^T\mathrm{log}p(x^{(t)}|M_{x\to y}(x^{t}))</script><p>总的优化目标为：</p><script type="math/tex; mode=display">L(\theta)=L(\theta_{x\to y})+L(\theta_{y\to x})</script><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p><img src="/2021/04/19/Joint-Training-for-Neural-Machine-Translation-Models-with-Monolingual-Data/image-20210420212934733.png" alt="image-20210420212934733"></p><p><strong>JT-NMT</strong>是本模型。</p><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1803.00353v1">Joint Training for Neural Machine Translation Modelswith Monolingual Data</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> Paper </tag>
            
            <tag> Neural Machine Translation </tag>
            
            <tag> Back Translation </tag>
            
            <tag> Semi-supervised Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nginx</title>
      <link href="/2021/04/18/Nginx/"/>
      <url>/2021/04/18/Nginx/</url>
      
        <content type="html"><![CDATA[<h1 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="配置文件位置"><a href="#配置文件位置" class="headerlink" title="配置文件位置"></a>配置文件位置</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/nginx/conf.d</span><br></pre></td></tr></tbody></table></figure><h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx start</span><br></pre></td></tr></tbody></table></figure><h3 id="停止"><a href="#停止" class="headerlink" title="停止"></a>停止</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx stop</span><br></pre></td></tr></tbody></table></figure><h3 id="重启"><a href="#重启" class="headerlink" title="重启"></a>重启</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service nginx [restart|reload|force-reload|status|configtest|rotate|upgrade]</span><br></pre></td></tr></tbody></table></figure><h3 id="查看正在监听的端口"><a href="#查看正在监听的端口" class="headerlink" title="查看正在监听的端口"></a>查看正在监听的端口</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ntlp | grep nginx</span><br></pre></td></tr></tbody></table></figure><h2 id="Nginx搭建文件服务器"><a href="#Nginx搭建文件服务器" class="headerlink" title="Nginx搭建文件服务器"></a>Nginx搭建文件服务器</h2><h4 id="新建配置文件"><a href="#新建配置文件" class="headerlink" title="新建配置文件"></a>新建配置文件</h4><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ubuntu@VM-0-2-ubuntu:/etc/nginx/conf.d$ vim file.conf</span><br></pre></td></tr></tbody></table></figure><h4 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h4><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">server {</span><br><span class="line">        listen       9000;</span><br><span class="line">        listen       [::]:9000;</span><br><span class="line">        server_name  file;</span><br><span class="line"></span><br><span class="line">        location / {</span><br><span class="line">        root /data</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        error_page 404 /404.html;</span><br><span class="line">                location = /40x.html {</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        error_page 500 502 503 504 /50x.html;</span><br><span class="line">                location = /50x.html {</span><br><span class="line">        }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h3 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h3><p><img src="/2021/04/18/Nginx/blog\blog\source\_posts\Nginx\image-20210810181320969.png" alt="image-20210810181320969"></p>]]></content>
      
      
      <categories>
          
          <category> 后端, Linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ML Metrics</title>
      <link href="/2021/04/17/ML-Metrics/"/>
      <url>/2021/04/17/ML-Metrics/</url>
      
        <content type="html"><![CDATA[<h1 id="ML-Metrics"><a href="#ML-Metrics" class="headerlink" title="ML Metrics"></a>ML Metrics</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h2 id="Classification-Metrics（分类）"><a href="#Classification-Metrics（分类）" class="headerlink" title="Classification Metrics（分类）"></a>Classification Metrics（分类）</h2><h3 id="confusion-matrix"><a href="#confusion-matrix" class="headerlink" title="confusion matrix"></a>confusion matrix</h3><p><img src="/2021/04/17/ML-Metrics/image-20210418230321354.png" alt="image-20210418230321354" style="zoom: 50%;"></p><ul><li><strong>TP（真正例）</strong>：将<strong>正类</strong>样本预测<strong>正确（为正）</strong>；</li><li><strong>FN（假反例）</strong>：将<strong>正例</strong>样本预测<strong>错误（为反）</strong>；</li><li><strong>FP（假正例）</strong>：将<strong>反例</strong>样本预测<strong>错误（为正）</strong>；</li><li><strong>TN（真反例）</strong>：将<strong>反例</strong>样本预测<strong>正确（为反）</strong>；</li></ul><h3 id="Accuracy"><a href="#Accuracy" class="headerlink" title="Accuracy"></a>Accuracy</h3><p><strong>准确率</strong></p><p><strong>预测正确</strong>的样本数占<strong>样本总数</strong>的比例。</p><script type="math/tex; mode=display">ACC=\frac{TP+TN}{TP+FN+FP+TN}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=<span class="literal">True</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Error-rate"><a href="#Error-rate" class="headerlink" title="Error rate"></a>Error rate</h3><p><strong>错误率</strong></p><p><strong>预测错误</strong>的样本数占<strong>样本总数</strong>的比例。</p><script type="math/tex; mode=display">Error=\frac{FP+FN}{TP+FN+FP+TN}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> - sklearn.metrics.accuracy_score(y_true, y_pred, *, normalize=<span class="literal">True</span>, sample_weight=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Recall"><a href="#Recall" class="headerlink" title="Recall"></a>Recall</h3><p><strong>召回率、查全率</strong>、<strong>灵敏度（sensitivity）</strong>、<strong>真阳性率</strong></p><p><strong>真实为正</strong>中<strong>预测为正</strong>的比例。</p><script type="math/tex; mode=display">Recall=\frac{TP}{TP+FN}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.recall_score(y_true, y_pred, *, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">'binary'</span>, sample_weight=<span class="literal">None</span>, zero_division=<span class="string">'warn'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Precision"><a href="#Precision" class="headerlink" title="Precision"></a>Precision</h3><p><strong>精确度、查准率</strong></p><p><strong>预测为正</strong>中<strong>真实为正</strong>的比例。</p><script type="math/tex; mode=display">Precision=\frac{TP}{TP+FP}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.precision_score(y_true, y_pred, *, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">'binary'</span>, sample_weight=<span class="literal">None</span>, zero_division=<span class="string">'warn'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Specificity"><a href="#Specificity" class="headerlink" title="Specificity"></a>Specificity</h3><p><strong>特异度</strong>、<strong>真阴性率</strong></p><p><strong>真实为反</strong>中<strong>预测为反</strong>的比例。</p><script type="math/tex; mode=display">Sepcificity=\frac{TN}{TN+FP}</script><hr><h3 id="F1"><a href="#F1" class="headerlink" title="F1"></a>F1</h3><p>可以看成对<strong>精确度</strong>和<strong>召回率</strong>的加权平均。</p><script type="math/tex; mode=display">F1=2\cdot \frac{recall\cdot precision}{recall + precision}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.f1_score(y_true, y_pred, *, labels=<span class="literal">None</span>, pos_label=<span class="number">1</span>, average=<span class="string">'binary'</span>, sample_weight=<span class="literal">None</span>, zero_division=<span class="string">'warn'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="ROC"><a href="#ROC" class="headerlink" title="ROC"></a>ROC</h3><p><strong>受试者工作特征曲线</strong>。</p><ul><li><strong>横坐标</strong>：假阳性率（FPR）【真实正样本被预测为负/真实正样本】</li><li><strong>纵坐标</strong>：真阳性率（TPR）【真实正样本被预测为正/真实正样本】</li></ul><p><img src="/2021/04/17/ML-Metrics/sphx_glr_plot_roc_0011.png" alt="../_images/sphx_glr_plot_roc_0011.png"></p><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.roc_curve(y_true, y_score, *, pos_label=<span class="literal">None</span>, sample_weight=<span class="literal">None</span>, drop_intermediate=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p><strong>ROC曲线下方面积</strong>。</p><p>面积越大的分类器越好。</p><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.roc_auc_score(y_true, y_score, *, average=<span class="string">'macro'</span>, sample_weight=<span class="literal">None</span>, max_fpr=<span class="literal">None</span>, multi_class=<span class="string">'raise'</span>, labels=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Regression-Metrics（回归）"><a href="#Regression-Metrics（回归）" class="headerlink" title="Regression Metrics（回归）"></a>Regression Metrics（回归）</h2><h3 id="MSE"><a href="#MSE" class="headerlink" title="MSE"></a>MSE</h3><p><strong>均方差</strong>误差。</p><script type="math/tex; mode=display">\mathrm{MSE}(y,\hat{y})=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error(y_true, y_pred, *, sample_weight=<span class="literal">None</span>, multioutput=<span class="string">'uniform_average'</span>, squared=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><p><strong>平均绝对值误差</strong>。</p><script type="math/tex; mode=display">\mathrm{MAE}(y,\hat{y})=\frac{1}{N}\sum_{i=1}^N|y_i-\hat{y_i}|</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_absolute_error(y_true, y_pred, *, sample_weight=<span class="literal">None</span>, multioutput=<span class="string">'uniform_average'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="MSLE"><a href="#MSLE" class="headerlink" title="MSLE"></a>MSLE</h3><p><strong>对数均方误差</strong>。</p><script type="math/tex; mode=display">\mathrm{MSLE}(y,\hat{y})=\frac{1}{N}\sum_{i=1}^N(\ln(1+y_i)-\ln(1+\hat{y_i}))^2</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_log_error(y_true, y_pred, *, sample_weight=<span class="literal">None</span>, multioutput=<span class="string">'uniform_average'</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Max-Error"><a href="#Max-Error" class="headerlink" title="Max Error"></a>Max Error</h3><p><strong>最大误差</strong>。</p><script type="math/tex; mode=display">\mathrm{Max\_Error}(y,\hat{y})=\max(|y_i-\hat{y_i}|)</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.max_error(y_true, y_pred)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="R-2"><a href="#R-2" class="headerlink" title="R^2"></a><script type="math/tex">R^2</script></h3><p><strong>决定系数</strong>。</p><script type="math/tex; mode=display">R^2(y,\hat{y_i})=1-\frac{\sum_{i=1}^N(y_i-\hat{y_i})^2}{\sum_{i=1}^N(y_i-\bar{y})^2}</script><p><strong>Code</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.r2_score(y_true, y_pred, *, sample_weight=<span class="literal">None</span>, multioutput=<span class="string">'uniform_average'</span>)</span><br></pre></td></tr></tbody></table></figure><p><strong>REF</strong></p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://scikit-learn.org/stable/modules/classes.html?highlight=metric#module-sklearn.metrics">sklearn.metrics</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Metrics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Standardization</title>
      <link href="/2021/04/03/Standardization/"/>
      <url>/2021/04/03/Standardization/</url>
      
        <content type="html"><![CDATA[<h1 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>数据标准化</strong>就是把数据集中服从不同”分布“的各个属性空间，通过数学变换到同一的”分布空间“，一般映射到<strong>[0,1]</strong>范围。</p><h3 id="数据标准化的必要性"><a href="#数据标准化的必要性" class="headerlink" title="数据标准化的必要性"></a>数据标准化的必要性</h3><p>例如给定一条数据，其不同属性的值分别为</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">100, 1.0, 0.1, 0.01</span><br></pre></td></tr></tbody></table></figure><p>那么，假如每个属性都有一个0.01的微小扰动，则第四个属性的扰动对整个数据的影响远远大于第一个属性。</p><p>同时，对于机器学习模型，数据未标准化会对模型产生影响。</p><h2 id="数据标准化方法"><a href="#数据标准化方法" class="headerlink" title="数据标准化方法"></a>数据标准化方法</h2><p>数据集：</p><script type="math/tex; mode=display">X=\{x_1,x_2, \dots,x_n\}</script><h3 id="min-max方法"><a href="#min-max方法" class="headerlink" title="min-max方法"></a>min-max方法</h3><p>即通过<strong>线性变换</strong>将数据映射到<strong>[0,1]</strong>范围。</p><script type="math/tex; mode=display">f(x_i)=\frac{x_i-\mathrm{min}(X)}{\mathrm{max}(X)-\mathrm{min}(X)}\ for\ 1\le i \le n</script><p><strong>适用范围</strong>：将数据简单变换到某一个范围，当有新数据加入时，<strong>最大值</strong>，<strong>最小值</strong>会发生变化。</p><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p2.png" alt=""></p><h3 id="z-score方法"><a href="#z-score方法" class="headerlink" title="z-score方法"></a>z-score方法</h3><p>通过<strong>按比例缩放</strong>的形式，将数据映射到<strong>[-x,x]</strong>范围，要求<script type="math/tex">\mu=0,\sigma=1</script>。</p><script type="math/tex; mode=display">f(x_i)=\frac{x_i- \mu}{\sigma},\mu=\mathrm{mean}(X),\sigma=\sqrt{\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^2}</script><p><strong>适用范围</strong>：适用于数据中<strong>最大值</strong>、<strong>最小值</strong>未知，数据<strong>分布离散</strong>的情况。</p><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p3.png" alt=""></p><h3 id="小数定标标准化"><a href="#小数定标标准化" class="headerlink" title="小数定标标准化"></a>小数定标标准化</h3><p>将数据全部投影到小数。</p><script type="math/tex; mode=display">f(x_i)=\frac{x_i}{10^k}$$，其中$$k$$是数据中**最大绝对值**的位数。### logistic方法通过函数映射，将数据映射到**[0,1]**范围。$$f(x_i)=\frac{1}{1+e^{-x_i}}</script><p><img src="/2021/04/03/Standardization/p1.png" alt=""></p><p>下面是该变换方法执行前后数据的分布对比。</p><p><img src="/2021/04/03/Standardization/p4.png" alt=""></p><p><strong>Code</strong></p><p><a href="https://github.com/baowj-678/Machine-Learning/tree/master/Standardization">github:baowj-678</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Feature Engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feature Selection: Embedding</title>
      <link href="/2021/04/02/Feature-Selection-Embedding/"/>
      <url>/2021/04/02/Feature-Selection-Embedding/</url>
      
        <content type="html"><![CDATA[<h1 id="嵌入式特征选择"><a href="#嵌入式特征选择" class="headerlink" title="嵌入式特征选择"></a>嵌入式特征选择</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>嵌入式特征选择（Embedding）</strong>是将特征选择和模型训练融为一体，也就是模型中包括了特征选择的过程，而训练时只需将原始数据全部输入到模型中即可。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>给定数据集</p><script type="math/tex; mode=display">D=\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\}</script><script type="math/tex; mode=display">x_i\in \mathbb{R}^d</script><script type="math/tex; mode=display">y_i\in \mathbb{R}</script><p>如果使用<strong>线性回归模型</strong>，则优化目标为：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d</script><p>而上述模型可能会陷入<strong>过拟合（over fitting）</strong>，为了解决这个问题，会引入<strong>L1正则化</strong>和<strong>L2正则化</strong>；</p><h3 id="L2正则化"><a href="#L2正则化" class="headerlink" title="L2正则化"></a>L2正则化</h3><p><strong>L2正则化</strong>将优化目标修改为：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d+\lambda||w||_2^2,\lambda>0</script><p>该模型称为<strong>岭回归</strong>，能降低过拟合风险。</p><h3 id="L1正则化"><a href="#L1正则化" class="headerlink" title="L1正则化"></a>L1正则化</h3><p>将<strong>L2正则化</strong>的<strong>2范数</strong>替换成<strong>1范数</strong>，就得到<strong>L1正则化</strong>：</p><script type="math/tex; mode=display">\mathrm{min}_{w}\sum_{i=1}^m(y_i-w^Tx_i)^2,w\in\mathbb{R}^d+\lambda||w||_1,\lambda>0</script><p>该模型称为<strong>LASSO</strong>。</p><p><strong>L1正则化</strong>除了能降低<strong>过拟合</strong>风险，而且该模型还会倾向于<strong>”稀疏解“</strong>，即该模型倾向于只使用原数据的某些<strong>子属性</strong>，这就间接等同于进行了<strong>特征选择</strong>。</p><p>下面介绍为何会倾向<strong>稀疏解</strong>。</p><p><img src="/2021/04/02/Feature-Selection-Embedding/p1.png" alt="image-20210403211035916"></p><p>假设该模型只有两个参数<script type="math/tex">w_1,w_2</script>，则其<strong>L1范数</strong>和<strong>L2范数</strong>等值线如图所示。<strong>平方项</strong>（即原始的最优化目标）等值线近似如图所示。可以发现，平方误差加上范数误差，对于<strong>L1范数</strong>来说，最小值易在<strong>两个坐标轴上取的</strong>，而<strong>L2范数</strong>偏向在<strong>第一象限取的</strong>。当最优值在坐标轴取的时，<script type="math/tex">w_1,w_2</script>其中一个值就为0，也就是该特征被剔除了，所以易得到<strong>稀疏解</strong>。</p><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p253</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Feature Engineering </tag>
            
            <tag> L1 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feature Selection: Wrapper</title>
      <link href="/2021/04/02/Feature-Selection-Wrapper/"/>
      <url>/2021/04/02/Feature-Selection-Wrapper/</url>
      
        <content type="html"><![CDATA[<h1 id="包裹式特征选择"><a href="#包裹式特征选择" class="headerlink" title="包裹式特征选择"></a>包裹式特征选择</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><strong>包裹式特征选择</strong>（Wrapper）是特征选择的三大方法之一，其主要思路是：利用选择的特征的子数据集进行训练，并把训练结果直接作为评判特征选择的标准。</p><p>所以在<strong>特定模型</strong>的最终结果上，<strong>包裹式特征选择</strong>比<strong>选择式</strong>更好。</p><h2 id="LVW"><a href="#LVW" class="headerlink" title="LVW"></a>LVW</h2><p><strong>拉斯维加斯方法（LVW，Las Vegas Wrapper）</strong>是一种典型的<strong>包裹式特征选择</strong>方法。</p><p>该方法每次随机选取特征子集，然后送入模型，如果模型损失下降则更新特征子集，依次迭代进行。</p><hr><p><strong>输入</strong>：数据集<script type="math/tex">D</script>；</p><p>​            选择的特征集<script type="math/tex">A</script>；</p><p>​            某个学习器<script type="math/tex">\zeta</script>；</p><p>​            停止条件控制参数<script type="math/tex">T</script>【搜索次数】；</p><p><strong>过程</strong>：</p><ol><li><script type="math/tex">E=\infty</script>；</li><li><script type="math/tex">d=|A|</script>；</li><li><script type="math/tex">A^*=A</script>；</li><li><script type="math/tex">t=0</script>；</li><li><strong>while</strong> <script type="math/tex">\ t < T</script> <strong>do</strong></li><li>​    随机产生<strong>特征子集</strong><script type="math/tex">A'</script>；</li><li>​    <script type="math/tex">d'=|A'|</script>；</li><li>​    <script type="math/tex">E'=\mathrm{Cross Validation}(\zeta(D^{A'}))</script>【计算<strong>loss</strong>】；</li><li>​    <strong>if</strong> <script type="math/tex">(E'<E)\ or\ ((E'=E)\ and\ (d'<d))</script> 【<strong>loss</strong>降低，或者<strong>loss</strong>不变，特征数量减少】<strong>then</strong></li><li>​         <script type="math/tex">t=0</script>；【更新】</li><li>​        <script type="math/tex">E=E'</script>；</li><li>​        <script type="math/tex">d=d'</script>；</li><li>​        <script type="math/tex">A^*=A'</script>；</li><li>​    <strong>else</strong></li><li>​        <script type="math/tex">t=t+1</script></li><li>​    <strong>end if</strong></li><li><strong>end while</strong></li></ol><p><strong>输出</strong>：特征子集 <script type="math/tex">A^*</script></p><hr><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p251</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Feature Engineering </tag>
            
            <tag> LVW </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Server</title>
      <link href="/2021/04/02/Server/"/>
      <url>/2021/04/02/Server/</url>
      
        <content type="html"><![CDATA[<h1 id="服务器管理"><a href="#服务器管理" class="headerlink" title="服务器管理"></a>服务器管理</h1><h3 id="查看IP登录情况"><a href="#查看IP登录情况" class="headerlink" title="查看IP登录情况"></a>查看IP登录情况</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> -n 指定登录次数前多少名</span></span><br><span class="line">awk '{print $1}' blog-log |sort |uniq -c|sort -nr|head -n 10000</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Server </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Forest</title>
      <link href="/2021/04/02/Deep-Forest/"/>
      <url>/2021/04/02/Deep-Forest/</url>
      
        <content type="html"><![CDATA[<h1 id="Deep-Forest-christmas-tree"><a href="#Deep-Forest-christmas-tree" class="headerlink" title="Deep Forest:christmas_tree:"></a>Deep Forest<span class="github-emoji"><span>🎄</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f384.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h1><h3 id="简介-ticket"><a href="#简介-ticket" class="headerlink" title="简介:ticket:"></a>简介<span class="github-emoji"><span>🎫</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3ab.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><p>用<strong>深度神经网络(DNN)</strong>的思路来组织<strong>随机森林(RF)</strong>，极大地提高了随机森林的准确率。</p><h3 id="安装-wrench"><a href="#安装-wrench" class="headerlink" title="安装:wrench:"></a>安装<span class="github-emoji"><span>🔧</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f527.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install deep-forest</span><br></pre></td></tr></tbody></table></figure><h3 id="函数-funeral-urn"><a href="#函数-funeral-urn" class="headerlink" title="函数:funeral_urn:"></a>函数<span class="github-emoji"><span>⚱</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/26b1.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><p>用法详见<strong>测试代码</strong></p><ul><li><p><strong>deepforest.CascadeForestClassifier</strong>：对<strong>Deep Forest</strong>的实现；</p></li><li><p><strong>deepforest.DecisionTreeClassifier</strong>：<strong>Deep Forest</strong>的树的实现；</p><p>​    </p></li></ul><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>分类的类别需从<strong>0</strong>开始标记，即<strong>label={0,1,2…}</strong></p><h3 id="原论文阅读-page-with-curl"><a href="#原论文阅读-page-with-curl" class="headerlink" title="原论文阅读:page_with_curl:"></a>原<a href="https://arxiv.org/pdf/1702.08835.pdf">论文</a>阅读<span class="github-emoji"><span>📃</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></h3><h4 id="background"><a href="#background" class="headerlink" title="background"></a>background</h4><ul><li><p>对<strong>深度神经网络(DNN)</strong>成功的原因分析：</p><ol><li>一层一层的堆叠（<em>layer-by-layer processing</em>）；</li><li>模型内部的数据表征方式的转变【例如LSTM中<strong>词向量</strong>的传递和维度变化】（<em>in-model feature transformation</em>）；</li><li>足够的模型复杂度（<em>sufficient model complexity</em>）；</li></ol></li><li><p><strong>DNN</strong>的缺陷：</p><ol><li>超参数太多（too many hyper-parameters），模型表现十分依赖参数选择和训练（parameter tuning）；</li><li>需要大量训练数据（a huge amount of training data）；</li><li><strong>黑箱系统</strong>（玄学），很难进行理论分析（theoretical analysis）；</li><li>模型结构的确定先于模型训练；</li></ol></li></ul><h4 id="inspiration"><a href="#inspiration" class="headerlink" title="inspiration"></a>inspiration</h4><p><strong>从DNN</strong>：</p><ul><li><p>从<strong>DNN</strong>中观察到，在<strong>DNN</strong>每层的传播中，<strong>数据特征（feature）</strong>越来越集中，越来越抽象。</p></li><li><p><strong>DNN</strong>的成功和<strong>模型复杂度</strong>关系不大，否则<em>为什么无限扩大模型参数量并不能提升模型效果？</em>；</p></li><li><strong>DNN</strong>的层次性和<strong>决策树</strong>的层次性不一样：<ul><li><strong>决策树</strong>始终利用的是输入的数据，并没有对<strong>数据表征（feature）</strong>做出任何改变（work on the original feature representation），没有出现（<em>in-model feature transformation</em>）；</li><li><strong>DNN</strong>每一层的输出都会对<strong>数据表征（feature）</strong>做出改变；</li></ul></li></ul><p><strong>从集成学习（Ensemble Learning）</strong></p><ul><li><p>要做好集成学习，每个<strong>学习单元（learner）</strong>要做到<strong>准确（accurate）</strong> <strong>多样（diverse）</strong>；</p></li><li><p>实践中常常会通过技巧提高模型的<strong>多样性</strong>：</p><ul><li><p><strong>数据采样（data sample manipulation）</strong>：</p><p>  从原始数据集中采样出不同的<strong>子训练集</strong>来训练不同的<strong>学习单元（learner）</strong>；</p><p>  例如：</p><p>  <strong>Bagging</strong>中的<strong>bootstrap sampling</strong>；</p><p>  <strong>AdaBoost</strong>中的<strong>sequential importance sampling</strong>；</p></li><li><p><strong>输入特征采样（input feature manipulation）</strong>：</p><p>  从原始的数据特征中采样出不同的子特征（feature）生成<strong>子空间（subspace）</strong>，训练不同的<strong>学习单元（learner）</strong>；</p></li><li><p><strong>学习参数区别（learning parameter manipulation）</strong>：</p><p>  不同的<strong>学习单元（learner）</strong>采用不同的参数训练；</p></li><li><p><strong>输出表征区别（output representation manipulation）</strong>：</p><p>  对不同的学习单元使用不同的<strong>表征（representation）</strong>；</p></li></ul></li></ul><h4 id="gcForest"><a href="#gcForest" class="headerlink" title="gcForest"></a>gcForest</h4><p><strong>层次森林结构（Cascade Forest Structure）</strong></p><p><img src="/2021/04/02/Deep-Forest/p1.png" alt="pic"></p><ul><li><p>每一层从其前面的层获取数据，再将数据传递到下一层；</p></li><li><p>每一层都是<strong>随机森林</strong>的集成；</p></li><li><p>每个森林中的树个数作为超参数；</p></li><li><p>图中：</p><ul><li><p>黑色森林是<strong>随机森林（random forest）</strong>；</p><p>  每个森林包括500棵<strong>随机树</strong>，树的每个节点从随机选择的$\sqrt{d}$（d是特征个数）个<strong>候选特征</strong>中按照<strong>gini</strong>系数选择一个特征来切分；</p></li><li><p>蓝色森林是<strong>完全随机树森林（completely-random tree forest）</strong>；</p><p>  每个森林包括500棵<strong>完全随机树</strong>，树的每个节点会从<strong>所有的特征</strong>中选择一个特征切分出来，树生长直到完全是叶子；</p></li><li><p>假设数据分为三类，每个<strong>随机森林</strong>将输出<strong>三维向量</strong>，然后将所有向量连接（concatenate）作为输出；</p></li></ul></li><li><p>每个<strong>随机森林</strong>的输出是所有树的平均，如下图：<img src="/2021/04/02/Deep-Forest/p2.png" alt="pic"></p></li><li><p>为了减小<strong>过拟合（overfitting）</strong>风险，每个<strong>随机森林</strong>的输出都使用<strong>K折交叉验证（k-fold cross validation）</strong>：</p><ul><li>每个条数据会被训练<em>k-1</em>次，生成<em>k-1</em>个向量，然后平均作为该树的输出；</li><li>交叉验证的结果作为判定条件，如果模型效果相对上一层有提高则继续扩展下一层，否则结束；</li></ul></li></ul><p><strong>卷积特征提取（Multi-Grained Scanning）</strong></p><p><img src="/2021/04/02/Deep-Forest/p3.png" alt="pic"></p><ul><li><p>用一个一维或者二维的窗口扫描原数据，将窗口数据提取出来作为<strong>新特征</strong>；</p></li><li><p>将<strong>新特征</strong>送入训练，再将结果连接起来，作为最终的输出结果；</p></li><li><p>有可能某些<strong>新特征</strong>与结果丝毫没有关系（例如：需要识别一张图片的<em>汽车</em>，但提取出来的小片段不包含任何相关内容），这时，可以把<strong>新特征</strong>看成一种<strong>output representation manipulation</strong>，可以提高模型的多样性；</p></li><li><p>当<strong>新特征</strong>太多时，可以对其进行<strong>采样</strong>；</p></li><li><p>模型中通常使用不同大小的窗口进行特征提取，如下图：</p><p>  <img src="/2021/04/02/Deep-Forest/p4.png" alt="pic"></p></li></ul><p><strong>Code</strong>：</p><p><a href="https://github.com/baowj-678/Machine-Learning/tree/master/Deep-Forest">github:baowj-678</a></p><p><strong>REF</strong>：</p><p><a href="https://arxiv.org/pdf/1702.08835.pdf">arXiv:1702.08835 [cs.LG]</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Decision Tree </tag>
            
            <tag> Random Forest </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows Terminal添加登录腾讯云服务器</title>
      <link href="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <url>/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="Windows-Terminal配置腾讯云服务器"><a href="#Windows-Terminal配置腾讯云服务器" class="headerlink" title="Windows Terminal配置腾讯云服务器"></a>Windows Terminal配置腾讯云服务器</h1><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><ol><li>租一台<strong>腾讯云服务器</strong>；</li><li>本地安装好<strong>Windows Terminal</strong>；</li></ol><h2 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h2><h3 id="生成SSH私钥"><a href="#生成SSH私钥" class="headerlink" title="生成SSH私钥"></a>生成SSH私钥</h3><p>在腾讯云<strong>控制台</strong>，进入<strong>SSH密钥</strong>界面，点击<strong>创建密钥</strong>：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p1.png" alt="p1"></p><p>填写密钥的名称（描述密钥存放位置和用处，方便记忆）：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p2.png" alt="p2"></p><p>点击<strong>确定</strong>，系统会自动下载密钥文件。</p><h3 id="保存密钥"><a href="#保存密钥" class="headerlink" title="保存密钥"></a>保存密钥</h3><p>将上一步下载的密钥改名为<strong>id_rsa_tencent</strong>（自定义），并置于<em>~/.ssh/</em>目录下：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p3.png" alt="p3"></p><h3 id="配置Windows-Terminal"><a href="#配置Windows-Terminal" class="headerlink" title="配置Windows Terminal"></a>配置Windows Terminal</h3><p>打开<strong>Windows Terminal</strong>，点击设置：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p4.png" alt="p4"></p><p>在<strong>list</strong>下添加下列<strong>配置信息</strong>：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p5.png" alt="p5"></p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;guid&quot;</span>: <span class="string">&quot;&#123;******************************&#125;&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;TencentCloud&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;commandline&quot;</span>: <span class="string">&quot;ssh -i 密钥文件位置 服务器用户名@服务器ip地址&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;hidden&quot;</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="attr">&quot;icon&quot;</span>: <span class="string">&quot;图标位置&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中：</p><p><strong>guid</strong>可以在<a href="https://www.guidgen.com/"><em>guidgen.com</em></a>生成。</p><h2 id="配置结果"><a href="#配置结果" class="headerlink" title="配置结果"></a>配置结果</h2><p>点击<strong>腾讯云图标</strong>即可自动登录，并进入<strong>控制台</strong>：</p><p><img src="/2021/04/02/Windows-Terminal%E6%B7%BB%E5%8A%A0%E7%99%BB%E5%BD%95%E8%85%BE%E8%AE%AF%E4%BA%91%E6%9C%8D%E5%8A%A1%E5%99%A8/p6.png" alt="p6"></p>]]></content>
      
      
      
        <tags>
            
            <tag> Server </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Position Encoding Methods of Transformer</title>
      <link href="/2021/03/31/Position-Encoding-Methods-of-Transformer/"/>
      <url>/2021/03/31/Position-Encoding-Methods-of-Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h1><h2 id="Background1"><a href="#Background1" class="headerlink" title="Background1"></a>Background<a href="#refer-anchor-1"><sup>1</sup></a></h2><p>2017年<strong>谷歌</strong>提出的<strong>Transformer</strong>模型深刻地影响了<strong>NLP</strong>领域，<strong>Transformer</strong>模型是基于<strong>Attention</strong>机制的<strong>降噪自编码器（denoising autoencoder）</strong>模型。因为采用了这种架构，所有的输入文本都<strong>平行地进行</strong>计算。优点是提高了模型效率，但缺点就导致了需要加入<strong>位置编码</strong>，并且<strong>位置编码</strong>很大程度上影响着模型的效果。</p><h2 id="Vanilla-Position-Encoding1"><a href="#Vanilla-Position-Encoding1" class="headerlink" title="Vanilla Position-Encoding1"></a>Vanilla Position-Encoding<a href="#refer-anchor-1"><sup>1</sup></a></h2><h3 id="绝对位置编码"><a href="#绝对位置编码" class="headerlink" title="绝对位置编码"></a>绝对位置编码</h3><p>在这篇论文中，作者提出下面的编码方法：</p><script type="math/tex; mode=display">PE_{(pos,2i)} =sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>其中： </p><ul><li><p><script type="math/tex">pos</script>：句中字词（token）所在的位置；</p></li><li><p><script type="math/tex">i</script>：编码的<strong>位置向量</strong>的一个维度；</p></li></ul><p>这个编码中，<strong>每个位置的每个维度</strong>都服从一个<strong>正弦函数</strong>的“分布”，且可以编码的距离也较长。</p><p>该编码可以学习到<strong>相对位置</strong>，对于任意一个位置<script type="math/tex">PE_{pos+k}</script>都可以表示成<script type="math/tex">PE_{pos}</script>的<strong>线性函数</strong>。</p><h3 id="编码原理2"><a href="#编码原理2" class="headerlink" title="编码原理2"></a>编码原理<a href="#refer-anchor-2"><sup>2</sup></a></h3><p>假设模型为<script type="math/tex">f(\dots,x_m,\dots,x_n,\dots)</script>，其中参数都是输入的<strong>词向量</strong>，而<script type="math/tex">x_m,x_n</script>是在<script type="math/tex">n,m</script>位置上的词向量。这就是<strong>Transformer</strong>的基本模型（不包括位置编码）。</p><p>在加入位置编码后，变为：<script type="math/tex">\tilde{f}(\dots,x_m+p_m,\dots,x_n+p_n,\dots)</script>。</p><p>然后用<strong>泰勒公式</strong>将其展开：</p><script type="math/tex; mode=display">\tilde{f}\approx f+p^T_m\frac{\partial f}{\partial x_m}+p^T_n\frac{\partial f}{\partial x_n}+\frac{1}{2}p^T_m\frac{\partial^2 f}{\partial x_m^2}p_m+\frac{1}{2}p^T_n\frac{\partial^2 f}{\partial x_n^2}p_n+p_m^T\frac{\partial^2f}{\partial x_m\partial x_n}p_n</script><p>可以发现：</p><ul><li><strong>第一项</strong>和位置无关，<strong>第二、三、四、五项</strong>只依赖于单一位置，所以他们只是<strong>绝对位置信息</strong>；</li><li><strong>第六项</strong>简化为<script type="math/tex">p^T_m\mathcal{H}p_n</script>，目标是希望该项能表达<strong>相对位置信息</strong>；</li></ul><p>将<script type="math/tex">\mathcal{H}</script>简化为<script type="math/tex">I</script>（单位阵），则位置编码的相对信息为<script type="math/tex">p_m^Tp_n</script></p><p>当维度<script type="math/tex">d=2</script>时，<strong>Sinusoidal</strong>编码为<script type="math/tex">p_m=[\mathrm{cos}(\frac{m}{10000^{\theta}}),\mathrm{sin}(\frac{m}{10000^{\theta}})]</script>,<script type="math/tex">p_n=[\mathrm{cos}(\frac{n}{10000^{\theta}}),\mathrm{sin}(\frac{n}{10000^{\theta}})]</script>。而<script type="math/tex">p^T_mp_n</script>计算结果为：</p><script type="math/tex; mode=display">\mathrm{cos}(\frac{m-n}{10000^\theta})</script><p>所以可以表示成关于<script type="math/tex">m-n</script>的函数，进而可以表示<strong>相对位置信息</strong>。</p><p>当<script type="math/tex">d</script>为多维时，可以把上述方法叠加，编码就变成</p><script type="math/tex; mode=display">p_m=[\mathrm{cos}(\frac{m}{10000^{\theta_1}}),\mathrm{sin}(\frac{m}{10000^{\theta_1}}),\dots,\mathrm{cos}(\frac{m}{10000^{\theta_{d/2}}}),\mathrm{sin}(\frac{m}{10000^{\theta_{d/2}}})]</script><script type="math/tex; mode=display">p^T_mp_n=\sum_d(\mathrm{cos}(\frac{n-m}{10000^{\theta_i}}))</script><p>对于<script type="math/tex">n-m\in [1, 1000]</script>的数值计算结果如下图：</p><p><img src="/2021/03/31/Position-Encoding-Methods-of-Transformer/Figure_1.png" alt=""></p><h3 id="编码的缺点"><a href="#编码的缺点" class="headerlink" title="编码的缺点"></a>编码的缺点</h3><p>由于<strong>Transformer</strong>一般由多层<strong>Self-Attention</strong>叠加构成。而<strong>Vanilla Position Encoding</strong>只在第一层的输入对位置进行了编码，因此在网络的前向传播过程中，位置编码的信息会逐渐消减。</p><p><img src="/2021/03/31/Position-Encoding-Methods-of-Transformer/Figure_2.png" alt="image-20210416230442907" style="zoom:80%;"></p><p>上图是<strong>Gaussian Transformer</strong><a href="#refer-anchor-8"><sup>8</sup></a>论文中的图片，这里主要关注<strong>（a）</strong>，在<strong>（a）</strong>图中，每个柱形的高度代表每个单词对<strong>book</strong>单词的“关注程度”（即Transformer的Attention矩阵的值）。从图中可以发现，对于同一个单词<strong>new</strong>在三个不同的位置，但模型训练的结果显示它们对<strong>book</strong>的关注度相同。这显然是<strong>违反语言常识</strong>的，因为</p><p>第一个<strong>new</strong>是修饰<strong>book</strong>的，显然应该对<strong>book</strong>有更高的关注度；</p><p>而后面两个<strong>new</strong>分别是修饰<strong>friend</strong>以及是<strong>New York</strong>短语的一部分，显然对<strong>book</strong>没什么关注度；</p><p>三相同的单词，在不同的位置，意味着这三个单词在模型中的<strong>词向量</strong>是一样的，唯一区别的只有<strong>位置编码</strong>，但<strong>Attention</strong>结果又没有区别，说明<strong>位置编码没有发挥应有的作用！</strong></p><h2 id="Relative-Position-Encoding3"><a href="#Relative-Position-Encoding3" class="headerlink" title="Relative Position-Encoding3"></a>Relative Position-Encoding<a href="#refer-anchor-3"><sup>3</sup></a></h2><p><strong>Self-Attention</strong>主要将输入的<strong>n</strong>元素：<script type="math/tex">x=(x_1,\dots,x_n),x_i\in \mathbb{R}^{d_x}</script>转化成新的长度为<strong>n</strong>的表示序列：<script type="math/tex">z=(z_1,\dots,z_n),z_i\in\mathbb{R}^{d_z}</script></p><p>对于其中的每个输出元素<script type="math/tex">z_i</script>，<strong>Self-Attention</strong>计算过程如下：</p><script type="math/tex; mode=display">z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V),W^V\in\mathbb{R}^{d_x\times d_z} \tag{1}</script><p>式中：<script type="math/tex">\alpha</script>是权重</p><script type="math/tex; mode=display">\alpha_{ij}=\frac{\mathrm{exp}(e_{ij})}{\sum_{k=1}^n\mathrm{exp}(e_{ik})} \tag{2}</script><p>式中：<script type="math/tex">e_{ij}</script>就是<strong>Q、K</strong>向量作用的结果：</p><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(x_jW^K)}{\sqrt{d_z}},W^K,W^Q\in\mathbb{R}^{d_x\times d_z} \tag{3}</script><h3 id="编码原理"><a href="#编码原理" class="headerlink" title="编码原理"></a>编码原理</h3><p>这里引入<strong>输入元素的成对关系（pairwise relationships between input elements）</strong>【注意，该关系是和位置相关的，与字词语义无关】，其关系可以表示成<strong>有向完全图</strong>。比如<script type="math/tex">x_i,x_j</script>的关系可以被<script type="math/tex">a_{ij}^V,a_{ij}^K\in \mathbb{R}^{d_a},d_a=d_z</script>表示。</p><p>从而，上式<font color="red">(1)</font>可以修改为【因为位置会影响查询的结果】：</p><script type="math/tex; mode=display">z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V+a_{ij}^V) \tag{4}</script><p>上式<font color="red">(3)</font>也可以修改为【因为位置会影响查询的权重】：</p><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(x_jW^K+a_{ij}^K)}{\sqrt{d_z}} \tag{5}</script><p>其中<script type="math/tex">a_{ij}^V,a_{ij}^K</script>可以通过学习得到。</p><p>在语言模型中，为了表示字词的<strong>相对位置关系</strong>，<script type="math/tex">a_{ij}</script>应当只与<script type="math/tex">i-j</script>有关，所以该模型可以简化为：</p><script type="math/tex; mode=display">a_{ij}=a_{i-j}\tag{6}</script><p>同时在实践中，当句子较长时，可以采用<strong>修剪（clip）</strong>方法，如下：</p><script type="math/tex; mode=display">a_{ij}=a_{\mathrm{min(i-j,k)}},k=const</script><h2 id="DeBERTa-Position-Encoding4"><a href="#DeBERTa-Position-Encoding4" class="headerlink" title="DeBERTa Position-Encoding4"></a>DeBERTa Position-Encoding<a href="#refer-anchor-4"><sup>4</sup></a></h2><p>位置编码模型可以简化为：</p><script type="math/tex; mode=display">A_{i,j}=(W_i+P_i)(W_j+P_j)=W_iW_j+W_iP_j+P_iW_j+P_iP_j \tag{*}</script><p><strong>DeBERTa</strong>认为<strong>位置和语义</strong>之间的作用是十分重要的，而因为使用的是<strong>相对位置</strong>，所有位置和位置之间的作用的没用的。所以该模型舍去了<font color="red">(*)</font>的右边第四项。得到下列模型：</p><script type="math/tex; mode=display">Q_c=HW_{q,c},K_c=HW_{k,c},V_c=HW_{v,c}</script><script type="math/tex; mode=display">\tilde{A}_{i,j}=Q_i^cK_j^{cT}+Q_i^cK_{\delta(i,j)}^{rT}+K_j^cQ_{\delta(j,i)}^{rT}</script><script type="math/tex; mode=display">H_o=\mathrm{softmax}(\frac{\tilde{A}}{\sqrt{3d}})V_c</script><h2 id="XL-Position-Encoding5"><a href="#XL-Position-Encoding5" class="headerlink" title="XL Position-Encoding5"></a>XL Position-Encoding<a href="#refer-anchor-5"><sup>5</sup></a></h2><p>该方法在<strong>Transformer-XL</strong>模型中提出，主要考虑的也是<strong>相对位置编码</strong>。</p><p>该论文认为<strong>Vanilla Transformer</strong>是：</p><script type="math/tex; mode=display">A_{i,j} = (E_{x_i}+U_i)^TW_q^TW_k(E_{x_j}+U_j)</script><script type="math/tex; mode=display">=E_{x_i}^TW_q^TW_{k}E_{x_j}+E_{x_i}^TW_q^TW_{k}U_j+U_i^TW_{q}^TW_kE_{x_j}+U_i^TW_{q}^TW_kU_j</script><p>所以，该方法主要是将上式修改成如下形式：</p><script type="math/tex; mode=display">A_{i,j}=E_{x_i}^TW_q^TW_{k,E}E_{x_j}+E_{x_i}^TW_q^TW_{k,R}R_{i-j}+u^TW_{k,E}E_{x_j}+v^TW_{k,R}R_{i-j}</script><p>主要考虑是用相对位置编码<script type="math/tex">R_{i-j}</script>替换绝对位置编码<script type="math/tex">U_j</script>，用另一待训练参数替换绝对位置编码<script type="math/tex">U_i</script>。</p><p>该方法在预训练模型<strong>XLNet</strong>中也有应用。</p><h2 id="Position-aware-Attention-Position-Encoding6"><a href="#Position-aware-Attention-Position-Encoding6" class="headerlink" title="Position-aware Attention Position-Encoding6"></a>Position-aware Attention Position-Encoding<a href="#refer-anchor-6"><sup>6</sup></a></h2><p>该方法是<strong>RPE</strong><a href="#refer-anchor-3"><sup>3</sup></a>的改进，当然该论文不仅提出了该<strong>位置编码</strong>还提出了一种<strong>网络架构</strong>（这里不介绍）。</p><p>该方法将<font color="red">(5)</font>式修改为下面形式：</p><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(x_jW^K)+x_iR_{j-i}^Kx_j^T}{\sqrt{d_z}} \tag{6}</script><p>其中，加号前面部分是<strong>文本相关</strong>的，后面部分是<strong>位置相关</strong>的，并且<script type="math/tex">R_{j-i}</script>是相对位置相关的。</p><p>该模型<strong>亮点</strong>在于在<strong>相对位置</strong>编码中考虑了<strong>文本信息</strong>。</p><p>而后，为了优化参数数量以及计算复杂度，该模型提出将<script type="math/tex">R_{j-i}^K</script>进行<strong>SVD</strong>分解：</p><script type="math/tex; mode=display">R_{j-i}^K=U_c\Gamma^K_{j-i}V_c^T \tag{7}</script><p>其中<script type="math/tex">U_c\in \mathbb{R}^{d_z\times d_c},V_c\in \mathbb{R}^{d_z \times d_c},\Gamma^K_{j-i}\in \mathbb{R}^{d_c\times d_c}</script>是一个<strong>对角阵</strong>。</p><p>为了简化参数，令<script type="math/tex">U_c=W^Q,V_c=W^K</script></p><p>式<font color="red">(7)</font>可以写为：</p><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(x_jW^K)+(x_iW^Q)\Gamma^K_{j-i}(x_j^TW^K)}{\sqrt{d_z}}</script><script type="math/tex; mode=display">e_{ij}=\frac{(x_iW^Q)(I+\Gamma^K_{j-i})(x_jW^K)}{\sqrt{d_z}} \tag{8}</script><p>式<font color="red">(4)</font>相应改写为：</p><script type="math/tex; mode=display">z_i=\sum_{j=1}^n\alpha_{ij}(x_jW^V(I+\Gamma^V_{j-i})) \tag{9}</script><h2 id="T5-Position-Encoding"><a href="#T5-Position-Encoding" class="headerlink" title="T5 Position-Encoding"></a>T5 Position-Encoding</h2><h2 id="Complex-Position-Encoding7"><a href="#Complex-Position-Encoding7" class="headerlink" title="Complex Position-Encoding7"></a>Complex Position-Encoding<a href="#refer-anchor-7"><sup>7</sup></a></h2><p>该论文创新之处在于利用<strong>复数</strong>进行<strong>相对位置</strong>编码，其不仅修改了编码方法，还修改了<strong>神经网络</strong>架构，使之成为<strong>复数神经网络</strong>。</p><p>对于位置向量长度为<script type="math/tex">D</script>的模型，其编码方法如下：</p><script type="math/tex; mode=display">f(j,pos)=[r_{j,1}e^{i(w_{j,1}\times pos+\theta_{j,1})},r_{j,2}e^{i(w_{j,2}\times pos+\theta_{j,2})},\dots,r_{j,D}e^{i(w_{j,D}\times pos+\theta_{j,D})}]</script><p>其中：</p><ul><li><p><script type="math/tex">f(j,pos)</script>：表示<strong>词汇表</strong>中第<script type="math/tex">j</script>个单词在<script type="math/tex">pos</script>位置的编码结果，只和单词有关（可以是词向量），该向量会进行<strong>Attention</strong>；</p></li><li><p><script type="math/tex">r_j\in \mathbb{R}^D</script>：带训练的模型参数；</p></li><li><p><script type="math/tex">w_j\in \mathbb{R}^D</script>：频率相关的向量；</p></li><li><p><script type="math/tex">\theta_j\in\mathbb{R}^D</script>：初相位向量；</p></li></ul><h3 id="编码原理-1"><a href="#编码原理-1" class="headerlink" title="编码原理"></a>编码原理</h3><p>该编码满足下列性质【因为该编码方式是先假定满足性质，而后推导出的编码方程】</p><ul><li>位置的偏移变换和和位置本身无关的，即<script type="math/tex">g(pos+n)=Transform_n(g(pos))</script>；</li><li>有界性，即<script type="math/tex">|g(pos)|\le \delta,pos\in\mathbb{N}</script></li></ul><h3 id="与Vanilla-Tranformer联系"><a href="#与Vanilla-Tranformer联系" class="headerlink" title="与Vanilla Tranformer联系"></a>与Vanilla Tranformer联系</h3><p><strong>Vanilla Tranformer</strong>可以看成该方法的一种特例。</p><p>令<script type="math/tex">f(\cdot,pos,k)=e^{i\times10000 ^{2k/d_{model}}}=cos(10000^{2k/d_{model}}pos)+isin(10000^{2k/d_{model}}pos)</script></p><p>所以：</p><script type="math/tex; mode=display">PE_{pos,2k}=img(f(\cdot,pos,k))</script><script type="math/tex; mode=display">PE_{pos,2k+1}=real(f(\cdot,pos,k))</script><p>与<strong>Complex</strong>不同的是，<strong>Vanilla</strong>将位置编码<strong>加</strong>上<strong>语义编码</strong>，而<strong>Complex</strong>是<strong>乘</strong>上。</p><p><strong>REF</strong>：</p><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a><p></p><p></p><div id="refer-anchor-2"></div> [2] <a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/1150593142">Transformer升级之路：Sinusoidal位置编码追根溯源</a><p></p><p></p><div id="refer-anchor-3"></div> [3] <a href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a><p></p><p></p><div id="refer-anchor-4"></div> [4] <a href="https://arxiv.org/abs/2006.03654">DeBERTa: Decoding-enhanced BERT with Disentangled Attention</a><p></p><p></p><div id="refer-anchor-5"></div> [5] <a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a><p></p><p></p><div id="refer-anchor-6"></div> [6] <a href="https://ieeexplore.ieee.org/document/9099090">Improving Self-Attention Networks With Sequential Relations</a><p></p><p></p><div id="refer-anchor-7"></div> [7] <a href="https://arxiv.org/abs/1912.12333">ENCODING WORD ORDER IN COMPLEX  EMBEDDINGS</a><p></p><p></p><div id="refer-anchor-8"></div> [8] <a href="https://ojs.aaai.org//index.php/aaai/article/view/4614">Gaussian Transformer: A Lightweight Approach for Natural Language Inference</a><p></p>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feature Selection: Relief</title>
      <link href="/2021/03/30/Feature-Selection-Relief/"/>
      <url>/2021/03/30/Feature-Selection-Relief/</url>
      
        <content type="html"><![CDATA[<h1 id="Relief"><a href="#Relief" class="headerlink" title="Relief"></a>Relief</h1><p><strong>Relief（Relevant Features）</strong>是一种著名的<strong>过滤式特征选择</strong>方法，所谓<strong>过滤式</strong>就是先对<strong>数据集</strong>进行<strong>特征选择</strong>，然后再利用选择的数据集训练学习器。</p><p><strong>Relief</strong>设置了一个“相关统计量”，来度量一个特征的重要性，</p><h2 id="Relief算法过程（二分类）"><a href="#Relief算法过程（二分类）" class="headerlink" title="Relief算法过程（二分类）"></a>Relief算法过程（二分类）</h2><p>给定训练集：</p><script type="math/tex; mode=display">\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m\}</script><p>对于每个实例$x_i$：</p><p><strong>Relief</strong>，在<script type="math/tex">x_i</script>的<strong>同类样本</strong>（即<script type="math/tex">y_i</script>相同）中寻找<strong>最近邻 </strong><script type="math/tex">x_{i,nh}</script>，称为<strong>猜中近邻（near-hit）</strong>；</p><p>然后，在<script type="math/tex">x_i</script>的<strong>异类样本</strong>（即<script type="math/tex">y_i</script>不同）中寻找<strong>最近邻</strong><script type="math/tex">x_{i,nm}</script>​，称为<strong>猜错近邻（near-miss）</strong>；</p><p>则，对于属性$j$，其<strong>相关统计量</strong>为：</p><script type="math/tex; mode=display">\delta^j=\sum_idiff(x_i^j,x_{i,nm}^j)^2-diff(x_i^j,x_{i,nh}^j)^2</script><p>其中：</p><p>$x_a^j$：样本实例$x_a$在属性$j$上的取值；</p><p>$diff(a,b)$：$a$，$b$之间的差值；</p><ul><li>若是<strong>连续数据</strong>：<script type="math/tex">diff(a,b)=|a-b|</script></li><li>若是<strong>离散数据</strong>：<script type="math/tex">diff(a,b)=0,if\ a=b</script>；<script type="math/tex">diff(a,b)=1,if\ a\ne b</script></li></ul><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2><ul><li>如果<script type="math/tex">sum_idiff(x_i^j,x_{i,nm}^j)^2-diff(x_i^j,x_{i,nh}^j)^2>0</script>，即<script type="math/tex">x_i</script>，与其<strong>猜中近邻</strong><script type="math/tex">x_{i,nh}</script>在属性<script type="math/tex">j</script>上的<strong>距离</strong>小于<strong>猜错近邻</strong><script type="math/tex">x_{i,nm}</script>。则说明属性$j$对于区分类别是有益的，所以其<strong>相关统计量</strong>增大；</li><li>相反情况，其<strong>相关统计量</strong>减小；</li><li>最终，<strong>相关统计量</strong>值越大的属性，说明其作用越显著；</li></ul><h3 id="Relief-F算法（多分类）"><a href="#Relief-F算法（多分类）" class="headerlink" title="Relief-F算法（多分类）"></a>Relief-F算法（多分类）</h3><p>给定数据集：</p><ul><li><p>数据集：<script type="math/tex">\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m\}</script></p></li><li><p>其类别集：<script type="math/tex">y\in \{1,2,\dots,|\mathcal{Y}|\}</script></p></li></ul><p>对于每个实例$x_i$：</p><p><strong>Relief-F</strong>，在<script type="math/tex">x_i</script>的<strong>同类样本</strong>（即<script type="math/tex">y_i</script>相同）中寻找<strong>最近邻</strong><script type="math/tex">x_{i,nh}</script>，称为<strong>猜中近邻</strong>；</p><p>然后，在<script type="math/tex">x_i</script>的每个不同类（共<script type="math/tex">|\mathcal{Y}|-1</script>个）寻找一个<strong>最近邻</strong><script type="math/tex">x_{i,l,nm},l\in\{1,2,\dots,|\mathcal{Y}|;l\ne k\}</script>，称为<strong>猜错近邻</strong>；</p><p>于是，改进的<strong>相关统计量</strong>为：</p><script type="math/tex; mode=display">\delta^j=\sum_i(\sum_{l\ne k}(p_l\times diff(x_i^j,x_{i,l,nm}^j)^2)-diff(x_i^j.x_{i,nh}^j)^2)</script><p>其中：</p><p>$p_l$：第$l$类样本在数据集$D$中所占的比例。</p><p><strong>REF</strong>：</p><p>周志华.2015.机器学习.北京.清华大学出版社.p249</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Feature Engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo ERROR:hexo-renderer-swig</title>
      <link href="/2021/03/28/Hexo-ERROR-hexo-renderer-swig/"/>
      <url>/2021/03/28/Hexo-ERROR-hexo-renderer-swig/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-ERROR-hexo-renderer-swig"><a href="#Hexo-ERROR-hexo-renderer-swig" class="headerlink" title="Hexo ERROR: hexo-renderer-swig"></a>Hexo ERROR: hexo-renderer-swig</h1><h3 id="报错内容"><a href="#报错内容" class="headerlink" title="报错内容"></a>报错内容</h3><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;% extends &#x27;_layout.swig&#x27; %&#125; &#123;% import &#x27;_macro/post.swig&#x27; as post_template %&#125; &#123;% import &#x27;_macro/sidebar.swig&#x27; as sidebar_template %&#125; &#123;% block title %&#125;&#123;&#123; config.title &#125;&#125;&#123;% if theme.index_with_subtitle and config.subtitle %&#125; - &#123;&#123;config.subtitle &#125;&#125;&#123;% endif %&#125;&#123;% endblock %&#125; &#123;% block page_class %&#125; &#123;% if is_home() %&#125;page-home&#123;% endif -%&#125; &#123;% endblock %&#125; &#123;% block content %&#125;</span><br><span class="line">&#123;% for post in page.posts %&#125; &#123;&#123; post_template.render(post, true) &#125;&#125; &#123;% endfor %&#125;</span><br><span class="line">&#123;% include &#x27;_partials/pagination.swig&#x27; %&#125; &#123;% endblock %&#125; &#123;% block sidebar %&#125; &#123;&#123; sidebar_template.render(false) &#125;&#125; &#123;% endblock %&#125;</span><br></pre></td></tr></table></figure><p><img src="/2021/03/28/Hexo-ERROR-hexo-renderer-swig/image-20210328221727193.png" alt="image-20210328221727193"></p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><strong>Hexo</strong>在5.0+中删除了<em>hexo-renderer-swig</em>，因此需要自己安装。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i hexo-renderer-swig</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XLNet-PPT</title>
      <link href="/2021/03/28/XLNet-PPT/"/>
      <url>/2021/03/28/XLNet-PPT/</url>
      
        <content type="html"><![CDATA[<p><img src="/2021/03/28/XLNet-PPT/PPT1.PNG" alt="1"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%872.PNG" alt="2"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%873.PNG" alt="3"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%874.PNG" alt="4"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%875.PNG" alt="5"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%876.PNG" alt="6"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%877.PNG" alt="7"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%878.PNG" alt="8"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%879.PNG" alt="9"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8710.PNG" alt="10"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8711.PNG" alt="11"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8712.PNG" alt="12"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8713.PNG" alt="13"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8714.PNG" alt="14"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8715.PNG" alt="15"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8716.PNG" alt="16"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8717.PNG" alt="17"></p><p><img src="/2021/03/28/XLNet-PPT/%E5%B9%BB%E7%81%AF%E7%89%8718.PNG" alt="18"></p>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XLNet</title>
      <link href="/2021/03/28/XLNet/"/>
      <url>/2021/03/28/XLNet/</url>
      
        <content type="html"><![CDATA[<h1 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h1><p><a href="https://arxiv.org/pdf/1906.08237.pdf">XLNET</a></p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="AR-AutoRegressive"><a href="#AR-AutoRegressive" class="headerlink" title="AR(AutoRegressive)"></a>AR(AutoRegressive)</h3><p>用模型估计一个文本库的<strong>概率分布</strong></p><p>给定<strong>文本序列</strong>：</p><p>$ \bold{x}=[x_1,x_2,\dots,x_T] $</p><p><strong>AR</strong>模型会将<strong>似然</strong>分解成<strong>前向的乘积(forward product)</strong>：</p><p>$p(\bold{x})=\prod<em>{t=1}^Tp(x_t|\bold{x}</em>{&lt;t})$</p><p>或者<strong>后向乘积(backward product)</strong>：</p><p>$p(\bold{x})=\prod<em>{t=T}^1p(x_t|\bold{x}</em>{&gt;t})$</p><p>而其中每个的<strong>概率分布</strong>可以用带参数的模型(神经网络等)来建模。</p><p><strong>AR</strong>通过最大化<strong>对数似然估计</strong>进行预训练：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \mathrm{log}\ p_{\theta}(\bold{x})=\sum_{t=1}^T\mathrm{log}\ p_{\theta}(x_t|\bold{x}_{<t})=\sum_{t=1}^T\mathrm{log}\frac{\mathrm{exp}(h_{\theta}(\bold{x}_{1:t-1})^Te(x_t))}{\sum_{x'}\mathrm{exp}(h_{\theta}(\bold{x}_{1:t-1})^Te(x'))}</script><p><strong>其中</strong></p><p>$h<em>{\theta}(\bold{x}</em>{1:t-1})$：是用<em>神经网络模型</em>计算的文本的表征，可以通过<strong>RNN</strong>，<strong>Transformer</strong>等计算；</p><p>$e(x)$：是$x$的<strong>embedding</strong>；</p><p><strong>缺陷</strong>：<strong>AR</strong>模型只能<strong>单向(uni-directional)建模</strong></p><h3 id="AE-AutoEncoding"><a href="#AE-AutoEncoding" class="headerlink" title="AE(AutoEncoding)"></a>AE(AutoEncoding)</h3><p><strong>AE（自编码器）</strong>模型不会建立<strong>清晰的概率密度估计(explicit density estimation)</strong>，而是对原始数据进行<strong>重构(reconstruct)</strong>，例如：<strong>BERT</strong></p><p><strong>BERT</strong>：基于<strong>降噪自编码器（denoising auto-encoding）</strong>，给定输入，每次<strong>15%</strong>的<strong>字词(token)</strong>会被替换成特殊的标记<strong>[MASK]</strong>，然后模型会被训练去从输入的<strong>不全(corrupted)的数据</strong>中恢复原始的字词。因为<strong>BERT</strong>不用做<strong>密度估计(density estimation)</strong>，所以可以双向建模。</p><p>最优化训练公式如下：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \mathrm{log}\ p_{\theta}(\bar{\bold{x}}|\hat{\bold{x}})\approx \sum_{t=1}^Tm_t\mathrm{log}\ p_{\theta}(x_t|\hat{\bold{x}})=\sum_{t=1}^Tm_t\mathrm{log}\frac{\mathrm{exp}(H_\theta(\hat{\bold{x}})_t^Te(x_t))}{\sum_{x'}\mathrm{exp}(H_\theta(\hat{\bold{x}})_t^Te(x'))}</script><p>其中：</p><p>$\hat{\bold{x}}$：<strong>mask</strong>后的文本；</p><p>$\bar{\bold{x}}$：被<strong>mask</strong>的文本；</p><p>$m_t$：<strong>指示变量</strong>，当$x_t$是被<strong>mask</strong>的时，$m_t=1$，其他情况下，$m_t=0$；</p><p>$H_\theta$：Transformer模型；</p><p><strong>缺陷</strong>：</p><ul><li><strong>BERT</strong>预训练使用的标记<strong>[MASK]</strong>不会出现在真正数据中，会导致<strong>预训练-微调差异(pretrain-finetune discrepancy)</strong></li><li>输入中不包括待预测字词（被mask掉），<strong>BERT</strong>模型不能像<strong>AR</strong>中一样用乘法规则建模<strong>联合概率(joint probability)</strong></li><li><strong>BERT</strong>假设待预测的字词即$\bar{x}$<strong>互相独立</strong></li></ul><p><strong>优点</strong>：</p><ul><li><strong>BERT</strong>每个位置的字词可以注意到双向的上下文所有的文本；</li></ul><h2 id="XLNet-1"><a href="#XLNet-1" class="headerlink" title="XLNet"></a>XLNet</h2><ul><li><strong>XLNet</strong>最大化基于所有可能的<strong>分解顺序的排列(permutation of the factorization order)</strong>的<strong>极大对数似然估计</strong>，由于这个原因每个位置的字词可以学习左右的文本；</li><li><strong>XLNet</strong>预训练不会通过mask部分字词的方法，因此避免<strong>预训练-微调差异</strong>；</li><li><strong>XLNet</strong>集成了<strong>段RNN机制(segment recurrence)</strong>和<strong>Transformer-XL</strong>；</li></ul><h3 id="Permutation-Language-Modeling"><a href="#Permutation-Language-Modeling" class="headerlink" title="Permutation Language Modeling"></a>Permutation Language Modeling</h3><p>一般来说，对于一个长度为$T$的序列$\bold{x}$，<strong>AE</strong>的<strong>因子分解</strong>存在$T!$个不同的排列顺序。直觉上，如果模型参数能够从所有的排列中学得，那么这个模型就可以从双向的所有位置获得信息。</p><p>根据这个想法，得出以下优化公式：</p><script type="math/tex; mode=display">\mathrm{max}_\theta\ \ \ \mathbb{E}_{z\sim\mathcal{Z}_T}[\sum_{t=1}^T\mathrm{log}\ p_\theta(x_{z_t}|\bold{x}_{\bold{z}_{<t}})]</script><p>其中：</p><p>$\mathcal{Z}_T$：长度为$T$的序列的所有排列方式；</p><p><img src="/2021/03/28/XLNet/image-20210322211204908.png" alt="image-20210322211204908"></p><p>实践中，对于一个文本序列$\bold{x}$，我们<strong>采样</strong>出一个排列顺序，然后按照这个顺序将$p_\theta(\bold{x})$的似然估计进行<strong>因子分解</strong>，因为$\theta$参数是共享的，所以理论上$x_i$会”看见“每个$x_j,x_i\neq x_j$。</p><p><strong>排列采样</strong>在<strong>Transformer</strong>的实践中通过<strong>Attention</strong>的<strong>mask</strong>矩阵实现。</p><p>这样就避免了<strong>独立性假设</strong>和<strong>预训练-微调差异</strong>。</p><h4 id="Remark-on-Permutation"><a href="#Remark-on-Permutation" class="headerlink" title="Remark on Permutation"></a>Remark on Permutation</h4><p>上面的<strong>排列采样</strong>仅针对<strong>因子分解</strong>计算，不是序列本身的顺序。同时为了保留序列原本顺序的信息，这里使用基于原本顺序的<strong>位置编码</strong>。</p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><h4 id="Target-Aware-Representations"><a href="#Target-Aware-Representations" class="headerlink" title="Target-Aware Representations"></a>Target-Aware Representations</h4><p><strong>模型公式</strong>：</p><script type="math/tex; mode=display">p_\theta(\bold{X}_{z_t}=x|\bold{x}_{\bold{z}_{<t}})=\frac{\mathrm{exp}(e(x)^Th_\theta(\bold{x}_{\bold{z}_{<t}}))}{\sum_{x'}\mathrm{exp}(e(x')^Tg_\theta(\bold{x}_{\bold{z}_{<t}}))}</script><p>其中$\bold{x}<em>{\bold{z}</em>{&lt;t}}$不依赖其即将预测的<strong>位置信息</strong>，$\bold{z}$是按照<strong>因子分解顺序</strong>排列的。因此这里需要加入位置信息；所以将公式修改为：</p><script type="math/tex; mode=display">p_\theta(\bold{X}_{z_t}=x|\bold{x}_{\bold{z}_{<t}})=\frac{\mathrm{exp}(e(x)^Tg_\theta(\bold{x}_{\bold{z}_{<t}},z_t))}{\sum_{x'}\mathrm{exp}(e(x')^Tg_\theta(\bold{x}_{\bold{z}_{<t}},z_t))}</script><p>其中：</p><p>$g<em>\theta(\bold{x}</em>{\bold{z}_{&lt;t}}, z_t)$：表示将预测目标的位置编码$z_t$加入的新的表征。</p><p>这可以看成<strong>站在</strong>目标位置$z<em>t$，然后依靠位置信息去收集文本信息$\bold{x}</em>{\bold{z}_{&lt;t}}$。</p><h4 id="Two-Stream-Self-Attention"><a href="#Two-Stream-Self-Attention" class="headerlink" title="Two-Stream Self-Attention"></a>Two-Stream Self-Attention</h4><p>使用<strong>Target-Aware Representation</strong>会出现两个问题：</p><ul><li>为了预测$z<em>t$位置的字词$x</em>{z<em>{t}}$，$g</em>\theta(\bold{x}<em>{\bold{z}</em>{&lt;t}},z<em>t)$不能使用$x</em>{z_{t}}$的信息；</li><li>为了预测其他位置的字词$x<em>{z_t},j&gt;t$，$g</em>\theta(\bold{x}<em>{\bold{z}</em>{&lt;t}},z<em>t)$需要加入$x</em>{z_{t}}$的信息。</li></ul><p>为了解决上面问题，这里提出了<strong>Two-Stream Self-Attention</strong>方法</p><p><img src="/2021/03/28/XLNet/image-20210325230153410.png" alt="image-20210325230153410"></p><ul><li><strong>content representation</strong>$h<em>\theta(\bold{x}</em>{\bold{z}<em>{\le t}})$，这个和<strong>Transformer</strong>中的类似，它保留了上游文本信息和$x</em>{z_t}$的信息；</li><li><strong>query representation</strong>$g<em>\theta(\bold{x}</em>{\bold{z}<em>t},z_t)$，这个只保留了$\bold{z}</em>{&lt;t}$的信息。</li></ul><p>在计算上：</p><p>第一层的<strong>query stream</strong>被初始化为可训练的向量，$g_i^{(0)}=w$；</p><p>第一层的<strong>content stream</strong>被初始化为对应的<strong>词向量</strong>，$h_i^{(0)}=e(x_i)$；</p><p>对于每个<strong>self-attention layer</strong>，其更新方式为：</p><script type="math/tex; mode=display">g_{z_t}^{(m)}=\mathrm{Attention}(Q=g_{z_t}^{(m-1)},KV=h_{\bold{z}_{<t}}^{(m-1)};\theta)</script><script type="math/tex; mode=display">h_{z_t}^{(m)}=\mathrm{Attention}(Q=h_{z_t}^{(m-1)},KV=h_{\bold{z}_{\le t}}^{(m-1)};\theta)</script><p><strong>content representation</strong>和标准的<strong>self-attention</strong>一样，所以在<strong>微调</strong>的时候我们会丢掉<strong>query stream</strong>，只使用<strong>content stream</strong>作为普通的<strong>Transformer</strong>看，同时我们将$g_{z_t}^{(M)}$作为最终的预测结果。</p><h4 id="Partial-Prediction"><a href="#Partial-Prediction" class="headerlink" title="Partial Prediction"></a>Partial Prediction</h4><p>利用语句排列来训练模型存在<strong>难收敛(slow convergence)</strong>的问题，为了解决这个问题，将$\bold{z}$分为$\bold{z}<em>{\le c}$和$\bold{z}</em>{&gt;c}$，然后最大化以下概率：</p><script type="math/tex; mode=display">\mathrm{max}_{\theta}\ \ \ \mathbb{E}_{\bold{z}\sim \mathcal{Z}_T}[\mathrm{log}p_{\theta}(\bold{x}_{\bold{z}_{>c}}|\bold{x}_{\bold{z}_{\le c}})]=\mathbb{E}_{\bold{z}\sim \mathcal{Z}_T}[\sum_{t=c+1}^{\bold{z}}\mathrm{log}p_{\theta}(x_{z_t}|\bold{x}_{\bold{z}_{<t}})]</script><p>预测$\bold{z}_{&gt;c}$的原因是因为，能够传递足够的信息。一般会设置一个超参数$K$，$1/K$的字词被选择用做预测。</p><h3 id="Incorporating-Ideas-from-Transformer-XL"><a href="#Incorporating-Ideas-from-Transformer-XL" class="headerlink" title="Incorporating Ideas from Transformer-XL"></a>Incorporating Ideas from Transformer-XL</h3><p>这里使用了<strong>Transformer-XL</strong>的两个技巧：<strong>相对位置编码（relative positional encoding scheme）</strong>和<strong>段循环机制（segment recurrence mechanism）</strong></p><ul><li><p><strong>相对位置编码</strong>：即前文的$g<em>\theta(\bold{x}</em>{\bold{z}_t},z_t)$</p></li><li><p><strong>段循环机制</strong>：</p><p>  <img src="/2021/03/28/XLNet/image-20210328204303326.png" alt="image-20210328204303326"></p><p>  假设有长文本中的两段文本，长文本为：$\bold{s}$，两端文本为：$\tilde{\bold{x}}=\bold{s}<em>{1:T},\bold{x}=\bold{s}</em>{T+1:2T}$，两端文本对应的采样的排列顺序为$\tilde{\bold{z}}=[1\dots T],\bold{z}=[T+1\dots 2T]$</p><p>  于是基于$\tilde{\bold{z}}$，先处理第一段，获得<strong>文本表征</strong>$\tilde{\bold{h}}^{(m)}$($m$代表层数)，然后对于下一段，计算公式为：</p><script type="math/tex; mode=display">h_{z_t}^{(m)}\leftarrow \mathrm{Attention}(Q=h_{z_t}^{(m-1)},KV=[\tilde{\bold{h}}^{(m-1)},\bold{h}_{\bold{z}_{\le t}}^{(m-1)}];\theta)</script><p>  $[\dots]$：表示将矩阵按照<strong>序列长度的维度</strong>拼接起来。</p></li></ul><h3 id="Modeling-Multiple-Segments"><a href="#Modeling-Multiple-Segments" class="headerlink" title="Modeling Multiple Segments"></a>Modeling Multiple Segments</h3><p>预训练时，对于多段文本，<strong>XLNet</strong>随机采样两段文本（可能从同一上下文，也可能从不同上下文）然后将其拼接成一段文本（方法和<strong>BERT</strong>一样：<strong>[A, SEP, B, SEP, CLS]</strong>）</p><h4 id="Relative-Segment-Encoding"><a href="#Relative-Segment-Encoding" class="headerlink" title="Relative Segment Encoding"></a>Relative Segment Encoding</h4><p>给定两个位置$i,j$，如果这两个位置是来自同一段，则用编码$s<em>{i,j}=s</em>{+}$，否则用编码$s<em>{i,j}=s</em>{-}$。这两个编码都是学得的参数。</p><p>当$i$查询$j$时，段位置编码$s<em>{i,j}$会被用于计算<strong>注意权重（attention weight）</strong>$a</em>{i,j}=(\bold{q}<em>i+\bold{b})^T\bold{s}</em>{i,j}$。</p><p>$\bold{q}_i$是查询向量，$\bold{b}$是待学习的依赖于头的偏差向量<strong>（learnable head-specific bias vector）</strong>。最后该权重会被加到最终的<strong>注意权重</strong>上。</p><p>该方法有两个作用：</p><ul><li>提高泛化能力；</li><li>让模型能够适应多段文本任务，因为多段文本不能用绝对位置编码；</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL-install</title>
      <link href="/2020/10/29/SQL-install/"/>
      <url>/2020/10/29/SQL-install/</url>
      
        <content type="html"><![CDATA[<h1 id="安装MySQL"><a href="#安装MySQL" class="headerlink" title="安装MySQL"></a>安装MySQL</h1><h3 id="下载："><a href="#下载：" class="headerlink" title="下载："></a>下载：</h3><ul><li><p>在<a href="https://dev.mysql.com/downloads/mysql/">MySQL官网</a>下载<strong>zip</strong>文件；</p></li><li><p>本地直接解压；</p></li></ul><h3 id="安装："><a href="#安装：" class="headerlink" title="安装："></a>安装：</h3><ul><li>配置<strong>环境变量</strong>：添加解压文件的<strong>bin</strong>文件夹路径至<strong>PATH</strong>；</li><li>初始化，生成DATA文件：<em>mysqld –initialize-insecure –user=mysql</em></li><li>网络连接：<em>mysqld -install</em></li><li>启动服务：<em>net start mysql</em>【在bin目录下】</li></ul><h3 id="登录："><a href="#登录：" class="headerlink" title="登录："></a>登录：</h3><ul><li><p>登录【不用密码】：<em>mysql -u root -p</em></p></li><li><p>查询用户密码：<em>select host,user,authentication_string from mysql.user;</em></p></li><li><p>设置<strong>root</strong>用户密码：<strong>ALTER USER ‘root’@’localhost’ IDENTIFIED WITH mysql_native_password BY ‘123456’;</strong></p></li><li><p>保存修改：<em>flush privileges;</em></p></li></ul><h3 id="基本操作："><a href="#基本操作：" class="headerlink" title="基本操作："></a>基本操作：</h3><ul><li><strong>退出</strong>：<em>quit</em></li><li></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LDA(Fisher)</title>
      <link href="/2020/07/07/LDA-Fisher/"/>
      <url>/2020/07/07/LDA-Fisher/</url>
      
        <content type="html"><![CDATA[<h1 id="LDA-Fisher"><a href="#LDA-Fisher" class="headerlink" title="LDA(Fisher)"></a>LDA(Fisher)</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>LDA(Linear Discriminant Analysis)</strong>，又称<strong>Fisher判别方法</strong>。是一种经典的<strong>线性判别方法</strong>。该方法主要思想是：将样例投影到一维直线上，使得<strong>同类样例</strong>的投影点尽可能<strong>接近</strong>和<strong>密集</strong>；<strong>异类</strong>投影点尽可能<strong>远离</strong>。</p><p><img src="\LDA-Fisher\fisher.png" alt="fisher"></p><h3 id="计算推导"><a href="#计算推导" class="headerlink" title="计算推导"></a>计算推导</h3><p>假设已知样本 $C_1$ 和 $C_2$ ，$|C_1|、|C_2|$ 分别两类样本数据的总数。</p><p>则两类样例的<strong>类中心</strong>分别为：</p><script type="math/tex; mode=display">\mu_1=\frac{1}{|C_1|}\sum_{x\in{C_1}}x</script><script type="math/tex; mode=display">\mu_2=\frac{1}{|C_2|}\sum_{x\in{C_2}}x</script><p>假设最佳的投影方向为 $w$ 则，样本点 $x$ 投影到 $w$ 上的点的坐标为：$y=w^Tx$</p><p>所以，投影后的<strong>类中心</strong>为：</p><script type="math/tex; mode=display">m_k=\frac{1}{|C_k|}\sum_{x\in{C_k}}w^Tx=w^T\frac{1}{|C_k|}\sum_{x\in{C_k}}x=w^T\mu_k</script><h4 id="类间距离"><a href="#类间距离" class="headerlink" title="类间距离"></a>类间距离</h4><p><strong>类中心</strong>的<strong>间距</strong>为：</p><script type="math/tex; mode=display">d_{(1, 2)}=(m_1-m_2)^2=(m_1-m_2)(m_1-m_2)^T=w^T(\mu_1-\mu_2)(\mu_1-\mu_2)^Tw=w^TS_bw</script><p>其中，$S_b$ 为<strong>类间散度矩阵</strong>：</p><script type="math/tex; mode=display">S_b=(\mu_1-\mu_2)(\mu_1-\mu_2)^T</script><h4 id="类内距离"><a href="#类内距离" class="headerlink" title="类内距离"></a>类内距离</h4><p><strong>类内距离</strong>用类内样本的方差来衡量，对于第 $k$ 个类，方差为：</p><script type="math/tex; mode=display">\begin{split}S_k^2=\sum_{x\in{C_k}}(y-m_k)^2=\sum_{x\in{C_k}}(w^T(x-\mu_k))^2\\=\sum_{x\in{C_k}}(w^T(x-\mu_k))((x-\mu_k)^Tw)\\=w^T[\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T]w \end{split}</script><p>所有类别<strong>类内距离</strong>之和为：</p><script type="math/tex; mode=display">\sum_{k\in n}S_k^2=w^T[\sum_{k\in n}\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T]w</script><p><strong>类内散度矩阵</strong>为：</p><script type="math/tex; mode=display">S_w=\sum\sum_{x\in{C_k}}(x-\mu_k)(x-\mu_k)^T</script><h4 id="最优化"><a href="#最优化" class="headerlink" title="最优化"></a>最优化</h4><p>我们的优化目的是增加<strong>类间距离</strong>，减小<strong>类内距离</strong>，所有可以最大化函数：</p><script type="math/tex; mode=display">J(w)=\frac{(m_1-m_2)^2}{S_1^2+S_2^2}=\frac{w^TS_bw}{w^TS_ww}</script>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python</title>
      <link href="/2020/06/02/Python/"/>
      <url>/2020/06/02/Python/</url>
      
        <content type="html"><![CDATA[<h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><h2 id="常用代码片段"><a href="#常用代码片段" class="headerlink" title="常用代码片段"></a>常用代码片段</h2><h4 id="查看类的方法"><a href="#查看类的方法" class="headerlink" title="查看类的方法"></a>查看类的方法</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">dir</span>(class_name)</span><br></pre></td></tr></tbody></table></figure><h4 id="查看类的变量"><a href="#查看类的变量" class="headerlink" title="查看类的变量"></a>查看类的变量</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class_name.__dict__.items()</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 编程语言 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>PowerShell</title>
      <link href="/2020/03/08/PowerShell/"/>
      <url>/2020/03/08/PowerShell/</url>
      
        <content type="html"><![CDATA[<h1 id="PowerShell"><a href="#PowerShell" class="headerlink" title="PowerShell"></a>PowerShell</h1><h2 id="basic-command-lines"><a href="#basic-command-lines" class="headerlink" title="basic command lines"></a>basic command lines</h2><ol><li><p>get information about the power-shell (version…)</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$psversiontable</span></span><br></pre></td></tr></table></figure></li><li><p>enter ps through CMD</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;powershell</span><br></pre></td></tr></table></figure></li><li><p>basic math operation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1024</span>*<span class="number">1024</span></span><br></pre></td></tr></table></figure></li><li><p>get service information</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-service</span></span><br></pre></td></tr></table></figure></li><li><p>print environment variables</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$env:path</span></span><br></pre></td></tr></table></figure></li><li><p>get all of the commands</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-command</span></span><br><span class="line"><span class="built_in">gcm</span></span><br></pre></td></tr></table></figure></li><li><p>help</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-help</span> [<span class="type">command</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>history commands</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-history</span></span><br></pre></td></tr></table></figure></li><li><p>find the real name of a short name</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">get-alias</span> <span class="literal">-name</span> [<span class="type">short</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>set alias</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set-alias</span> <span class="literal">-name</span> [<span class="built_in">new-name</span>] <span class="literal">-value</span> [<span class="type">old</span>-<span class="type">name</span>]</span><br></pre></td></tr></table></figure></li><li><p>input</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$input</span>=<span class="built_in">read-host</span> <span class="string">&quot;please input&quot;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Shortcut-Key"><a href="#Shortcut-Key" class="headerlink" title="Shortcut Key"></a>Shortcut Key</h2><ol><li><p>cancel the progress running</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Ctrl+C</span><br></pre></td></tr></table></figure></li></ol><h2 id="Special-methods"><a href="#Special-methods" class="headerlink" title="Special methods"></a>Special methods</h2><h4 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a | b</span><br></pre></td></tr></table></figure><h4 id="redirection"><a href="#redirection" class="headerlink" title="redirection"></a>redirection</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;<span class="comment">#append</span></span><br><span class="line">&gt;&gt;<span class="comment">#overwrite</span></span><br></pre></td></tr></table></figure><h4 id="format"><a href="#format" class="headerlink" title="format"></a>format</h4><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;...&#123;0&#125;...&#123;1&#125;...&quot;</span> <span class="operator">-f</span> <span class="variable">$first</span>, <span class="variable">$second</span>, ...</span><br></pre></td></tr></table></figure><h2 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h2><ol><li><p>definition</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$</span>[<span class="type">variable</span>-<span class="type">name</span>]=[<span class="type">variable</span>-<span class="type">value</span>]<span class="comment">#$a equals $A</span></span><br></pre></td></tr></table></figure></li><li><p>check the variables</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> variables:</span><br></pre></td></tr></table></figure></li><li><p>array</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$arr</span>=<span class="selector-tag">@</span>()<span class="comment">#empty array</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>..<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>=<span class="number">1</span>, <span class="string">&#x27;hello&#x27;</span>, <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>.count<span class="comment">#return array&#x27;s numbers</span></span><br><span class="line"></span><br><span class="line"><span class="variable">$arr</span>+=[<span class="built_in">new-element</span>]</span><br></pre></td></tr></table></figure></li><li><p>string</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$str</span>.split(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.endswith(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.contains(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.compareto(<span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.indexof(<span class="string">&quot;[char]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.insert(position<span class="literal">-num</span>, <span class="string">&quot;[chars]&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="variable">$str</span>.replace(<span class="string">&quot;old-char&quot;</span>, <span class="string">&quot;new-char&quot;</span>)</span><br></pre></td></tr></table></figure></li></ol><h2 id="Operation"><a href="#Operation" class="headerlink" title="Operation"></a>Operation</h2><ol><li><p>comparation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-eq</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#equal?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-ne</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#not equal?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-gt</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#greater?</span></span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-lt</span> [<span class="type">variable</span>-<span class="type">b</span>]<span class="comment">#less?</span></span><br></pre></td></tr></table></figure></li><li><p>bool operation</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="operator">-not</span> [<span class="type">variable</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-and</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-or</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br><span class="line"></span><br><span class="line">[<span class="type">variable</span>-<span class="type">a</span>] <span class="operator">-xor</span> [<span class="type">variable</span>-<span class="type">b</span>]</span><br></pre></td></tr></table></figure></li></ol><h2 id="Basic-Grammar"><a href="#Basic-Grammar" class="headerlink" title="Basic Grammar"></a>Basic Grammar</h2><ol><li><p>if</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(conditon)</span><br><span class="line">&#123;expr1&#125;</span><br><span class="line"><span class="keyword">elseif</span></span><br><span class="line">&#123;expr2&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;expr3&#125;</span><br></pre></td></tr></table></figure></li><li><p>switch</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span>(key<span class="literal">-variable</span>)</span><br><span class="line">&#123;</span><br><span class="line">&#123;<span class="variable">$_</span> condition1&#125;&#123;expr1&#125;</span><br><span class="line">&#123;<span class="variable">$_</span> condition2&#125;&#123;expr2&#125;</span><br><span class="line">default&#123;expr3&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>foreach</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">foreach</span>(<span class="variable">$each</span> <span class="keyword">in</span> [<span class="built_in">array</span>-<span class="type">variable</span>])</span><br><span class="line">&#123;</span><br><span class="line">expr</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>while</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(condition)</span><br><span class="line">&#123;</span><br><span class="line">expr</span><br><span class="line">[<span class="type">break</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>for</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(expr1; condition; expr2)</span><br><span class="line">&#123;</span><br><span class="line">expr3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h2 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h2><ol><li><p>definition</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">fun-name</span><span class="params">([parameter])</span></span></span><br><span class="line">&#123;</span><br><span class="line">expression</span><br><span class="line"><span class="keyword">return</span> [<span class="type">return</span>-<span class="type">value</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>using</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> [<span class="title">variable1</span>] [<span class="title">variable2</span>] ...</span></span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> powershell </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HMM-algorithm</title>
      <link href="/2020/01/06/HMM-algorithm/"/>
      <url>/2020/01/06/HMM-algorithm/</url>
      
        <content type="html"><![CDATA[<h1 id="隐马尔可夫模型-HMM"><a href="#隐马尔可夫模型-HMM" class="headerlink" title="隐马尔可夫模型(HMM)"></a>隐马尔可夫模型(HMM)</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>隐马尔可夫模型(Hidden-Markov-Model)<strong>是一种</strong>概率图</strong>模型，在<strong>深度学习</strong>出现之前，该模型被广泛应用于<em>语音识别，文本标注</em>等方面。</p><p>隐马尔可夫模型是关于时序的概率图模型。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux</title>
      <link href="/2019/12/17/Linux/"/>
      <url>/2019/12/17/Linux/</url>
      
        <content type="html"><![CDATA[<h3 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取当前目录下文件</span></span><br><span class="line">ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入子文件夹</span></span><br><span class="line"><span class="built_in">cd</span> [folder name]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 返回上一目录</span></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回上两级目录</span></span><br><span class="line"><span class="built_in">cd</span> ../..</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回home目录</span></span><br><span class="line"><span class="built_in">cd</span>/<span class="built_in">cd</span> ~ </span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回指定目录</span></span><br><span class="line"><span class="built_in">cd</span> - [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取文件内容</span></span><br><span class="line">cat [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件头</span></span><br><span class="line">head [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件尾</span></span><br><span class="line">tail [file name]</span><br><span class="line"></span><br><span class="line">tail -f [file name] <span class="comment">#实时显示文件尾，跟随日志变化</span></span><br><span class="line"><span class="comment"># 修改/编写文件内容</span></span><br><span class="line">vim [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改文件名</span></span><br><span class="line">mv [old file name] [new file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建文件</span></span><br><span class="line">touch [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建文件夹</span></span><br><span class="line">mkdir [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件</span></span><br><span class="line">rm [file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除目录下所有文件</span></span><br><span class="line">rm *</span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归删除文件夹</span></span><br><span class="line">rm -r [folder name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝文件</span></span><br><span class="line">cp [old file name] [new file name]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示文件夹内文件大小</span></span><br><span class="line">ls -lh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找指定文件(可用匹配符)</span></span><br><span class="line">find [path] -name [name]</span><br></pre></td></tr></tbody></table></figure><h3 id="Vim-操作"><a href="#Vim-操作" class="headerlink" title="Vim 操作"></a>Vim 操作</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 进入插入模式</span></span><br><span class="line">按i键</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出插入模式</span></span><br><span class="line">按Eac键</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 退出Vim</span></span><br><span class="line">:wq + Enter</span><br></pre></td></tr></tbody></table></figure><h3 id="进程操作"><a href="#进程操作" class="headerlink" title="进程操作"></a>进程操作</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让进程在后台运行</span></span><br><span class="line">nuhup [<span class="built_in">command</span>] $</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取所有进程记录</span></span><br><span class="line">ps -aux</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死进程</span></span><br><span class="line"><span class="built_in">kill</span> -9 [进程id]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示挂起进程</span></span><br><span class="line"><span class="built_in">jobs</span> -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 杀死当前bash内运行的进程</span></span><br><span class="line">Ctrl+c</span><br><span class="line"></span><br><span class="line"><span class="comment"># 挂起当前bash内运行的进程</span></span><br><span class="line">Ctrl+z</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行刚刚挂起的进程</span></span><br><span class="line">Ctrl+y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据pid查看进程</span></span><br><span class="line">ps -ef|grep [pid]</span><br></pre></td></tr></tbody></table></figure><h3 id="查看历史命令"><a href="#查看历史命令" class="headerlink" title="查看历史命令"></a>查看历史命令</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">history</span>[num]</span><br></pre></td></tr></tbody></table></figure><h3 id="查看显卡利用情况"><a href="#查看显卡利用情况" class="headerlink" title="查看显卡利用情况"></a>查看显卡利用情况</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></tbody></table></figure><h3 id="查看文件大小"><a href="#查看文件大小" class="headerlink" title="查看文件大小"></a>查看文件大小</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">du -ah</span><br></pre></td></tr></tbody></table></figure><h3 id="查看磁盘"><a href="#查看磁盘" class="headerlink" title="查看磁盘"></a>查看磁盘</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df -h</span><br></pre></td></tr></tbody></table></figure><h3 id="查看硬件信息"><a href="#查看硬件信息" class="headerlink" title="查看硬件信息"></a>查看硬件信息</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/cpuinfo # 查看cpu，内存等</span><br></pre></td></tr></tbody></table></figure><h3 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实时查看GPU显存利用率，需要用pip按照gpustat</span></span><br><span class="line">watch --color -n1 gpustat -cpu</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看显存利用率</span></span><br><span class="line">nvidia-smi</span><br></pre></td></tr></tbody></table></figure><h3 id="远程传文件"><a href="#远程传文件" class="headerlink" title="远程传文件"></a>远程传文件</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 本地传文件到远程</span></span><br><span class="line">scp [-r] local_path user@255.255.255.255:server_path</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 远程传到本地</span></span><br><span class="line">scp [-r] user@255.255.255.255:server_path local_path</span><br></pre></td></tr></tbody></table></figure><h3 id="更新操作"><a href="#更新操作" class="headerlink" title="更新操作"></a>更新操作</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 更新源列表信息</span></span><br><span class="line">apt-get update</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 更新软件</span></span><br><span class="line">apt-get upgrade</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看可以更新软件</span></span><br><span class="line">apt-get list --upgradable</span><br></pre></td></tr></tbody></table></figure><h3 id="查看网络端口占用情况"><a href="#查看网络端口占用情况" class="headerlink" title="查看网络端口占用情况"></a>查看网络端口占用情况</h3><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netsta</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch</title>
      <link href="/2019/12/15/Pytorch/"/>
      <url>/2019/12/15/Pytorch/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><h3 id="background"><a href="#background" class="headerlink" title="background"></a>background</h3><p>Torch是一个使用Lua语言的神经网络</p><h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h3 id="import"><a href="#import" class="headerlink" title="import"></a>import</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="data-type"><a href="#data-type" class="headerlink" title="data type"></a>data type</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create empty tensor</span></span><br><span class="line">torch.empty(a, b, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create rand tensor</span></span><br><span class="line">torch.rand(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create zero tensor</span></span><br><span class="line">torch.zeros(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create ones tensor</span></span><br><span class="line">torch.ones(a, b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create from list</span></span><br><span class="line">torch.tensor(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create rand tensor according a tensor</span></span><br><span class="line">torch.rand_like(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the size of a tensor</span></span><br><span class="line">x.size() <span class="comment"># return a tuple</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># resize the tensor</span></span><br><span class="line">x.view(-<span class="number">1</span>, b)</span><br></pre></td></tr></tbody></table></figure><h3 id="tansformation"><a href="#tansformation" class="headerlink" title="tansformation"></a>tansformation</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># numpy to tensor</span></span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line"><span class="comment"># the ndarray of numpy and the tensor of torch share the same storage space</span></span><br><span class="line"><span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor to numpy</span></span><br><span class="line">np_data = torch_data.numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># int to tensor</span></span><br><span class="line">torch_data = torch.IntTensor(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># float to tensor</span></span><br><span class="line">torch_data = torch.FloatTensor(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor to python(only one element in torch_data)</span></span><br><span class="line">data = torch_data.item()</span><br></pre></td></tr></tbody></table></figure><h3 id="basic-math-method"><a href="#basic-math-method" class="headerlink" title="basic math method"></a>basic math method</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># matrix multiplication</span></span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">tensor = torch.FloatTensor(data)</span><br><span class="line">ans = torch.mm(tensor, tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate mean</span></span><br><span class="line">mean = torch.mean(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># add</span></span><br><span class="line">torch.add(a, b)</span><br><span class="line"><span class="built_in">print</span>(a + b)</span><br><span class="line">a.add_(b) <span class="comment"># change a</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"><span class="comment"># any func that can change the tensor has a '_' in its name</span></span><br><span class="line"><span class="comment">###########################################################</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="activation-function"><a href="#activation-function" class="headerlink" title="activation function"></a>activation function</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># relu</span></span><br><span class="line">torch.relu(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># sigmoid</span></span><br><span class="line">torch.sigmoid(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tanh</span></span><br><span class="line">torch.tanh(tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># softmax</span></span><br><span class="line">torch.softmax(tensor)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h2 id="examples"><a href="#examples" class="headerlink" title="examples"></a>examples</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># regression</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">torch.nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, n_feature, n_hidden, n_output</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(n_feature, n_hidden)</span><br><span class="line">        self.predict = torch.nn.Linear(n_hidden, n_output)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self</span>):</span></span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    net = Net(<span class="number">1</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(net)</span><br><span class="line">    </span><br><span class="line">    optimizer = torch.optim.SGD(net.parameters(), lr = <span class="number">0.5</span>)</span><br><span class="line">    loss_func = torch.nn.MSELoss()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        prediction = net(x)</span><br><span class="line">        </span><br><span class="line">        loss = loss_func(prediction, y)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># classification</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="autograd"><a href="#autograd" class="headerlink" title="autograd"></a>autograd</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># turn on or turn of autograd</span></span><br><span class="line">torch_data.requires_grad_()</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">requires_grad = <span class="literal">True</span></span><br><span class="line">requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># backward</span></span><br><span class="line">out.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get a same tensor without requiring gradient</span></span><br><span class="line">torch_data.detach()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Variational Inference</title>
      <link href="/2019/12/14/Variational-Inference/"/>
      <url>/2019/12/14/Variational-Inference/</url>
      
        <content type="html"><![CDATA[<h1 id="Variational-Inference-变分推断"><a href="#Variational-Inference-变分推断" class="headerlink" title="Variational Inference(变分推断)"></a>Variational Inference(变分推断)</h1><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>$X\quad:observed\ data$</p><p>$Z\quad:latent\ variable+parameter$</p><p>$(X+Z)\quad:complete\ data$</p><p>$$log(P(x))=logP(x,z)-logP(z|x)=log\frac{P(x,z)}{q(z)}-log\frac{P(z|x)}{q(z)}$$</p><p>$$Left=\int_zlogP(x)q(z)dz=logP(x)$$</p><p>$$Right=\int_zq(z)log\frac{P(x,z)}{q(z)}dz-\int_zq(z)log\frac{P(z|x)}{q(z)}dz$$</p><p>$$ELBO(evidence\ lower\ bound)=\int_zq(z)log\frac{P(x,z)}{q(z)}$$</p><p>$$KL(q||p)=-\int_zq(z)log\frac{P(z|x)}{q(z)}dz\ge0$$</p><p>$$\quad\mathscr{L}(q)+KL(q||p)$$</p><p>令 $q(z)=\prod_{i=1}^mq_i(z_i)$</p><p>有 $\mathscr{L}(q)=\int_zq(z)logP(x,z)dz-\int_zq(z)logq(z)dz$</p>]]></content>
      
      
      
        <tags>
            
            <tag> machine-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multiple sequence alignment with hierarchical clustering</title>
      <link href="/2019/12/05/Multiple-sequence-alignment-with-hierarchical-clustering/"/>
      <url>/2019/12/05/Multiple-sequence-alignment-with-hierarchical-clustering/</url>
      
        <content type="html"><![CDATA[<h1 id="Multiple-sequence-alignment-with-hierarchical-clustering"><a href="#Multiple-sequence-alignment-with-hierarchical-clustering" class="headerlink" title="Multiple sequence alignment with hierarchical clustering"></a>Multiple sequence alignment with hierarchical clustering</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p> 无论是在蛋白质还是核酸的<strong>多序列的匹配</strong>问题，用计算机来实现，都是非常容易和准确的。这个方法是基于<strong>两两匹配</strong>的普通的<strong>贪心算法</strong>。开始的时候，用<strong>双匹配</strong>的<strong>分数矩阵来</strong>来实现序列的<strong>层次聚类</strong>。<strong>最近</strong>的序列会被<strong>联合</strong>起来，从而生成<strong>联合序列</strong>的群体(group)。然后，当一个<strong>群体</strong>内的所以序列都被<strong>联合(Aligned)<strong>起来后，最近的</strong>群体(Group)<strong>的会被联合(aligned)起来。在</strong>多匹配</strong>中的两个匹配好的序列会生成一个新的矩阵，而这个矩阵被用来产生一个<strong>层次聚类</strong></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>  核酸(nucleic acids)和蛋白质(proteins)，可以被一些<strong>升序排列</strong>的分子生物学上的<strong>数字序列</strong>来表示，而这些数字可以通过自动且快速的技术来获得。因此，一个<strong>升序序列</strong>需要被我们分析，而这个在没有<strong>数据分析</strong>的帮助是不可能的。</p><p>  确认某些部分和一个相同的<strong>族</strong>(family)里面的许多序列对应部分相似的过程是非常有趣的。比如，蛋白质序列的相似区域和许多活着的的微生物(organism)有着相同的功能，这在功能和结构的观点来看是非常重要的。</p><p>  这些分析都需要，<strong>序列的匹配</strong></p><p>  两个序列的匹配可以许多自从1970就出现的算法实现。但是，当有多于两个序列时，</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="两个序列的匹配算法"><a href="#两个序列的匹配算法" class="headerlink" title="两个序列的匹配算法"></a>两个序列的匹配算法</h3><p>假设存在两个序列 $A、B$ 长度分别为 $m,n$，并且 $A(i),B(j)$ 分别表示 对应序列中的第 $i$ 个和第 $j$ 的元素。对于每一个<strong>元素对</strong>(pair of elements) $A(i),B(j)$ ，其权值 $weight\ w(i,j)$ 可以通过一个适合的矩阵 $D$ 来给定，比如<strong>Dayhoff</strong> 的氨基酸<strong>突变数据矩阵</strong>（如果需要的话，可以通过增加一个适当的<strong>常数</strong>来使得矩阵<strong>非负</strong>）。由此 $w(i,j)=D(A(i),A(j))$。$w$ 的值不需要存储，当需要的时候从存储的矩阵计算获得。计算中使用的方法和<strong>Needleman and Wunsh</strong>和<strong>Murata</strong>的方法一样，是从<strong>元素</strong>(cell) $(m,n)$ 开始，<strong>后向</strong>(backward)计算，依次计算从每条不同的元素来的<strong>路</strong>(path)的最大权值之和(maximum total value)。</p><p>令 $S(i,j)$ 表示从所有的从元素 $(i,j)$ 到底部(bottom)或者右边(right side)的路径上的所有元素权值之和再减去 $g$ 乘以路上的<strong>间隔</strong>(gap)数，即 $\sum w-g(n-1)$，的<strong>最大值</strong></p><p>这个<strong>间隔惩罚</strong>(gap penalty)是<strong>Barton and Sternberg</strong>建议的独立于间隔的长度。</p><p>令 $M(i,j)$ 表示 $S$ 上所有满足 $(i,k) and(l,j)\quad(j\le k\le n\ and\ i\le l\le m)$元素的最大值，根据它的定义 $M(i,j)$ 就表示 $S$ 上所有满足  $(l,k),(l\ge i \ and \ k\ge j)$ 的元素最大值。</p><p>下面的算法就是用来计算 $S\ and\ M$的：</p><p>$$S(i,j)=w(i,j)+max(S(i+1,j+1,M(i+1,j+1)-g)$$</p><p>$$M(i,j)=max(S(i,j),M(i+1,j),M(i,j+1))$$</p><p>一旦矩阵 $S$ 被计算出来，就会执行一个回溯(traceback) 的过程，取寻找最好的路径上的最优元素。它的首选元素就是<strong>第一行或第一列的最大元素</strong>。这个值就是<strong>匹配的分数</strong>。在每个路径的末尾都不需要增加<strong>间隔惩罚</strong>(gap penalty)。</p><p>为了比较多于两个的序列，已经匹配好的会被利用一个<strong>匹配算法</strong>(alignment algorithm)一步一步的重新组合，这也是<strong>过程一</strong>的延申。</p><h3 id="两个联合序列类的联合"><a href="#两个联合序列类的联合" class="headerlink" title="两个联合序列类的联合"></a>两个联合序列类的联合</h3><p>令 $B_1,…B_p$ 为一个<strong>类</strong>(cluster)中的序列，$C_1,…,C_Q$ 为第二个类中的序列。当生成一个矩阵 $S$ 来<strong>联合</strong>(align) $C$ 序列和 $B$ 序列。我们需要引入一个<strong>分数体制</strong>(scoring scheme)，它包含所有的以前已经联合起来的序列的贡献，因此需要赋予已经联合起来的区域更多的权重。</p><p>令 $i,j$ 分别表示序列 $B,C$ 某个<strong>联合的产物</strong>(aligned residue)的位置。则：</p><p>$$w(i,j)=\frac{1}{PQ}\sum_{R=1}^{R=P}\sum^{S=Q}_{S=1}D(B_R(i),C_S(j))$$</p><p>其中 $D$ 表示<strong>氨基酸联合分数</strong>(amino acid pair scores)。例如，**(Ala-Val-Leu)<strong>和</strong>(Ala-Leu)**联合的分数就是 $[w(Ala\ vs.\ Ala)+(Ala\ vs.\ Ala)+(Val\ vs. Ala)+(Val\ vs.\ Leu)+(Leu\ vs.\ Ala)+(Leu\ vs.\ Leu)]*\frac{1}{6}$</p><p>矩阵 $S$ 和矩阵 $M$ 和前面的算法一样，但是需要用新的 $w$ 来计算。</p><p>一旦获得一个由 $P+Q$ 联合的<strong>类</strong>，就会替代 $P\ Q$ 序列。</p><h3 id="聚类的顺序"><a href="#聚类的顺序" class="headerlink" title="聚类的顺序"></a>聚类的顺序</h3><p>聚类的顺序会影响聚类的结果，因此我们需要选择一个好的聚类顺序。这里用到的方法是：使用两两比较的得分作为序列间的相似度的序号。原则就是：聚类从基本的序列出发，通过<strong>联合两个最近的类</strong>产生新的类。</p><p>假设 $A_1,A_2,…,A_N$ 是 $N$ 个待联合的序列，所有的<strong>两两比较</strong>已经执行好，并被存储在一个矩阵 $T_1$ 中。其中 $T_1(I,J)$ 表示 $A_I$ 和 $A_J$ 联合的分数。然后，联合的序列的类按如下定义：</p><p>步骤一，有N个类，每个类有1个序列。最优分数在矩阵 $T_1$ 中。序列 $A_I$ 和序列 $A_J$ 【他们的分数是最适合的】被<strong>联合</strong>(aligned)起来，并且两个序列的<strong>联合体</strong>(alignment)产生一个聚类，这个类代替了第 $I$ 个序列，第 $J$ 个序列则被删除。然后，产生新的<strong>分数矩阵</strong> $T_2$，它的维度是 $N-1$ ，并且它等于：将 $T_1$ 的<strong>第 $J$ 行和第 $J$ 列</strong>删除，且<strong>第 $I$ 行和第 $I$ 列</strong>重新从 $T_1$ 中的 <strong>第 $I\  J$行列</strong>产生。其中，$T_2(I,k)$ 是 $T_1(I,k)$ 和 $T_1(J,k)$ 的平均值，$T_2(I,k)$ 称为**类 $I$ <strong>对</strong>类 $J$ **的分数(cluster $I$ vs. cluster $K$)。</p><p>步骤 $s\quad(s=1,2…,N-1)$，现在有 $N-s+1$个序列的类，$T_S$ 表示分数矩阵，如果 $T_S$ 的最大元素是 $T_S(I,J)$，<strong>类 $I$<strong>和</strong>类 $J$</strong> 将会被<strong>联合</strong>，并且会产生一个新的**类 $I$**，类 $J$ 会被删除。$T_{S+1}$ 按如下算法生成：</p><p>$$T_{S+1}(K,L)=T_S(K,L)\quad  if\ K,L\not=I,J$$</p><p>$$T_{s+1}(J,K)and \ T_{s+1}(K,J)$$ 不存在</p><p>$$T_{s+1}(I,K) = T_s(K,I)=(N_IT_s(I,K)+N_JT_s(J,K))/(N_I+N_J)\quad if K\not=I,J$$</p><p>其中 $N_I$ 是**类 $I$**中的序列数，类似 $N_J$</p><h3 id="完整的算法"><a href="#完整的算法" class="headerlink" title="完整的算法"></a>完整的算法</h3><ol><li>初始化：执行所有的两两之间的比较，并且记录他们的分数；</li><li>用得分矩阵进行序列的聚类；</li><li>利用两两聚类的分数获取完整的联合体，使层次树生长；</li><li>联合体产生，计算多个联合体之间的分数</li><li>用这些新的分数计算新的层次聚类</li><li>如果新的聚类和旧的不一样，可以根据聚类情况产生新的联合体转至3，否则结束循环。</li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h2 id="算法分析"><a href="#算法分析" class="headerlink" title="算法分析"></a>算法分析</h2>]]></content>
      
      
      
        <tags>
            
            <tag> Algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mySQL Command Line</title>
      <link href="/2019/11/09/mySQL-Command-Line/"/>
      <url>/2019/11/09/mySQL-Command-Line/</url>
      
        <content type="html"><![CDATA[<h1 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h1><h5 id="新建数据库"><a href="#新建数据库" class="headerlink" title="新建数据库"></a>新建数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE library;</span><br></pre></td></tr></table></figure><h5 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP DATABASE library</span><br></pre></td></tr></table></figure><h5 id="使用数据库"><a href="#使用数据库" class="headerlink" title="使用数据库"></a>使用数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USE library</span><br></pre></td></tr></table></figure><h5 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE book(name char(20), author char(20))</span><br></pre></td></tr></table></figure><h5 id="查看表的内容"><a href="#查看表的内容" class="headerlink" title="查看表的内容"></a>查看表的内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM book WHERE id &#x3D; 1;</span><br></pre></td></tr></table></figure><h5 id="插入内容"><a href="#插入内容" class="headerlink" title="插入内容"></a>插入内容</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INSERT INTO book VALUES(&#39;jave&#39;, &#39;kkk&#39;)</span><br></pre></td></tr></table></figure><h5 id="查看数据库内的表"><a href="#查看数据库内的表" class="headerlink" title="查看数据库内的表"></a>查看数据库内的表</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW TABLES</span><br></pre></td></tr></table></figure><h5 id="查看数据库"><a href="#查看数据库" class="headerlink" title="查看数据库"></a>查看数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SHOW DATABASES</span><br></pre></td></tr></table></figure><h5 id="显示表的结构"><a href="#显示表的结构" class="headerlink" title="显示表的结构"></a>显示表的结构</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DESC book</span><br></pre></td></tr></table></figure><h5 id="获取上一次插入数据的ID"><a href="#获取上一次插入数据的ID" class="headerlink" title="获取上一次插入数据的ID"></a>获取上一次插入数据的ID</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT LAST_INSERT_ID();</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GitHub Command Line</title>
      <link href="/2019/10/29/GitHub-Command-Line/"/>
      <url>/2019/10/29/GitHub-Command-Line/</url>
      
        <content type="html"><![CDATA[<h3 id="链接仓库"><a href="#链接仓库" class="headerlink" title="链接仓库"></a>链接仓库</h3><ol><li><p>复制仓库的链接</p><p><img src="/2019/10/29/GitHub-Command-Line/one.png" alt="Position"></p></li><li><p>在需要链接的文件夹下面打开<strong>Git Bash</strong></p><p><img src="/2019/10/29/GitHub-Command-Line/two.png" alt="git"></p></li><li><p>命令行界面如下：</p><p><img src="/2019/10/29/GitHub-Command-Line/three.png" alt="GitBash"></p></li><li><p>运行命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git init<span class="comment">#初始化本地仓库</span></span><br><span class="line">git config user.email<span class="string">&quot;your email@address&quot;</span></span><br><span class="line">git config user.name<span class="string">&quot;your name&quot;</span></span><br><span class="line">git add .</span><br><span class="line">git commit -m<span class="string">&#x27;My first post&#x27;</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure></li><li><p>获取ssh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.ssh</span><br><span class="line">cat id_rsa.pub</span><br></pre></td></tr></table></figure></li><li><p>撤回上一次commit</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset HEAD~</span><br><span class="line">git reset HEAD@&#123;index&#125;</span><br></pre></td></tr></table></figure></li><li><p>获取已经commit但未push的文件信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git cherry -v</span><br></pre></td></tr></table></figure></li><li><p>当前提交状态</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure></li><li><p>列出所有操作记录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git reflog</span><br></pre></td></tr></table></figure></li><li><p>合并到最后一次提交</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit --amend</span><br></pre></td></tr></table></figure></li><li><p>获取最新提交记录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git cherry-pick master</span><br></pre></td></tr></table></figure></li><li><p>对远程仓库的操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看所有的远程仓库</span></span><br><span class="line">git remote</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看远程仓库对应的地址</span></span><br><span class="line">git remote -v</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置新的远程仓库地址</span></span><br><span class="line">git remote set-url origin [url]</span><br></pre></td></tr></table></figure></li></ol><h3 id="Git-Problem"><a href="#Git-Problem" class="headerlink" title="Git Problem"></a>Git Problem</h3><ol><li><pre><code class="bash">fatal: unable to access &#39;https://github.com/Vilily/python.git/&#39;: SSL certificate problem: self signed certificate in certificate chainsolution：git config --global http.sslVerify false</code></pre></li><li></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> GitHub </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PCA algorithm</title>
      <link href="/2019/10/28/PCA-algorithm/"/>
      <url>/2019/10/28/PCA-algorithm/</url>
      
        <content type="html"><![CDATA[<h1 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>PCA(Principal Component Analysis)【主成分分析】</strong>，是一种常用的<strong>无监督学习(Unsupervised Learning)<strong>算法，它主要被用在数据降维方面，是机器学习中一种数据进行</strong>预处理</strong>的重要方法</p><h2 id="降维-Dimensionality-Reduction"><a href="#降维-Dimensionality-Reduction" class="headerlink" title="降维(Dimensionality Reduction)"></a>降维(Dimensionality Reduction)</h2><h3 id="啥是降维"><a href="#啥是降维" class="headerlink" title="啥是降维"></a>啥是降维</h3><p><strong>降维</strong>就是将原来维度过高的数据集经过<strong>线性</strong>或<strong>非线性</strong>的变换，变成低维的数据集。比如我有一个观测数据数据 $\vec x={1,2,2.3}$ ，经过某个变换后，剔除了它的<strong>多余</strong>的维度得到 $\vec x={2,2.3}$。这就是一个降维的过程</p><h3 id="为啥要降维"><a href="#为啥要降维" class="headerlink" title="为啥要降维"></a>为啥要降维</h3><p>降维的原因有许多，这里主要讲两个比较重要的原因：</p><ol><li>剔除无用的维度可以减少训练样本的数据量，便于我们的训练学习过程，特别是当样本量特别巨大时【一般有几百G或者几T的数据】。</li><li>降低数据的<strong>多余的无用的</strong>维度可以提高训练出来的模型的<strong>泛化能力</strong>，避免**过拟合(Over Fitting)**。因为数据维度过高时需要训练的模型参数也会增加，这就很容易导致模型过拟合。</li></ol><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>比如我们看这些数据：</p><p><img src="/2019/10/28/PCA-algorithm/Figure_2.png" alt="Figure_1"></p><img src="/2019/10/28/PCA-algorithm/Figure_3.png" alt="Figure_2" style="zoom: 200%;"><p>我们发现，这些数据在y维度上的偏差均匀分布且偏差极小，因此可以对其进行降维处理【删除该维度】，得到下列数据</p><p><img src="/2019/10/28/PCA-algorithm/Figure_1.png" alt="Figure"></p><p>这些数据不仅保留了原数据的<strong>主要特征</strong>，而且剔除了多余的干扰，更利于训练何提高泛化性。</p><h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><h3 id="PCA降维原则"><a href="#PCA降维原则" class="headerlink" title="PCA降维原则"></a>PCA降维原则</h3><p>我们先考虑二维情况，然后再推广到高维。</p><p>考虑下面的数据集：</p><p><img src="/2019/10/28/PCA-algorithm/Figure_5.png" alt="data"></p><p>可以发现，图中不在线上的蓝色数据点基本上沿着一条直线分布。因此我们可以断定，数据集的 x 和 y 之间存在相关关系。因此我们可以进行降维。</p><p>为了对这个数据集进行降维，我们首先要确定什么样的<strong>维</strong>是<strong>好的维</strong>，什么样的<strong>维</strong>是我们要<strong>丢弃的维</strong>。</p><p>为此我们分别沿数据的分布方向和其垂直方向做两条垂直的直线(如图)，根据<strong>线性代数</strong>相关知识，这两条直线代表的构成了该数据空间的<strong>标准正交基</strong>。也就是说以这两条直线为<strong>坐标轴</strong>也可以表示该数据集。</p><p>现在我们将数据点分别<strong>投影</strong>到这两条直线上(如上图)。</p><p>如果进行降维，我们必须要<strong>丢弃一个坐标轴</strong>，也就是所降维后的数据集就是，原始数据集在这两条线<strong>其中一条</strong>上的投影。</p><p>假设我们选择<strong>保留橘黄色</strong>的坐标轴，而丢弃蓝色坐标轴，可以发现：数据的分布十分紧密【<strong>方差较小</strong>】，因此数据对原始数据信息(<strong>数据的分布特征</strong>)保留的也较少。</p><p>而相反，如果我们选择<strong>保留蓝色坐标轴</strong>，这时：数据的分布十分稀疏【<strong>反差较大</strong>】，因此数据对原始数据信息的保留的也较多。</p><p>根据上面的分析，我们得出结论：</p><p>进行降维时我们要保留<strong>使数据在该维度上面投影方差最大</strong>的维度，而丢弃方差最小的维度，这也就是**主成分分析(PCA)**的主要思想。</p><h3 id="PCA推导"><a href="#PCA推导" class="headerlink" title="PCA推导"></a>PCA推导</h3><p>原始数据集 ${\vec x_i},\quad (i=1,2,…,N)$，为<strong>列向量</strong>；</p><p>为了方便计算我们先对数据集进行<strong>中心化</strong>即令 $ \vec x_i=\vec x_i-\vec\mu$ ，$\vec\mu$ 为原始数据集的均值，下面使用的$x_i$ 都指以及<strong>中心化</strong>的数据。</p><p>所以<strong>中心化</strong>后的数据的<strong>平均值</strong> $\vec\mu=0$;</p><p>假设某一<strong>正交基</strong>为 $\vec{u}$ 即投影坐标轴的方向；</p><p>因此 $\vec x_i$ 在 $\vec u$ 方向的投影坐标为 $\hat x_i = \vec x_i^T\cdot\vec u$【根据线性代数知识】；</p><p>而投影后的样本均值</p><p>$$\hat \mu = \frac{\sum_{i=1}^N\hat x_i}{N}=0$$</p><p>因此<strong>PCA</strong>的求解目标就是:</p><p>$$J=\frac{1}{N}\sum_{i=1}^N\Big((\vec x_i^T\cdot\vec u)-\hat\mu\Big)^2$$</p><p>$$s.t.\quad \vec u^T\vec u=1$$</p><p>$$=\frac{1}{N}\sum_{i=1}^N(\vec x_i^T\cdot\vec u)^2$$</p><p>$$=\frac{1}{N}\sum_{i=1}^N\vec u^T\cdot \hat x_i\cdot \hat x_i^T\cdot\vec u$$</p><p>$$=\frac{1}{N}\vec u^T\Big(\sum_{i=1}^N\hat x_i\cdot\hat x_i^T\Big)\vec u$$    <font color="red">(*)</font></p><h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><p>根据统计学相关知识，协方差为：</p><p>$$\Sigma=cov(\vec x,\vec x)=E[(\vec x-\vec\mu)\cdot(\vec x-\vec\mu)^T]$$</p><hr><p>因此，<font color="red">(*)</font>式可以化为：</p><p>$$J=\vec u^T\Sigma\vec u$$</p><p>于是我们最终的优化目标就是：</p><p>$$argmax_u J=argmax_u(\vec u^T\Sigma\vec u)$$</p><p>$$s.t.\quad\vec u^T\vec u=1$$    <font color="red">(1)</font></p><h4 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h4><p>为了求解上面得到的<strong>最优化</strong>问题，我们引入<strong>拉格朗日乘子</strong> $\lambda$</p><p>定义<strong>拉格朗日函数</strong>：</p><p>$$L=\vec u^T\Sigma\vec u-\lambda(\vec u^T\vec u-1)$$</p><p>对该函数求导并令其为0得：</p><p>$$\Sigma\vec u-\lambda\vec u=0$$</p><p>即：$\Sigma\vec u=\lambda\vec u$</p><p>由线性代数知识得到，$\lambda$ 为 $\Sigma$ 得特征值，$\vec u$  对应得特征向量，于是：</p><p>$$J=\vec u^T\Sigma\vec u=\vec u^T\lambda\vec u=\lambda$$</p><p>又因为 $\Sigma$ 的特征向量都是相互<strong>正交</strong>的<strong>单位向量</strong>，且所有的特征向量构成了原数据集的<strong>标准正交基</strong></p><p>所以，对原数据降维只需要：<strong>保留较大特征值对应的特征向量，而丢弃较小特征值对应的特征向量</strong>。</p><p>以上就是<strong>主成分分析</strong>的主要方法。</p><h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>**奇异值分解(PCA,Principle Component Analysis)**，是矩阵论的重要内容。</p><p><strong>奇异值分解基本定理</strong>：若A为一个$m\times n$ 实矩阵，$A\in R^{m\times n}$，则 $A$ 的奇异值分解存在：</p><p>$$\vec A=\vec U\vec \Sigma\vec V^T$$    <font color="red">(#)</font></p><p>其中 $U$ 是m阶正交矩阵，$V$是n阶正交矩阵， $\Sigma$ 是 $m\times n$ 对角矩阵，其对角线元素非负，且按降序排列。</p><h4 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h4><p>此处简单进行一下<strong>构造性证明</strong></p><ol><li><p>确定 $\vec V$：</p><p>对于待分解的 $m\times n$ 矩阵 $A$，构造一个 n阶 <strong>实对称矩阵</strong> $A^TA$。由线性代数知识我们得到 $A^TA$ 的 n 个<strong>特征值</strong>，记为 $\lambda_i(i=1,2,…,n)$，每个矩阵对应的<strong>特征向量</strong>记为 $\vec v_i(i=1,2,…,n)$。</p><p>则我们令：</p><p>$$\vec V=[\vec v_1,\vec v_2,…,\vec v_n]$$</p></li></ol><ol start="2"><li><p>确定 $\vec\Sigma$：</p><p>令 $\sigma_i=\sqrt{\lambda_i}(i=1,2,…,n)$</p><p>$$\Sigma_1=\begin{pmatrix}\sigma_1&amp;0&amp;\dots&amp;0\\0&amp;\sigma_2&amp;\dots&amp;0\\\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\0&amp;0&amp;\dots&amp;\sigma_n\end{pmatrix}$$</p><p>则：</p><p>$$\Sigma=\begin{pmatrix}\Sigma_1&amp;0\\0&amp;0\end{pmatrix}$$</p></li></ol><ol start="3"><li><p>确定 $\vec U$：</p><p>令 $\vec u_i=\frac{1}{\sigma_i}\vec A\vec v_i$</p><p>令 $\vec U_1=[\vec u_1, \vec u_2,…,\vec u_n]$，$\vec U_2$ 为 $\vec A^T$ 的一组<strong>标准正交基</strong>。</p><p>则最终的：</p><p>$$\vec U=[\vec U_1,\vec U_2]$$</p></li></ol><ol start="4"><li>由线性代数知识知道上面得到的矩阵满足 <font color="red">(#)</font>式，且 $\vec V,\vec U$ 均为正交矩阵，$\vec\Sigma$ 是对角阵。</li></ol><h3 id="用SVD求PCA"><a href="#用SVD求PCA" class="headerlink" title="用SVD求PCA"></a>用SVD求PCA</h3><h4 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h4><p>对原始得数据矩阵 $\vec X=[\vec x_1,\vec x_2,…,\vec x_n]$【已经进行了<strong>中心化</strong>】进行奇异值分解：</p><p>$$\vec X=\vec U\vec\Sigma\vec V^T$$</p><p>由<font color="red">(1)</font>式得，<strong>主成分</strong>就是原<strong>协方差矩阵</strong>的特征值。</p><p>又由协方差矩阵的定义得 $\vec\Sigma=\vec X^T\vec X=\vec V\vec\Sigma^T\vec U^T\vec U\vec\Sigma\vec V^T$</p><p>因为 $\vec U^T\vec U=\vec I,\vec V^T\vec V=\vec I$</p><p>所以上式继续化为</p><p>$$\Sigma=\vec V\Sigma^T\vec\Sigma\vec V^T$$</p><p>$$=\vec V\vec\Sigma^2\vec V^T$$</p><p>上式也就是协方差矩阵的<strong>特征值分解</strong>，所以 $\vec V$ 矩阵对应的列向量也就是 <strong>主成分</strong></p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GMM algorithm</title>
      <link href="/2019/10/25/GMM-algorithm/"/>
      <url>/2019/10/25/GMM-algorithm/</url>
      
        <content type="html"><![CDATA[<h1 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p><strong>高斯混合模型(Gaussian Mixture Model)<strong>，是前面推导的</strong>EM算法</strong>的一个重要应用。高斯混合模型应用广泛，特别是在**聚类(Cluster)**学习方面，是一种软聚类器。</p><h2 id="模型推导"><a href="#模型推导" class="headerlink" title="模型推导"></a>模型推导</h2><h3 id="高斯分布定义"><a href="#高斯分布定义" class="headerlink" title="高斯分布定义"></a>高斯分布定义</h3><p>我们先回顾一下高斯分布的定义：</p><p>对于n维样本空间 $\chi$ 中的随机变量 $x$，若 $x$ 服从高斯分布，其概率密度函数为：</p><p>$$P(x) =\phi(x) = \frac{1}{2\pi^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}e^{-\frac{1}{2}\frac{(x-\mu)^T(x-\mu)}{\Sigma}}$$    <font color="red">(1)</font></p><p>其中 $\vec\mu$ 为n维均值向量，$\vec\Sigma$ 为 $n*n$ 的<strong>协方差矩阵</strong>，由<font color="red">(1)</font>可以看出：高斯分布<strong>完全</strong>由均值向量 $\vec\mu$ 和协方差矩阵 $\vec\Sigma$ 这两个参数确定，为了明确高斯分布与相应参数的依赖关系，将概率密度函数记为$$\phi(\vec x|\vec\mu,\vec\Sigma)$$</p><p>下文中为了方便，<strong>向量不再加箭头</strong>。</p><h3 id="高斯混合模型-1"><a href="#高斯混合模型-1" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h3><h4 id="1-明确隐变量，写出完全数据的对数似然函数"><a href="#1-明确隐变量，写出完全数据的对数似然函数" class="headerlink" title="1.明确隐变量，写出完全数据的对数似然函数"></a>1.明确隐变量，写出完全数据的对数似然函数</h4><p>为了表达方便我们记 $\theta = (\mu,\Sigma)$ 为高斯分布的参数。</p><p>于是我们可以定义<strong>高斯混合模型</strong>为：</p><p>$$P(y|\theta) = \sum_{k=1}^{K}\alpha_k\phi(y|\theta_k)$$    <font color="red">(2)</font></p><p>其中，$\alpha_k$ 是系数，$\alpha_k\ge0$，$\sum_{k=1}^K=1$；$\phi(y|\theta_k)$ 是高斯概率密度函数。</p><p>$$\phi(y|\theta_k)=\frac{1}{\sqrt{2\pi}\sigma_i}exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})$$        <font color="red"> (3)</font></p><p><font color="red">(3)</font>式称为第 $i$ 个分模型。</p><p>设想观测数据 $y_j$ 是这样产生的：首先依概率 $\alpha_i$ 选择 第 $k$ 个高斯分布分模型 $\phi(y|\theta_k)$，然后依第 $k$ 个分模型的概率分布 $\phi(y|\theta_k)$ 生成观测数据 $y_j$。这时，观测数据 $y_j$ 是已知的；反映观测数据 $y_j$ 来自第 $k$ 个分模型的数据是未知的，$k=1,2,…,K$，以隐变量 $\gamma_{jk}$ 表示，其定义如下：</p><p>$$\gamma_{jk}=\begin{cases}1, \quad 第j个观测来自第k个模型\\0,\quad 否则\end{cases}$$</p><p>$$j=1,2,…,N;k=1,2,…,k$$    <font color="red">(4)</font></p><p>$\gamma_{jk}$ 是0-1随机变量【但也可以看成一种加权平均】</p><p>有了观测数据 $y_j$ 以及未观测数据 $\gamma_{jk}$，那么完全数据是：</p><p>$$(y_j,\gamma_{j1},\gamma_{j2},…,\gamma_{jk}),\quad j=1,2,…,N$$</p><p>于是我们可以写出完全数据的似然函数：</p><p>$$P(y,\gamma|\theta)=\prod_{j=1}^NP(y_j,\gamma_{j1},\gamma_{j2},…,\gamma_{jk}|\theta)$$</p><p>$$=\prod_{k=1}^K\prod_{j=1}^N[\alpha_k\phi(y_j|\theta_k)]^{\gamma_{jk}}$$</p><p>$$=\prod_{k=1}^K\alpha_k^{n_k}\prod_{i=1}^N[\phi(y_j|\theta_k)]^{\gamma_{jk}}$$</p><p>$$=\prod_{k=1}^K\alpha_k^{n_k}\prod_{j=1}^N[\frac{1}{\sqrt{2\pi}\sigma_k}exp(-\frac{(y_j-mu_k)^2}{2\sigma_k^2})]^{\gamma_{jk}}$$</p><p>式中，$n_k=\sum_{j=1}^N\gamma_{jk},\sum_{k=1}^Kn_k=N$</p><p>那么，完全数据的对数似然函数为：</p><p>$$logP(y,\gamma|\theta)=\sum_{k=1}^K\Bigg(n_klog\alpha_k+\sum_{j=1}^N\gamma_{jk}\Big[log\big(\frac{1}{\sqrt{2\pi}}-log\sigma_k-\frac{1}{2\sigma_k^2}(y_j-\mu_k\big)^2\Big]\Bigg)$$    <font color="red">(5)</font></p><h4 id="2-EM算法的E步：确定Q函数"><a href="#2-EM算法的E步：确定Q函数" class="headerlink" title="2.EM算法的E步：确定Q函数"></a>2.EM算法的E步：确定Q函数</h4><p>根据Q函数定义得到：</p><p>$$Q(\theta,\theta^{i})=E[logP\Big((y,\gamma|\theta)\Big)|y,\theta^{(i)}]$$</p><p>$$=E\Bigg(\sum_{k=1}^K\Big(n_klog\alpha_k+\sum_{j=1}^N\gamma_{jk}\Big[log\big(\frac{1}{\sqrt{2\pi}}\big)-log\sigma_k^2-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\Big]\Big)\Bigg)$$</p><p>$$=\sum_{k=1}^K\Bigg(\sum_{j=1}^Nlog\alpha_k+\sum_{j=1}^N\big(E(\gamma_{jk})\big)\Big[log\big(\frac{1}{\sqrt{2\pi}}\big)-log\sigma_k^2-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\Big]\Bigg)$$    <font color="red">(6)</font></p><p>这里的 $E(\gamma_{jk})$ 就是 $E(\gamma_{jk}|\theta)$ ，记为 $\hat\gamma_{jk}$</p><p>$$\hat\gamma_{jk}=E(\gamma_{jk}|y_j,\theta)=P(\gamma_{jk}=1|y_j,\theta)$$</p><p>【$\gamma_{kj}$ 为0-1随机变量】</p><p>$$=\Large \frac{P(\gamma_{jk}=1,y_j|\theta_k)}{\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta_k)}$$</p><p>【上式由以下公式推得：$P(\gamma_{jk}=1,y_j|\theta)=\frac{P(\gamma_{jk}=1,y_j,\theta)}{P(\theta)}$，$P(\gamma_{jk}=1|y_j,\theta)=\frac{P(\gamma_{jk}=1,y_j,\theta)}{P(y_j,\theta)}$，$\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta)=\sum_{k=1}^K\frac{P(\gamma_{jk}=1,y_j,\theta)}{P(\theta)}=\frac{P(y_j,\theta)}{P(\theta)}$】</p><p>$$=\Large \frac{P(y_j|\gamma_{jk}=1,\theta_k)P(\gamma_{jk}=1|\theta_k)}{\sum_{k=1}^KP(y_j|\gamma_{jk}=1,\theta_k)P(\gamma_{jk}=1|\theta_k)}$$</p><p>【上式由<strong>条件概率公式</strong>得到】</p><p>$$\Large{\frac{\alpha_k\phi(y_j|\theta_k)}{\sum_{k=1}^K\alpha_k\phi(y_j|\theta_k)}},\quad j=1,2,…,N;k=1,2,…,K$$    <font color="red">(7)</font></p><p>【上式因为 $\sum_{k=1}^KP(\gamma_{jk}=1,y_j|\theta_k)=P(y_j|\theta_k)=\alpha_k\phi(y_j|\theta_k)$】</p><p>$\hat\gamma_{jk}$ 是在当前模型参数下，第j个观测数据来自第k个分模型的概率，称为分模型k对观测数据 $y_j$ 的<strong>响应度</strong>。</p><p>将 $\hat\gamma_{jk}=E(\gamma_{jk})$ 以及 $n_k=\sum_{j=1}^NE(\gamma_{jk})$ 代入式<font color="red">(6)</font>得：</p><p>$$Q(\theta,\theta^{(i)})=\sum_{k=1}^K\Bigg(n_klog\alpha_k+\sum_{j=1}^N\hat\gamma_{jk}\Big[log\big(\frac{1}{\sqrt{2\pi}}\big)-log\sigma_k^2-\frac{1}{2\sigma_k^2}(y_j-\mu_k)^2\Big]\Bigg)$$    <font color="red">(8)</font></p><h4 id="3-确定EM算法得M步-求极大值"><a href="#3-确定EM算法得M步-求极大值" class="headerlink" title="3.确定EM算法得M步-求极大值"></a>3.确定EM算法得M步-求极大值</h4><p>EM算法的M步是求函数 $Q(\theta,\theta^{(i)})$ 对 $\theta$ 的极大值，即：</p><p>$$\theta^{(i+1)}=argmax_\theta Q(\theta,\theta^{(i)})$$</p><p>用 $\hat\mu_k$，$\hat\sigma_k^2$，以及 $\hat\alpha_k$ 分别表示 $\theta^{(i+1)}$ 的各个参数。然后将<font color="red">(7)</font>分别对这三个参数求偏导并令其为0得：</p><p>$$\Large\hat\mu_k=\Large \frac{\sum_{j=1}^N\hat\gamma_{jk}y_j}{\sum_{j=1}^N\hat\gamma_{jk}},\quad k=1,2,…,K$$</p><p>$$\Large\hat\sigma_k^2=\frac{\sum_{j=1}^N\hat\gamma_{jk}(y_j-\mu_k)^T(y_j-\mu_k)}{\sum_{j=1}^N\hat\gamma_{jk}},\quad k=1,2,…,K$$</p><p>$$\Large\hat\alpha_k=\frac{n_k}{N}=\frac{\sum_{j=1}^N\hat\gamma_{jk}}{N},\quad k=1,2,…,K$$</p><p>由此我们得到迭代计算的公式，最终得到<strong>高斯混合模型参数估计的EM算法</strong></p><h2 id="高斯混合模型参数估计算法"><a href="#高斯混合模型参数估计算法" class="headerlink" title="高斯混合模型参数估计算法"></a>高斯混合模型参数估计算法</h2><hr><p>输入：样本集 $D={x_1,x_2,…,x_m}$;</p><p>​            高斯混合成分个数(子高斯分布个数)K</p><p>过程：</p><p>1：初始化高斯混合模型的模型参数 ${(\alpha_k,\mu_k,\Sigma_k)|1\le i\le K}$【这里的 $\Sigma$ 就是前面推导过程中的 $\sigma$ 在多维空间        的推广。</p><p>2：<strong>repeat</strong></p><p>3：    <strong>for</strong>  $j=1,2,…,m$ <strong>do</strong></p><p>4：        根据式<font color="red">(7)</font>计算 $x_j$ 由各混合成分生成的后验概率，即</p><p>5：        $\hat\gamma_{jk}=P(\gamma_{jk}=1|x_j,\theta^{(0)})$</p><p>6：    <strong>end for</strong></p><p>7：    <strong>for</strong> $i=1,2,…,k$ <strong>do</strong></p><p>8：        计算<strong>新均值向量</strong>：$\Large\hat\mu_i=\frac{\sum_{j=1}^m\gamma_{jk}x_j}{\sum_{j=1}^m\gamma_{jk}}$;</p><p>9：        计算<strong>新协方差矩阵</strong>：$\Large\hat\Sigma_i=\frac{\sum_{j=1}^m\gamma_{jk}(x_j-\hat\mu)(x_j-\hat\mu)^T}{\sum_{j=1}^m\gamma_{jk}}$;</p><p>10：        计算<strong>新混合系数</strong>：$\Large\hat\alpha_i=\frac{\sum_{j=1}^m\gamma_{jk}}{m}$;</p><p>11：    <strong>end for</strong></p><p>12：    将模型参数更新为 ${(\hat\alpha_i,\hat\mu_i,\hat\Sigma_i)|1\le i \le K}$</p><p>13：<strong>until</strong> 满足停止条件【参数更新幅度小于某一值或者迭代指定次数】</p><p>14：$C_i=\emptyset\quad (1\le i\le K)$</p><p>15：<strong>for</strong> $j=1,2,…,m$ <strong>do</strong></p><p>16：    根据 $\lambda_j=argmax_{i=1,2,…,K}\gamma_{jk}$ 确定 $x_j$ 簇标记 $\lambda_j$;</p><p>17：    将 $x_j$ 划入相应的簇：$C_{\lambda_{j}}=C_{\lambda_k}\bigcup{x_j}$</p><p>18：<strong>end for</strong></p><p>输出：簇划分 $C={C_1,C_2,…,C_K}$</p><hr><h2 id="Python-代码实现"><a href="#Python-代码实现" class="headerlink" title="Python 代码实现"></a>Python 代码实现</h2><h3 id="高斯混合模型算法代码"><a href="#高斯混合模型算法代码" class="headerlink" title="高斯混合模型算法代码"></a>高斯混合模型算法代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.linalg <span class="keyword">as</span> llg</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GaussianF</span>(<span class="params">mu, sigma, data_Y, K</span>):</span></span><br><span class="line">    Gaussian = []</span><br><span class="line">    sigma_inv = llg.pinv(sigma)</span><br><span class="line">    sigma_value = llg.det(sigma)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        first = <span class="number">1</span>/(((<span class="number">2</span>*np.pi)**(data_Y.shape[<span class="number">0</span>]/<span class="number">2</span>))*np.sqrt(sigma_value[k]))</span><br><span class="line">        Gaussian_k = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_Y.shape[<span class="number">1</span>]):</span><br><span class="line">            second = np.exp(-<span class="number">0.5</span>*np.dot(np.dot((data_Y[:, i] - mu[:, k])[np.newaxis, :], sigma_inv[k,::]), (data_Y[:, i] - mu[:, k])[:np.newaxis]))</span><br><span class="line">            Gaussian_k.append((first*second).tolist())</span><br><span class="line">        Gaussian.append(Gaussian_k)</span><br><span class="line">    Gaussian = np.array(Gaussian)</span><br><span class="line">    Gaussian = np.transpose(Gaussian.reshape((Gaussian.shape[<span class="number">0</span>], Gaussian.shape[<span class="number">1</span>])))</span><br><span class="line">    <span class="keyword">return</span> Gaussian</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalcMu</span>(<span class="params">data_Y, gamma, K</span>):</span></span><br><span class="line">    mu = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        mu_up = <span class="number">0</span></span><br><span class="line">        mu_down = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(data_Y.shape[<span class="number">1</span>]):</span><br><span class="line">            mu_up += gamma[j, i]*data_Y[:, j]</span><br><span class="line">            mu_down += gamma[j, i]</span><br><span class="line">        mu.append(mu_up/mu_down)</span><br><span class="line">    mu = np.transpose(np.array(mu))</span><br><span class="line">    <span class="keyword">return</span> mu</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Covariance</span>(<span class="params">gamma, data_Y, mu, K</span>):</span></span><br><span class="line">    sigma = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        sigma_up = <span class="number">0</span></span><br><span class="line">        sigma_down = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(data_Y.shape[<span class="number">1</span>]):</span><br><span class="line">            y = data_Y[:, j][:, np.newaxis]</span><br><span class="line">            mu_i = mu[:, i][:, np.newaxis]</span><br><span class="line">            sigma_up += gamma[j, i]*np.dot((y - mu_i), np.transpose(y - mu_i))</span><br><span class="line">            sigma_down += gamma[j, i]</span><br><span class="line">        sigma.append(sigma_up/sigma_down)</span><br><span class="line">    sigma = np.array(sigma)</span><br><span class="line">    <span class="keyword">return</span> sigma</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Mixture</span>(<span class="params">gamma, num, K</span>):</span></span><br><span class="line">    alpha = []</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        alpha_j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">            alpha_j += gamma[j, k]</span><br><span class="line">        alpha.append(alpha_j/num)</span><br><span class="line">    <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">CalcGamma</span>(<span class="params">alpha, mu, sigma, data_Y, K</span>):</span></span><br><span class="line">    Gaussian = GaussianF(mu, sigma, data_Y, K)</span><br><span class="line">    gamma = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(data_Y.shape[<span class="number">1</span>]):</span><br><span class="line">        Gaussian_j = <span class="number">0</span></span><br><span class="line">        gamma_j = []</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">            Gaussian_j += alpha[k]*Gaussian[j, k]</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">            gamma_j.append(alpha[k]*Gaussian[j, k]/Gaussian_j)</span><br><span class="line">        gamma.append(gamma_j)</span><br><span class="line">    gamma = np.array(gamma)</span><br><span class="line">    <span class="keyword">return</span> gamma</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">GMM</span>(<span class="params">data_Y, K=<span class="number">2</span></span>):</span><span class="comment">#data_Y为列向量</span></span><br><span class="line">    alpha = [<span class="number">1</span>/K]*K</span><br><span class="line">    sigma = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        sigma.append(np.eye(data_Y.shape[<span class="number">0</span>]))</span><br><span class="line">    sigma = np.array(sigma)</span><br><span class="line">    mu = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        mu.append(data_Y[:,i])</span><br><span class="line">    mu = np.transpose(np.array(mu))</span><br><span class="line">    gamma = CalcGamma(alpha, mu, sigma, data_Y, K)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        mu = CalcMu(data_Y, gamma, K)</span><br><span class="line">        sigma = Covariance(gamma, data_Y, mu, K)</span><br><span class="line">        alpha = Mixture(gamma, data_Y.shape[<span class="number">1</span>], K)</span><br><span class="line">        gamma = CalcGamma(alpha, mu, sigma, data_Y, K)</span><br><span class="line">    <span class="keyword">return</span> gamma</span><br></pre></td></tr></table></figure><h3 id="高斯混合模型聚类学习样例"><a href="#高斯混合模型聚类学习样例" class="headerlink" title="高斯混合模型聚类学习样例"></a>高斯混合模型聚类学习样例</h3><p>这里，我们先利用<strong>正态分布随机数发生器</strong>随机生成三组正态分布参数不同的数据，然后将三组数据组合成一组数据作为训练数据。经过高斯混合模型训练后，利用matploylib绘图函数可视化分类结果。</p><p>代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    data_0 = np.random.normal(loc=<span class="number">1</span>, scale=<span class="number">2</span>, size=(<span class="number">100</span>,<span class="number">2</span>))</span><br><span class="line">    data_1 = np.random.normal(loc=<span class="number">10</span>, scale=<span class="number">2</span>, size=(<span class="number">100</span>,<span class="number">2</span>))</span><br><span class="line">    data_2 = np.random.normal(loc=<span class="number">5</span>, scale=<span class="number">1</span>, size=(<span class="number">100</span>,<span class="number">2</span>))</span><br><span class="line">    data_Y = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_0.shape[<span class="number">0</span>]):</span><br><span class="line">        data_Y.append(data_0[i, :])</span><br><span class="line">        data_Y.append(data_1[i, :])</span><br><span class="line">        data_Y.append(data_2[i, :])</span><br><span class="line">    data_Y = np.transpose(data_Y)</span><br><span class="line">    K = <span class="number">3</span></span><br><span class="line">    gamma = GMM(data_Y, K)</span><br><span class="line">    index = np.argmax(gamma, axis=<span class="number">1</span>)</span><br><span class="line">    index_ = []</span><br><span class="line">    data_Y_ = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(K):</span><br><span class="line">        data_Y_ = data_Y[:,np.where(index == i)]</span><br><span class="line">        plt.scatter(data_Y_[<span class="number">0</span>, :], data_Y_[<span class="number">1</span>, :])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>最后输出的结果图像如下：</p><p><img src="/2019/10/25/GMM-algorithm/Figure.png" alt="GMM"></p><p>可以看出，<strong>高斯混合模型聚类器</strong>很好的将数据进行聚类！</p>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Cluster </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>EM algorithm</title>
      <link href="/2019/10/19/EM-algorithm/"/>
      <url>/2019/10/19/EM-algorithm/</url>
      
        <content type="html"><![CDATA[<h1 id="Expectation-Maximization-algorithm"><a href="#Expectation-Maximization-algorithm" class="headerlink" title=" Expectation-Maximization algorithm "></a><center> Expectation-Maximization algorithm </center></h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>  <strong>EM</strong>算法（ Expectation-Maximization algorithm ），又称<strong>期望极大算法</strong>，是<strong>机器学习</strong>（Machine Learning）十大算法之一。</p><p>  <strong>EM</strong>算法主要通过迭代的方法来进行<strong>极大似然估计</strong>，最终使似然函数最大，从而获取模型参数进而建立预测模型的算法，<strong>EM</strong>算法通常和<strong>高斯混合分布</strong>算法联合使用来进行<strong>分类学习</strong>。</p><h2 id="对数似然函数-Log-likelihood"><a href="#对数似然函数-Log-likelihood" class="headerlink" title="对数似然函数(Log likelihood)"></a>对数似然函数(Log likelihood)</h2><h3 id="似然函数-Likelihood"><a href="#似然函数-Likelihood" class="headerlink" title="似然函数(Likelihood):"></a>似然函数(Likelihood):</h3><p>  <strong>似然函数</strong>是统计学上计算统计模型参数的函数，给定输出$y$后，关于参数$\theta$的似然函数记为$L(\theta|y)$.</p><p>  其计算方法为：$$L(\theta|Y) = P(Y=y|\theta)$$</p><p>  如果输出参数$y_i$有$N$个时，其<strong>似然函数</strong>为：$$L(\theta|Y) = \prod_{i=1}^{N}P(y_i|\theta)$$</p><hr><h3 id="极大似然估计-MLE"><a href="#极大似然估计-MLE" class="headerlink" title="极大似然估计(MLE):"></a>极大似然估计(MLE):</h3><p>为了求解到最优的统计模型参数，我们需要极大化<strong>似然函数</strong>，并取此时的$\theta$为我们的模型参数，即：</p><p>$$\theta = argmax_{\theta}L(\theta|Y)$$</p><hr><h3 id="对数似然函数-LLD-："><a href="#对数似然函数-LLD-：" class="headerlink" title="对数似然函数(LLD)："></a>对数似然函数(LLD)：</h3><p>  对于上面的普通的<strong>似然函数</strong>虽然很容易理解，但却不方便计算(特别是对于计算机来说)，因为当已知数据$y_i$数量很大时，$P(y_i|\theta) &lt; 1$ 从而使得 $L(\theta|Y) &lt;&lt; 1$，甚至会突破计算机对浮点数的存储极限。因此，我们将<strong>似然函数</strong>取对数，从而得到<strong>对数似然函数</strong>，即：</p><p>$$LL(\theta|Y) = log(L(\theta|Y))$$</p><p><strong>对数似然函数</strong>不仅满足了计算机的存储问题，而且对数函数是<strong>单调增函数</strong>，所以<strong>似然函数</strong>的数学性质不会改变，也满足我们的计算需求。</p><hr><h2 id="EM算法的导出"><a href="#EM算法的导出" class="headerlink" title="EM算法的导出"></a>EM算法的导出</h2><h3 id="隐变量"><a href="#隐变量" class="headerlink" title="隐变量"></a>隐变量</h3><p>**隐变量(latent variable)**即潜在变量，是模型中无法被观测到却又在理论上存在的中间变量。例如，有以下模型：假设有三枚硬币，分别记作A、B、C。这些硬币正面出现的概率分别为 $\pi、p、q$。进行如下试验：先掷硬币A，如果A正面向上，选B硬币；如果A反面向上，选C硬币；然后记录B或C硬币投掷的结果 $y$。</p><p>在上述试验中：$y$ 为观测变量， $\pi、p、q$ 为模型参数，<strong>投掷A硬币的结果即为隐变量</strong>。</p><h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><p>对于含有<strong>隐变量</strong>且参数未知的概率模型，我们的优化目标就是极大化观测数（不完全数据）$Y$关于参数$\theta$的<strong>对数似然函数</strong>，即极大化：$$L(\theta) = logP(Y|\theta) = log\sum_ZP(Y,Z|\theta)$$</p><p>$$=log\Big(\sum_ZP(Y|Z,\theta)P(Z|\theta)\Big)$$</p><p>对上式求极大化的困难主要在于，上式存在未观测变量$Z$</p><p>而<strong>EM算法</strong>并没有硬求为观测变量，它是通过迭代计算的方法逐步极大化$L(\theta)$。因此，我们假设第$i$次迭代后概率模型参数的估计值为$\theta^{(i)}$。我们希望新的估计值 $\theta$ 能使$L(\theta)$ 增加，即 $L(\theta) &gt; L(\theta^{(i)})$ ，为此，我们考虑两者之差：</p><p>$$L(\theta) - L(\theta^{(i)}) = log(\sum_zP(Y|Z,\theta)P(Z|\theta)) - logP(Y|\theta^{(i)})$$</p><p>然后利用<strong>Jensen 不等式</strong>($log\sum_j\lambda_jy_j\ge\sum_j\lambda_jlogy_j , \lambda_j\ge0,\sum_j\lambda_j=1$，这样我们得到下面的放缩：</p><p>$$L(\theta) - L(\theta^{(i)})=log\sum_{\small{Z}}P(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})})-logP(Y|\theta^{(i)})$$</p><p>$$\ge\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-logP(Y|\theta^{(i)})$$</p><p>$$ = \sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$$</p><p>令$$B(\theta,\theta^{(i)})=L(\theta,\theta^{(i)})+\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$$</p><p>则有：</p><p>$$L(\theta)\ge B(\theta,\theta^{(i)})$$</p><p>即说明函数 $B(\theta,\theta^{(i)})$ 是函数 $L(\theta)$ 的一个下界，而且由上式可得：</p><p>$$L(\theta^{(i)})=B(\theta^{(i)},\theta^{(i)})$$</p><p>因此，任何可以使 $B(\theta,\theta^{(i)})$ 增大的 $\theta$ ，都可以使 $L(\theta)$ 增大。为了使 $L(\theta)$ 有尽可能大的增长，我们选择 $\theta^{(i+1)}$,即：</p><p>$$L(\theta^{(i+1)})=argmax_{\small{\theta}}B(\theta,\theta^{(i)})$$</p><p>但现在 $\theta^{(i+1)}$  是未知的，所以我们需要推导出 $\theta^{(i+1)}$ 的表达式，如下：</p><p>$$\large{\theta^{(i+1)}}=argmax_{\small{\theta}}\Big(L(\theta^{(i)})+\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\Big)$$</p><p>[因为$L(\theta^{(i)}) , P(Z|Y,\theta^{(i)}),  P(Y|\theta^{(i)})$ , 与自变量 $\theta$ 无关，故删除]，得到：</p><p>$$=argmax_\theta\bigg(\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log\Big(P(Y|Z,\theta)P(Z|\theta)\Big)\bigg)$$</p><p>$$=argmax_\theta\Big(\sum_{\small{Z}}P(Z|Y,\theta^{(i)})log(P(Y,Z|\theta))\Big)$$</p><p>[令其为Q函数]，得到：</p><p>$$=argmax_{\theta}Q(\theta,\theta^{(i)})$$</p><h3 id="Q函数"><a href="#Q函数" class="headerlink" title="Q函数"></a>Q函数</h3><hr><p>完全数据的对数似然函数 $logP(Y,Z|\theta)$ 关于在给定观测数据 $Y$ 和当前参数 $\theta^{(i)}$ 下对未知观测数据 $Z$ 的条件概率分布 $P(Z|Y,\theta^{(i)})$ 的期望，称为<strong>Q函数</strong>：</p><p>$$Q(\theta,\theta^{(i)})=E_{\small{Z}}[logP(Y,Z|\theta)|Y,\theta^{(i)}]$$</p><p>$$=\sum_{\small{Z}}log\big(P(Y,Z|\theta)\big)P(Z|Y,\theta^{(i)})$$</p><hr><p>由于 $P(Y,Z|\theta)$ 我们可以通过原始数据求的，因此我们便得出最终的优化目标，即<strong>最大化Q函数</strong>，下面是<strong>EM算法</strong>的步骤：</p><h3 id="EM算法步骤"><a href="#EM算法步骤" class="headerlink" title="EM算法步骤"></a>EM算法步骤</h3><p>输入：观测变量数据<strong>Y</strong>，隐变量数据<strong>Z</strong>，联合分布**$P(Y,Z|\theta)$**，条件分布$P(Z|Y,\theta)$</p><p>输出：模型参数 $\theta$</p><ol><li><p>选择参数的初值 $\theta^{(0)}$ ，开始迭代；</p></li><li><p><strong>E步</strong>：记 $\theta^{(i)}$，为第 i 次迭代的参数 $\theta$ 的估计值，在第 i+1 次迭代的E步，计算：</p><p>$$Q(\theta,\theta^{(i)})=\sum_{\small{Z}}log\big(P(Y,Z|\theta)\big)P(Z|Y,\theta^{(i)})$$</p><p>【这里，$P(Z|Y,\theta^{(i)})$ 是在给定观测数据 $Y$ 和当前的参数估计 $\theta^{(i)}$ 下隐变量 $Z$ 的条件概率分布】</p></li><li><p><strong>M步</strong>：求使 $Q(\theta,\theta^{(i)})$ 极大化的 $\theta$，确定第 i+1 次迭代的参数估计值 $\theta^{(i+1)}$：</p><p>$$\theta^{(i+1)}=argmax_\theta Q(\theta,\theta^{(i)})$$</p></li><li><p>重复 2、3 步，直到收敛。</p></li></ol><h2 id="EM算法的收敛性证明"><a href="#EM算法的收敛性证明" class="headerlink" title="EM算法的收敛性证明"></a>EM算法的收敛性证明</h2><p><strong>定理 1</strong>：设 $P(Y|\theta)$ 为观测数据的似然函数，$\theta^{(i)}(i=1,2,…)$ 为<strong>EM算法</strong>得到的<strong>参数估计序列</strong>，$P(Y|\theta^{(i)})(i=1,2,…)$ 为对应的似然函数序列，则 $P(Y|\theta^{(i)})$ 是单调递增的，即</p><p>$$P(Y|\theta^{(i+1)})\ge P(Y|\theta^{(i)})$$     <font color="red">(*) </font></p><p><strong>证明</strong>：</p><p>由于【根据概率公式】：</p><p>$$P(Y|\theta)=\frac{P(Y,Z|\theta)}{P(Z|Y,\theta)}$$</p><p>两边取对数得：</p><p>$$logP(Y|\theta)=logP(Y,Z|\theta)-logP(Z|Y,\theta)$$</p><p>由Q函数定义，即：</p><p>$$Q(\theta,\theta^{(i)})=\sum_{\small{Z}}logP(Y,Z|\theta)P(Z|Y,\theta^{(i)})$$</p><p>令：</p><p>$$H(\theta,\theta^{(i)})=\sum_{\small{Z}}logP(Z|Y,\theta)P(Z|Y,\theta^{(i)})$$    <font color="red">(1)</font></p><p>于是上面得<strong>对数似然函数</strong>可以写成：</p><p>$$logP(Y|\theta)=Q(\theta,\theta^{(i)})-H(\theta,\theta^{(i)})$$</p><p>上式中的 $\theta$ 分别取 $\theta^{(i)}$ 和 $\theta^{(i+1)}$ 并相减，得到：</p><p>$$log(P(Y|\theta^{(i+1)}))-log(P(Y|\theta^{(i)}))$$</p><p>$$= [Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)}),\theta^{(i)}]-[H(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i)},\theta^{(i)})]$$    <font color="red">(*&#39;)</font></p><p>为了证明<font color="red">(*)</font>式，只需证明上式的右边是非负的。<em>上式右边第一项</em>，因为 $\theta^{(i+1)}$ 使 $Q(\theta,\theta^{(i)})$ 达到极大，所以有：</p><p>$$Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})\ge0$$    <font color="red">(2)</font></p><p><em>上式右边第二项</em>，由 <font color="red">(1)</font> 式得：</p><p>$$H(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i)},\theta^{(i)})$$</p><p>$$=\sum_{\small{Z}}\Big(log\frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})}\Big)P(Z|Y,\theta^{(i)})$$</p><p>下面由<strong>Jensen不等式</strong>得：</p><p>$$\le log\Big(\sum_{\small{Z}}\frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})}P(Z|Y,\theta^{(i)})\Big)$$</p><p>$$=log\Big(\sum_{\small{Z}}P(Z|Y,\theta^{(i+1)})\Big)=0$$    <font color="red">(3)</font></p><p>由<font color="red">(*&#39;)  (2) (3) </font>得，<font color="red">(*&#39;)</font>的左式是非负的，所以 <font color="red">(*)</font> 式得证。</p><hr><p><strong>定理 2</strong>：设 $L(\vec\theta)=logP(Y|\vec\theta)$ 为观测数据的对数似然函数，$\vec\theta^{(i)}(i=1,2,…)$ 为<strong>EM算法</strong>得到的<strong>参数估计序列</strong>，$L(\vec\theta^{(i)})=logP(Y|\vec\theta^{(i)})$ 为对应的对数似然函数序列。</p><ol><li>如果 $P(Y|\vec\theta)$ 有上界，则 $L(\vec\theta^{(i)})=logP(Y|\vec\theta^{(i)})$ 收敛到某一值 $L^*$ ;</li><li>在函数 $Q(\theta,\theta^{‘})$ 与 $L(\theta)$ 满足一定条件下，由<strong>EM</strong>算法得到的参数估计序列 $\vec\theta^{(i)}$ 的收敛值 $\vec\theta^{*}$ 是 $L(\vec\theta)$ 的稳定点</li></ol><p><strong>证明</strong>：</p><ol><li>由 $L(\vec\theta)=logP(Y|\vec\theta^{(i)})$ 的单调性及 $P(Y|\vec\theta)$ 的有界性得到，【<strong>单调有界原理</strong>】</li><li>证明<strong>略</strong>.</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>markdown-doc</title>
      <link href="/2019/10/17/markdown-doc/"/>
      <url>/2019/10/17/markdown-doc/</url>
      
        <content type="html"><![CDATA[<h1 id="MarkDown"><a href="#MarkDown" class="headerlink" title="MarkDown"></a>MarkDown</h1><h2 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h2><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><div class="table-container"><table><thead><tr><th style="text-align:center"><center>字母</center></th><th style="text-align:center"><center>写法(首字母大写即对应大写)</center></th></tr></thead><tbody><tr><td style="text-align:center"><center>$\alpha$</center></td><td style="text-align:center"><center>\alpha</center></td></tr><tr><td style="text-align:center"><center>$\beta$</center></td><td style="text-align:center"><center>\beta</center></td></tr><tr><td style="text-align:center"><center>$\gamma$</center></td><td style="text-align:center"><center>\gamma</center></td></tr><tr><td style="text-align:center"><center>$\delta$</center></td><td style="text-align:center"><center>\delta</center></td></tr><tr><td style="text-align:center"><center>$\epsilon$</center></td><td style="text-align:center"><center>\epsilon</center></td></tr><tr><td style="text-align:center"><center>$\varepsilon$</center></td><td style="text-align:center"><center>\varepsilon</center></td></tr><tr><td style="text-align:center"><center>$\zeta$</center></td><td style="text-align:center"><center>\zeta</center></td></tr><tr><td style="text-align:center"><center>$\eta$</center></td><td style="text-align:center"><center>\eta</center></td></tr><tr><td style="text-align:center"><center>$\theta$</center></td><td style="text-align:center"><center>\theta</center></td></tr><tr><td style="text-align:center"><center>$\iota$</center></td><td style="text-align:center"><center>\iota</center></td></tr><tr><td style="text-align:center"><center>$\kappa$</center></td><td style="text-align:center"><center>\kappa</center></td></tr><tr><td style="text-align:center"><center>$\lambda$</center></td><td style="text-align:center"><center>\lambda</center></td></tr><tr><td style="text-align:center"><center>$\mu$</center></td><td style="text-align:center"><center>\mu</center></td></tr><tr><td style="text-align:center"><center>$\nu$</center></td><td style="text-align:center"><center>\nu</center></td></tr><tr><td style="text-align:center"><center>$\xi$</center></td><td style="text-align:center"><center>\xi</center></td></tr><tr><td style="text-align:center"><center>$\omicron$</center></td><td style="text-align:center"><center>\omicron</center></td></tr><tr><td style="text-align:center"><center>$\pi$</center></td><td style="text-align:center"><center>\pi</center></td></tr><tr><td style="text-align:center"><center>$\rho$</center></td><td style="text-align:center"><center>\rho</center></td></tr><tr><td style="text-align:center"><center>$\sigma$</center></td><td style="text-align:center"><center>\sigma</center></td></tr><tr><td style="text-align:center"><center>$\tau$</center></td><td style="text-align:center"><center>\tau</center></td></tr><tr><td style="text-align:center"><center>$\upsilon$</center></td><td style="text-align:center"><center>\upsilon</center></td></tr><tr><td style="text-align:center"><center>$\phi$</center></td><td style="text-align:center"><center>\phi</center></td></tr><tr><td style="text-align:center"><center>$\varphi$</center></td><td style="text-align:center"><center>\varphi</center></td></tr><tr><td style="text-align:center"><center>$\chi$</center></td><td style="text-align:center"><center>\chi</center></td></tr><tr><td style="text-align:center"><center>$\psi$</center></td><td style="text-align:center"><center>\psi</center></td></tr></tbody></table></div><h3 id="插入公式"><a href="#插入公式" class="headerlink" title="插入公式"></a>插入公式</h3><ol><li>行内公式：\$xyz\$ , 显示：$xyz$</li><li>独行公式：\$\$xyz\$\$ , 显示：<script type="math/tex">xyz</script></li></ol><h3 id="上标和下标以及组合"><a href="#上标和下标以及组合" class="headerlink" title="上标和下标以及组合"></a>上标和下标以及组合</h3><ol><li>上标：\^ , $x^2$ , 显示：$x^3$</li><li>下标：_ , $x_i$ , 显示：$x_i$</li><li>组合：{} , ${16}<em>{18}{2+}</em>{2}$ , 显示：${16}<em>{18}{2+}</em>{2}$</li></ol><h3 id="字母大小"><a href="#字母大小" class="headerlink" title="字母大小"></a>字母大小</h3><ol><li>\tiny : $\tiny{abc}$</li><li>\small : $\small{abc}$</li><li>\normalsize : $\normalsize{abc}$</li><li>\large : $\large{abc}$</li><li>\Large : $\Large{abc}$</li><li>\LARGE : $\LARGE{abc}$</li></ol><h3 id="分式"><a href="#分式" class="headerlink" title="分式"></a>分式</h3><ol><li>\dfrac{}{} : 表示该分式是以 displaystyle 设置的，例如： $\dfrac{abc}{xyz}$</li><li>\tfrac{}{} : 表示该分式是以 textstyle 设置的，例如：$\tfrac{abc}{xyz}$</li><li>\frac{}{} : 表示该分式根据环境设置样式，例如：$\frac{abc}{xyz}$</li><li>{}\over{} : 分式的另一种形式，例如：${abc}\over{xyz}$</li></ol><h3 id="根式"><a href="#根式" class="headerlink" title="根式"></a>根式</h3><ol><li>\sqrt{} : 二次根式，例如：$\sqrt{abc}$</li><li>\sqrt[n]{} : n次根式，例如：$\sqrt[abc]{xyz}$</li><li>\mathstrut : 使连根式变得整齐，例如：$\sqrt{\mathstrut a} + \sqrt{\mathstrut b} + \sqrt{\mathstrut c}$</li></ol><h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><ol><li>\vec{} : 矢量箭头，例如：$\vec{xyz}$</li><li>\overrightarrow{} : 右箭头，例如：$\overrightarrow{xyx}$</li><li>\overleft arrow{} : 左箭头，例如：$\overleftarrow{xyz}$</li><li>\overleftrightarrow{} : 双箭头，例如：$\overleftrightarrow{xyz}$</li></ol><h3 id="括号"><a href="#括号" class="headerlink" title="括号"></a>括号</h3><ol><li>\big() : 小括号，例如：$\big(xyz)$</li><li>\Big() : 大一点的小括号，例如：$\Big(xyz)$</li></ol><h2 id="文本"><a href="#文本" class="headerlink" title="文本"></a>文本</h2><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ol><li># 一级标题</li><li>## 二级标题</li><li>### 三级标题</li><li>#### 四级标题</li><li>##### 五级标题</li><li>###### 六级标题</li></ol><h3 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h3><ol><li>加粗：**加粗**，<strong>加粗</strong></li><li>斜体：*斜体*，<em>斜体</em></li><li>斜体加粗：***斜体加粗***，<strong><em>斜体加粗</em></strong></li><li>删除线：~~删除线~~，<del>删除线</del></li></ol><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><h3 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h3><ol><li>一层引用：&gt;</li><li>二层引用：&gt;&gt;</li><li>多层引用：&gt;&gt;&gt;&gt;</li></ol><p>&gt;<br>&gt;</p><blockquote><p>&gt;<br>&gt;</p><blockquote><p>&gt;<br>&gt;</p><blockquote><p>&gt;<br>&gt;</p><blockquote><p>引用</p></blockquote></blockquote></blockquote></blockquote><h3 id="分割线"><a href="#分割线" class="headerlink" title="分割线"></a>分割线</h3><p>三个以上---、三个以上***</p><hr><hr><h3 id="序号"><a href="#序号" class="headerlink" title="序号"></a>序号</h3><h4 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h4><p>使用-，+，* 任意一种都可以，可以嵌套：</p><ol><li>- 列表内容</li><li>+ 列表内容</li><li>* 列表内容</li></ol><p>效果：</p><ul><li>列表内容<ul><li>列表内容<ul><li>列表内容<ul><li>列表内容</li></ul></li></ul></li></ul></li></ul><h4 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h4><p>数字加点(英文)：</p><p>例如：1.</p><p>效果：</p><ol><li>效果</li><li>效果</li></ol><h2 id="插入资源"><a href="#插入资源" class="headerlink" title="插入资源"></a>插入资源</h2><h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><p>![图片名称](图片地址)</p><p><img src="/2019/10/17/markdown-doc/picture.png" alt="picture"></p><h3 id="超链接"><a href="#超链接" class="headerlink" title="超链接"></a>超链接</h3><p>[超链接名](超链接地址)</p><p>例如：<a href="https://vilily.github.io/">myblog</a></p><h3 id="引用-1"><a href="#引用-1" class="headerlink" title="引用"></a>引用</h3><h4 id="在文中插入角标"><a href="#在文中插入角标" class="headerlink" title="在文中插入角标"></a>在文中插入角标</h4><p><a href="#refer-anchor-1"><sup>1</sup></a></p><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="tag">&lt;<span class="name">sup</span>&gt;</span>1<span class="tag">&lt;/<span class="name">sup</span>&gt;</span>](#refer-anchor-1)</span><br></pre></td></tr></tbody></table></figure><h4 id="在结尾插入论文引用"><a href="#在结尾插入论文引用" class="headerlink" title="在结尾插入论文引用"></a>在结尾插入论文引用</h4><p></p><div id="refer-anchor-1"></div> [1] <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a><p></p><figure class="highlight html"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"refer-anchor-1"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span> [1] [Attention is all you need](https://arxiv.org/abs/1706.03762)</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> Markdown </tag>
            
            <tag> Latex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo-Learn</title>
      <link href="/2019/10/16/Hexo-Learn/"/>
      <url>/2019/10/16/Hexo-Learn/</url>
      
        <content type="html"><![CDATA[<h1 id="Hexo-Learning"><a href="#Hexo-Learning" class="headerlink" title="Hexo Learning"></a><center>Hexo Learning</center></h1><h2 id="Change-Theme"><a href="#Change-Theme" class="headerlink" title="Change Theme"></a>Change Theme</h2><ol><li><p>Get open open source <a href="https://hexo.io/themes/">theme</a>.</p></li><li><p>Clone the files of the themes in <a href="https://github.com/">Github</a> to the theme folder.</p></li><li><p>Open the (_config.yml)file which is a file to store the information of sets.</p><p><img src="/2019/10/16/Hexo-Learn/test.png" alt="config.yml"></p></li><li><p>run code [hexo generate]</p></li><li><p>run code [hexo deploy]</p></li></ol><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p><h2 id="Add-Tags"><a href="#Add-Tags" class="headerlink" title="Add Tags"></a>Add Tags</h2><p>tags: [tags1, tags 2, …]</p>]]></content>
      
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
